{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-19T14:56:13.181529Z",
     "start_time": "2025-05-19T14:56:13.069677Z"
    }
   },
   "source": [
    "from torch._inductor.select_algorithm import get_num_workers\n",
    "\n",
    "from preprocess_data import DataPreprocessor\n",
    "# Imports\n",
    "import optuna\n",
    "import numpy as np\n",
    "import helpers\n",
    "\n",
    "\n",
    "data_preprocessor = DataPreprocessor()\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = data_preprocessor.preprocess_data(lot_frontage_threshold=13)\n",
    "\n",
    "to_remove = ['ohe__Utilities_NoSeWa', 'ohe__Neighborhood_Blueste', 'ohe__Condition1_RRNe', 'ohe__Condition2_PosA', 'ohe__Condition2_RRAe', 'ohe__Condition2_RRAn', 'ohe__Condition2_RRNn', 'ohe__RoofMatl_Membran', 'ohe__RoofMatl_Metal', 'ohe__RoofMatl_Roll', 'ohe__Exterior1st_AsphShn', 'ohe__Exterior1st_CBlock', 'ohe__Exterior1st_ImStucc', 'ohe__Exterior1st_Stone', 'ohe__Exterior2nd_CBlock', 'ohe__Exterior2nd_Other', 'ohe__Electrical_Mix', 'ohe__MiscFeature_TenC'] #Removed based on <= 1 total non-zero appearances\n",
    "\n",
    "X_train = X_train.drop(to_remove, axis=1)\n",
    "X_test = X_test.drop(to_remove, axis=1)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated Global Median Ratio: 0.7235 (from 951 samples)\n",
      "Calculating for group level: 3way (['MSZoning', 'BldgType', 'LotShape'])\n",
      " -> Found 39 groups for 3way\n",
      "Calculating for group level: 2way_ZS (['MSZoning', 'LotShape'])\n",
      " -> Found 16 groups for 2way_ZS\n",
      "Calculating for group level: 2way_ZB (['MSZoning', 'BldgType'])\n",
      " -> Found 19 groups for 2way_ZB\n",
      "Calculating for group level: 2way_BS (['BldgType', 'LotShape'])\n",
      " -> Found 14 groups for 2way_BS\n",
      "Calculating for group level: 1way_Z (['MSZoning'])\n",
      " -> Found 5 groups for 1way_Z\n",
      "Calculating for group level: 1way_B (['BldgType'])\n",
      " -> Found 5 groups for 1way_B\n",
      "Calculating for group level: 1way_S (['LotShape'])\n",
      " -> Found 4 groups for 1way_S\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T14:56:13.185585Z",
     "start_time": "2025-05-19T14:56:13.182050Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "log_then_minmax = Pipeline([\n",
    "    ('log_transform', FunctionTransformer(np.log1p)), # Example log transform\n",
    "    ('min_max_scaler', MinMaxScaler())\n",
    "])\n",
    "\n",
    "ordinal_then_minmax_pipeline = Pipeline([\n",
    "    ('ordinal_encode', OrdinalEncoder(\n",
    "        categories=helpers.get_ordinal_cats_ordered(), # Make sure this returns the correct list of lists for categories\n",
    "        handle_unknown='use_encoded_value',\n",
    "        unknown_value=-1 # Or np.nan, but -1 works fine with MinMaxScaler\n",
    "    )),\n",
    "    ('minmax_scale_ordinal', MinMaxScaler()) # Scale the 0,1,2... output of OrdinalEncoder to [0,1]\n",
    "])\n",
    "\n",
    "model_pipeline = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('log_num', log_then_minmax, helpers.get_log_minmax_cols()),\n",
    "            ('ord', ordinal_then_minmax_pipeline, helpers.get_categorical_cols_ordinal()),\n",
    "            ('num', MinMaxScaler(), helpers.get_minmax_cols())\n",
    "        ],\n",
    "        remainder='passthrough',\n",
    "        sparse_threshold=1\n",
    "    )"
   ],
   "id": "56c9bf0e55d18057",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T14:56:13.227419Z",
     "start_time": "2025-05-19T14:56:13.186083Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "# Deep learning model:\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "model_pipeline.fit(X_train, Y_train)\n",
    "X_train = model_pipeline.transform(X_train)\n",
    "X_test = model_pipeline.transform(X_test)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# float64 acceptable for EDA, float32 preferred for training.\n",
    "X_train = torch.tensor(X_train.values, device=device, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test.values, device=device, dtype=torch.float32)\n",
    "Y_train = torch.tensor(Y_train.values, device=device, dtype=torch.float32)\n",
    "Y_test = torch.tensor(Y_test.values, device=device, dtype=torch.float32)\n",
    "\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(X_train.shape[1], 302),\n",
    "    nn.ELU(),\n",
    "    nn.Linear(302, 75),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(75, 18),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(18, 1)\n",
    ")\n",
    "\n",
    "try:\n",
    "    train_dataset = TensorDataset(X_train, Y_train)\n",
    "    val_dataset = TensorDataset(X_test, Y_test)\n",
    "    print(f\"Train dataset length: {len(train_dataset)}\")\n",
    "    print(f\"Validation dataset length: {len(val_dataset)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating TensorDataset: {e}\")\n",
    "    # Likely length mismatch between X and y tensors if error here\n",
    "\n",
    "batch_size = 64 # Batch size lowered, due to sample size being less than ideal.\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) # Shuffle training data\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False) # No need to shuffle validation\n",
    "\n",
    "loss_func = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.09078721496248407,\n",
    "                             betas=(0.8149819299051041, 0.9976723251615085),\n",
    "                             eps=2.3572355850494554e-08)\n",
    "\n",
    "model.to(device)"
   ],
   "id": "7ef03f6361cd05",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset length: 1168\n",
      "Validation dataset length: 292\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=242, out_features=302, bias=True)\n",
       "  (1): ELU(alpha=1.0)\n",
       "  (2): Linear(in_features=302, out_features=75, bias=True)\n",
       "  (3): LeakyReLU(negative_slope=0.01)\n",
       "  (4): Linear(in_features=75, out_features=18, bias=True)\n",
       "  (5): LeakyReLU(negative_slope=0.01)\n",
       "  (6): Linear(in_features=18, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "num_epochs = 155 # Hyperparameter: How many times to iterate over the dataset\n",
    "\n",
    "def get_choice(activation_choice_name):\n",
    "    \"\"\"Helper function to get an activation function instance.\"\"\"\n",
    "    if activation_choice_name == 'sigmoid':\n",
    "        return nn.Sigmoid()\n",
    "    elif activation_choice_name == 'tanh':\n",
    "        return nn.Tanh()\n",
    "    elif activation_choice_name == 'leaky_relu':\n",
    "        return nn.LeakyReLU()\n",
    "    elif activation_choice_name == 'elu':\n",
    "        return nn.ELU()\n",
    "    elif activation_choice_name == 'silu': # Swish\n",
    "        return nn.SiLU()\n",
    "    else: # Default to ReLU\n",
    "        return nn.ReLU()\n",
    "\n",
    "def objective(trial):\n",
    "    adam_lr = trial.suggest_float(\"adam_lr\", 0.09, 0.1, log=True)\n",
    "    adam_beta1 = trial.suggest_float(\"adam_beta1\", 0.8, 0.95)\n",
    "    adam_beta2 = trial.suggest_float(\"adam_beta2\", 0.9, 0.999)\n",
    "    adam_epsilon = trial.suggest_float(\"adam_epsilon\", 1e-9, 1e-7, log=True)\n",
    "    activation_choice_0 = trial.suggest_categorical('activation_0', ['relu', 'leaky_relu', 'elu', 'silu', 'tanh'])\n",
    "    activation_choice_1 = trial.suggest_categorical('activation_1', ['relu', 'leaky_relu', 'elu', 'silu', 'tanh'])\n",
    "    activation_choice_2 = trial.suggest_categorical('activation_2', ['relu', 'leaky_relu', 'elu', 'silu', 'tanh'])\n",
    "    \n",
    "    activation_0 = get_choice(activation_choice_0)\n",
    "    activation_1 = get_choice(activation_choice_1)\n",
    "    activation_2 = get_choice(activation_choice_2)\n",
    "    \n",
    "    input_features = X_train.shape[1]\n",
    "    \n",
    "    neurons_h1 = trial.suggest_categorical('neurons_h1', [\n",
    "        max(32, int(input_features * 0.5)), \n",
    "        max(32, int(input_features * 0.75)),\n",
    "        max(32, int(input_features * 1.0)),\n",
    "        max(32, int(input_features * 1.25)),\n",
    "        max(32, int(input_features * 1.5))\n",
    "    ])\n",
    "    \n",
    "    # Neurons for Hidden Layer 2 (H2) - descending from H1\n",
    "    # Using suggest_int with dynamic high bound based on neurons_h1\n",
    "    # Ensure low is less than or equal to high.\n",
    "    low_h2 = max(16, int(neurons_h1 * 0.25))\n",
    "    high_h2 = neurons_h1\n",
    "    if low_h2 > high_h2: # Ensure low is not greater than high\n",
    "        low_h2 = high_h2 \n",
    "    step_h2 = max(1, (high_h2 - low_h2) // 4) if high_h2 > low_h2 else 1 # Ensure step is at least 1\n",
    "    if high_h2 == low_h2: # If low and high are same, suggest_int might error with step > 1\n",
    "        neurons_h2 = high_h2\n",
    "    else:\n",
    "        neurons_h2 = trial.suggest_int('neurons_h2', low=low_h2, high=high_h2, step=max(1, step_h2 // 4 * 4 if step_h2 > 4 else step_h2)) # Ensure step is reasonable\n",
    "\n",
    "    # Neurons for Hidden Layer 3 (H3) - descending from H2\n",
    "    # Using suggest_int with dynamic high bound based on neurons_h2\n",
    "    # Ensure low is less than or equal to high.\n",
    "    low_h3 = max(8, int(neurons_h2 * 0.25))\n",
    "    high_h3 = neurons_h2\n",
    "    if low_h3 > high_h3: # Ensure low is not greater than high\n",
    "        low_h3 = high_h3\n",
    "    step_h3 = max(1, (high_h3 - low_h3) // 4) if high_h3 > low_h3 else 1 # Ensure step is at least 1\n",
    "    if high_h3 == low_h3:\n",
    "        neurons_h3 = high_h3\n",
    "    else:\n",
    "        neurons_h3 = trial.suggest_int('neurons_h3', low=low_h3, high=high_h3, step=max(1, step_h3 // 2 * 2 if step_h3 > 2 else step_h3))\n",
    "    \n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(input_features, neurons_h1),\n",
    "        activation_0,\n",
    "        nn.Linear(neurons_h1, neurons_h2),\n",
    "        activation_1,\n",
    "        nn.Linear(neurons_h2, neurons_h3),\n",
    "        activation_2,\n",
    "        nn.Linear(neurons_h3, 1)\n",
    "    )\n",
    "    \n",
    "    _optimizer = torch.optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=adam_lr,\n",
    "        betas=(adam_beta1, adam_beta2),\n",
    "        eps=adam_epsilon\n",
    "    )\n",
    "    \n",
    "    model.to(device)\n",
    "        \n",
    "    for epoch in range(num_epochs):\n",
    "        # --- Training Phase ---\n",
    "        model.train() # Set model to training mode (enables dropout, batchnorm updates)\n",
    "        running_train_loss = 0.0\n",
    "        for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "            # Move batch data to the target device (GPU or CPU)\n",
    "            features, targets = features.to(device), targets.to(device)\n",
    "            _optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            # Reshape the targets tensor to match the outputs shape ([batch_size, 1])\n",
    "            targets_reshaped = targets.unsqueeze(1)\n",
    "            loss = loss_func(outputs, targets_reshaped)\n",
    "            loss.backward()\n",
    "            _optimizer.step()\n",
    "    \n",
    "            running_train_loss += loss.item() * features.size(0)\n",
    "    \n",
    "        epoch_train_loss = running_train_loss / len(train_loader.dataset)\n",
    "    \n",
    "        # --- Validation Phase ---\n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for features, targets in val_loader:\n",
    "                features, targets = features.to(device), targets.to(device)\n",
    "                outputs = model(features)\n",
    "                targets_reshaped = targets.unsqueeze(1)\n",
    "                loss = loss_func(outputs, targets_reshaped)\n",
    "                running_val_loss += loss.item() * features.size(0)\n",
    "    \n",
    "        epoch_val_loss = running_val_loss / len(val_loader.dataset)\n",
    "        \n",
    "        trial.report(epoch_val_loss, epoch) # Report intermediate value for pruning\n",
    "        if trial.should_prune():\n",
    "            print(f\"Trial {trial.number} pruned at epoch {epoch+1}.\")\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "        \n",
    "        return epoch_val_loss\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction='minimize', \n",
    "    pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=150, interval_steps=10) \n",
    "    # n_startup_trials: Don't prune first 5 trials.\n",
    "    # n_warmup_steps: Don't prune a trial before it has completed 50 epochs.\n",
    "    # interval_steps: Check for pruning every 10 epochs after warmup.\n",
    ")\n",
    "\n",
    "# 2. Run the optimization.\n",
    "#    Optuna will call your 'objective' function 'n_trials' times.\n",
    "#    Each time, it passes a 'trial' object to your function.\n",
    "study.optimize(objective, n_trials=4000)\n",
    "\n",
    "# 3. Get the best results.\n",
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(study.get_trials(states=[optuna.trial.TrialState.PRUNED])))\n",
    "print(\"  Number of complete trials: \", len(study.get_trials(states=[optuna.trial.TrialState.COMPLETE])))\n",
    "\n",
    "print(\"\\nBest trial:\")\n",
    "best_trial = study.best_trial\n",
    "\n",
    "print(\"  Value (Min Validation Loss): \", best_trial.value)\n",
    "\n",
    "print(\"  Best hyperparameters: \")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "    # Print progress (e.g., every epoch or every few epochs)\n",
    "#    print(f'Epoch {epoch+1}/{num_epochs} | Train Loss: {epoch_train_loss:.6f} | Val Loss: {epoch_val_loss:.6f}')"
   ],
   "id": "7b485c5b9cac3bd4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T14:56:23.837207Z",
     "start_time": "2025-05-19T14:56:13.228439Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_epochs = 500 # Hyperparameter: How many times to iterate over the dataset\n",
    "\n",
    "best_validation_loss = float('inf')\n",
    "best_model_path = \"best_model_weights.pth\"\n",
    "patience_counter = 0\n",
    "patience_epochs = 50\n",
    "\n",
    "print(\"\\nStarting Training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    # --- Training Phase ---\n",
    "    model.train() # Set model to training mode (enables dropout, batchnorm updates)\n",
    "    running_train_loss = 0.0\n",
    "    for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "        # Move batch data to the target device (GPU or CPU)\n",
    "        features, targets = features.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features)\n",
    "        # Reshape the targets tensor to match the outputs shape ([batch_size, 1])\n",
    "        targets_reshaped = targets.unsqueeze(1)\n",
    "        loss = loss_func(outputs, targets_reshaped)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_train_loss += loss.item() * features.size(0)\n",
    "\n",
    "    epoch_train_loss = running_train_loss / len(train_loader.dataset)\n",
    "\n",
    "    # --- Validation Phase ---\n",
    "    model.eval() # Set model to evaluation mode (disables dropout, batchnorm updates)\n",
    "    running_val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad(): # No need to calculate gradients during validation\n",
    "        for features, targets in val_loader:\n",
    "            features, targets = features.to(device), targets.to(device)\n",
    "            outputs = model(features)\n",
    "            targets_reshaped = targets.unsqueeze(1)\n",
    "            loss = loss_func(outputs, targets_reshaped)\n",
    "            running_val_loss += loss.item() * features.size(0)\n",
    "\n",
    "    epoch_val_loss = running_val_loss / len(val_loader.dataset)\n",
    "\n",
    "    # Print progress (e.g., every epoch or every few epochs)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs} | Train Loss: {epoch_train_loss:.6f} | Val Loss: {epoch_val_loss:.6f}')\n",
    "    if epoch_val_loss < best_validation_loss:\n",
    "        print(f\"Validation loss improved from {best_validation_loss:.6f} to {epoch_val_loss:.6f}. Saving model to {best_model_path}\")\n",
    "        best_validation_loss = epoch_val_loss\n",
    "        torch.save(model.state_dict(), best_model_path) # Save the model's weights\n",
    "        patience_counter = 0  # Reset patience since we found a better model\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    if patience_counter >= patience_epochs:\n",
    "        print(f\"Early stopping triggered after {patience_epochs} epochs without improvement.\")\n",
    "        break\n"
   ],
   "id": "e7e1f1e79bbfce6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Training...\n",
      "Epoch 1/500 | Train Loss: 3194084.481544 | Val Loss: 31.439160\n",
      "Validation loss improved from inf to 31.439160. Saving model to best_model_weights.pth\n",
      "Epoch 2/500 | Train Loss: 5.767370 | Val Loss: 3.082345\n",
      "Validation loss improved from 31.439160 to 3.082345. Saving model to best_model_weights.pth\n",
      "Epoch 3/500 | Train Loss: 0.848507 | Val Loss: 0.130477\n",
      "Validation loss improved from 3.082345 to 0.130477. Saving model to best_model_weights.pth\n",
      "Epoch 4/500 | Train Loss: 0.108597 | Val Loss: 0.122169\n",
      "Validation loss improved from 0.130477 to 0.122169. Saving model to best_model_weights.pth\n",
      "Epoch 5/500 | Train Loss: 0.097866 | Val Loss: 0.121590\n",
      "Validation loss improved from 0.122169 to 0.121590. Saving model to best_model_weights.pth\n",
      "Epoch 6/500 | Train Loss: 0.097812 | Val Loss: 0.120879\n",
      "Validation loss improved from 0.121590 to 0.120879. Saving model to best_model_weights.pth\n",
      "Epoch 7/500 | Train Loss: 0.097853 | Val Loss: 0.121870\n",
      "Epoch 8/500 | Train Loss: 0.097621 | Val Loss: 0.120871\n",
      "Validation loss improved from 0.120879 to 0.120871. Saving model to best_model_weights.pth\n",
      "Epoch 9/500 | Train Loss: 0.097931 | Val Loss: 0.122195\n",
      "Epoch 10/500 | Train Loss: 0.097856 | Val Loss: 0.121886\n",
      "Epoch 11/500 | Train Loss: 0.098118 | Val Loss: 0.121709\n",
      "Epoch 12/500 | Train Loss: 0.097915 | Val Loss: 0.123467\n",
      "Epoch 13/500 | Train Loss: 0.098050 | Val Loss: 0.121035\n",
      "Epoch 14/500 | Train Loss: 0.097963 | Val Loss: 0.121808\n",
      "Epoch 15/500 | Train Loss: 0.097795 | Val Loss: 0.121016\n",
      "Epoch 16/500 | Train Loss: 0.097852 | Val Loss: 0.121244\n",
      "Epoch 17/500 | Train Loss: 0.097933 | Val Loss: 0.121199\n",
      "Epoch 18/500 | Train Loss: 0.097911 | Val Loss: 0.121907\n",
      "Epoch 19/500 | Train Loss: 0.097782 | Val Loss: 0.121522\n",
      "Epoch 20/500 | Train Loss: 0.097900 | Val Loss: 0.125433\n",
      "Epoch 21/500 | Train Loss: 0.098127 | Val Loss: 0.123189\n",
      "Epoch 22/500 | Train Loss: 0.097810 | Val Loss: 0.120706\n",
      "Validation loss improved from 0.120871 to 0.120706. Saving model to best_model_weights.pth\n",
      "Epoch 23/500 | Train Loss: 0.098069 | Val Loss: 0.120781\n",
      "Epoch 24/500 | Train Loss: 0.097878 | Val Loss: 0.121538\n",
      "Epoch 25/500 | Train Loss: 0.097887 | Val Loss: 0.120597\n",
      "Validation loss improved from 0.120706 to 0.120597. Saving model to best_model_weights.pth\n",
      "Epoch 26/500 | Train Loss: 0.099259 | Val Loss: 0.120530\n",
      "Validation loss improved from 0.120597 to 0.120530. Saving model to best_model_weights.pth\n",
      "Epoch 27/500 | Train Loss: 0.098723 | Val Loss: 0.120638\n",
      "Epoch 28/500 | Train Loss: 0.097767 | Val Loss: 0.120663\n",
      "Epoch 29/500 | Train Loss: 0.098115 | Val Loss: 0.121335\n",
      "Epoch 30/500 | Train Loss: 0.098198 | Val Loss: 0.121056\n",
      "Epoch 31/500 | Train Loss: 0.097802 | Val Loss: 0.124571\n",
      "Epoch 32/500 | Train Loss: 0.098104 | Val Loss: 0.122070\n",
      "Epoch 33/500 | Train Loss: 0.098271 | Val Loss: 0.121962\n",
      "Epoch 34/500 | Train Loss: 0.098418 | Val Loss: 0.121783\n",
      "Epoch 35/500 | Train Loss: 0.097656 | Val Loss: 0.125685\n",
      "Epoch 36/500 | Train Loss: 0.099619 | Val Loss: 0.123871\n",
      "Epoch 37/500 | Train Loss: 0.098923 | Val Loss: 0.120997\n",
      "Epoch 38/500 | Train Loss: 0.098112 | Val Loss: 0.124195\n",
      "Epoch 39/500 | Train Loss: 0.097645 | Val Loss: 0.121470\n",
      "Epoch 40/500 | Train Loss: 0.097575 | Val Loss: 0.120474\n",
      "Validation loss improved from 0.120530 to 0.120474. Saving model to best_model_weights.pth\n",
      "Epoch 41/500 | Train Loss: 0.097494 | Val Loss: 0.126076\n",
      "Epoch 42/500 | Train Loss: 0.099058 | Val Loss: 0.123554\n",
      "Epoch 43/500 | Train Loss: 0.098139 | Val Loss: 0.120614\n",
      "Epoch 44/500 | Train Loss: 0.098016 | Val Loss: 0.122078\n",
      "Epoch 45/500 | Train Loss: 0.098166 | Val Loss: 0.121949\n",
      "Epoch 46/500 | Train Loss: 0.098111 | Val Loss: 0.120511\n",
      "Epoch 47/500 | Train Loss: 0.097637 | Val Loss: 0.128027\n",
      "Epoch 48/500 | Train Loss: 0.098079 | Val Loss: 0.121230\n",
      "Epoch 49/500 | Train Loss: 0.098114 | Val Loss: 0.120438\n",
      "Validation loss improved from 0.120474 to 0.120438. Saving model to best_model_weights.pth\n",
      "Epoch 50/500 | Train Loss: 0.098769 | Val Loss: 0.121758\n",
      "Epoch 51/500 | Train Loss: 0.097863 | Val Loss: 0.120791\n",
      "Epoch 52/500 | Train Loss: 0.097926 | Val Loss: 0.125148\n",
      "Epoch 53/500 | Train Loss: 0.098250 | Val Loss: 0.123020\n",
      "Epoch 54/500 | Train Loss: 0.098217 | Val Loss: 0.125534\n",
      "Epoch 55/500 | Train Loss: 0.098933 | Val Loss: 0.123454\n",
      "Epoch 56/500 | Train Loss: 0.098495 | Val Loss: 0.120278\n",
      "Validation loss improved from 0.120438 to 0.120278. Saving model to best_model_weights.pth\n",
      "Epoch 57/500 | Train Loss: 0.099255 | Val Loss: 0.124458\n",
      "Epoch 58/500 | Train Loss: 0.098105 | Val Loss: 0.123072\n",
      "Epoch 59/500 | Train Loss: 0.099199 | Val Loss: 0.128925\n",
      "Epoch 60/500 | Train Loss: 0.098046 | Val Loss: 0.122662\n",
      "Epoch 61/500 | Train Loss: 0.098030 | Val Loss: 0.120311\n",
      "Epoch 62/500 | Train Loss: 0.098723 | Val Loss: 0.121326\n",
      "Epoch 63/500 | Train Loss: 0.098007 | Val Loss: 0.120655\n",
      "Epoch 64/500 | Train Loss: 0.098613 | Val Loss: 0.134434\n",
      "Epoch 65/500 | Train Loss: 0.100166 | Val Loss: 0.120674\n",
      "Epoch 66/500 | Train Loss: 0.098214 | Val Loss: 0.122982\n",
      "Epoch 67/500 | Train Loss: 0.099660 | Val Loss: 0.127650\n",
      "Epoch 68/500 | Train Loss: 0.100036 | Val Loss: 0.124577\n",
      "Epoch 69/500 | Train Loss: 0.098892 | Val Loss: 0.120709\n",
      "Epoch 70/500 | Train Loss: 0.097994 | Val Loss: 0.126578\n",
      "Epoch 71/500 | Train Loss: 0.099440 | Val Loss: 0.120387\n",
      "Epoch 72/500 | Train Loss: 0.098171 | Val Loss: 0.131109\n",
      "Epoch 73/500 | Train Loss: 0.100214 | Val Loss: 0.122389\n",
      "Epoch 74/500 | Train Loss: 0.098174 | Val Loss: 0.143380\n",
      "Epoch 75/500 | Train Loss: 0.107093 | Val Loss: 0.130790\n",
      "Epoch 76/500 | Train Loss: 0.101502 | Val Loss: 0.122046\n",
      "Epoch 77/500 | Train Loss: 0.097882 | Val Loss: 0.120351\n",
      "Epoch 78/500 | Train Loss: 0.098912 | Val Loss: 0.121230\n",
      "Epoch 79/500 | Train Loss: 0.098777 | Val Loss: 0.120411\n",
      "Epoch 80/500 | Train Loss: 0.100284 | Val Loss: 0.120564\n",
      "Epoch 81/500 | Train Loss: 0.098162 | Val Loss: 0.121565\n",
      "Epoch 82/500 | Train Loss: 0.096990 | Val Loss: 0.120817\n",
      "Epoch 83/500 | Train Loss: 0.098951 | Val Loss: 0.120783\n",
      "Epoch 84/500 | Train Loss: 0.101413 | Val Loss: 0.126870\n",
      "Epoch 85/500 | Train Loss: 0.099397 | Val Loss: 0.121372\n",
      "Epoch 86/500 | Train Loss: 0.098866 | Val Loss: 0.120237\n",
      "Validation loss improved from 0.120278 to 0.120237. Saving model to best_model_weights.pth\n",
      "Epoch 87/500 | Train Loss: 0.100966 | Val Loss: 0.121908\n",
      "Epoch 88/500 | Train Loss: 0.098248 | Val Loss: 0.143579\n",
      "Epoch 89/500 | Train Loss: 0.100310 | Val Loss: 0.125420\n",
      "Epoch 90/500 | Train Loss: 0.100516 | Val Loss: 0.120112\n",
      "Validation loss improved from 0.120237 to 0.120112. Saving model to best_model_weights.pth\n",
      "Epoch 91/500 | Train Loss: 0.098016 | Val Loss: 0.124989\n",
      "Epoch 92/500 | Train Loss: 0.097916 | Val Loss: 0.124362\n",
      "Epoch 93/500 | Train Loss: 0.101649 | Val Loss: 0.120112\n",
      "Epoch 94/500 | Train Loss: 0.099515 | Val Loss: 0.121168\n",
      "Epoch 95/500 | Train Loss: 0.098492 | Val Loss: 0.123804\n",
      "Epoch 96/500 | Train Loss: 0.098677 | Val Loss: 0.120133\n",
      "Epoch 97/500 | Train Loss: 0.098522 | Val Loss: 0.124096\n",
      "Epoch 98/500 | Train Loss: 0.100129 | Val Loss: 0.125985\n",
      "Epoch 99/500 | Train Loss: 0.099411 | Val Loss: 0.120818\n",
      "Epoch 100/500 | Train Loss: 0.102014 | Val Loss: 0.120011\n",
      "Validation loss improved from 0.120112 to 0.120011. Saving model to best_model_weights.pth\n",
      "Epoch 101/500 | Train Loss: 0.100269 | Val Loss: 0.120342\n",
      "Epoch 102/500 | Train Loss: 0.100280 | Val Loss: 0.121262\n",
      "Epoch 103/500 | Train Loss: 0.099002 | Val Loss: 0.120982\n",
      "Epoch 104/500 | Train Loss: 0.100505 | Val Loss: 0.120018\n",
      "Epoch 105/500 | Train Loss: 0.100789 | Val Loss: 0.141524\n",
      "Epoch 106/500 | Train Loss: 0.100178 | Val Loss: 0.120583\n",
      "Epoch 107/500 | Train Loss: 0.100265 | Val Loss: 0.124233\n",
      "Epoch 108/500 | Train Loss: 0.098900 | Val Loss: 0.120210\n",
      "Epoch 109/500 | Train Loss: 0.102699 | Val Loss: 0.120327\n",
      "Epoch 110/500 | Train Loss: 0.103900 | Val Loss: 0.124538\n",
      "Epoch 111/500 | Train Loss: 0.098148 | Val Loss: 0.119891\n",
      "Validation loss improved from 0.120011 to 0.119891. Saving model to best_model_weights.pth\n",
      "Epoch 112/500 | Train Loss: 0.100402 | Val Loss: 0.133044\n",
      "Epoch 113/500 | Train Loss: 0.100756 | Val Loss: 0.128612\n",
      "Epoch 114/500 | Train Loss: 0.099808 | Val Loss: 0.119853\n",
      "Validation loss improved from 0.119891 to 0.119853. Saving model to best_model_weights.pth\n",
      "Epoch 115/500 | Train Loss: 0.101361 | Val Loss: 0.126781\n",
      "Epoch 116/500 | Train Loss: 0.102856 | Val Loss: 0.120745\n",
      "Epoch 117/500 | Train Loss: 0.101225 | Val Loss: 0.124399\n",
      "Epoch 118/500 | Train Loss: 0.099644 | Val Loss: 0.124251\n",
      "Epoch 119/500 | Train Loss: 0.101200 | Val Loss: 0.124462\n",
      "Epoch 120/500 | Train Loss: 0.100972 | Val Loss: 0.195704\n",
      "Epoch 121/500 | Train Loss: 0.108575 | Val Loss: 0.120721\n",
      "Epoch 122/500 | Train Loss: 0.099020 | Val Loss: 0.119804\n",
      "Validation loss improved from 0.119853 to 0.119804. Saving model to best_model_weights.pth\n",
      "Epoch 123/500 | Train Loss: 0.097527 | Val Loss: 0.120322\n",
      "Epoch 124/500 | Train Loss: 0.106357 | Val Loss: 0.197448\n",
      "Epoch 125/500 | Train Loss: 0.114443 | Val Loss: 0.190003\n",
      "Epoch 126/500 | Train Loss: 0.105070 | Val Loss: 0.120060\n",
      "Epoch 127/500 | Train Loss: 0.099249 | Val Loss: 0.143557\n",
      "Epoch 128/500 | Train Loss: 0.104772 | Val Loss: 0.120532\n",
      "Epoch 129/500 | Train Loss: 0.098896 | Val Loss: 0.133252\n",
      "Epoch 130/500 | Train Loss: 0.101560 | Val Loss: 0.162575\n",
      "Epoch 131/500 | Train Loss: 0.118354 | Val Loss: 0.149476\n",
      "Epoch 132/500 | Train Loss: 0.106606 | Val Loss: 0.120057\n",
      "Epoch 133/500 | Train Loss: 0.102680 | Val Loss: 0.127407\n",
      "Epoch 134/500 | Train Loss: 0.104550 | Val Loss: 0.134986\n",
      "Epoch 135/500 | Train Loss: 0.101498 | Val Loss: 0.120590\n",
      "Epoch 136/500 | Train Loss: 0.102943 | Val Loss: 0.188494\n",
      "Epoch 137/500 | Train Loss: 0.112924 | Val Loss: 0.217859\n",
      "Epoch 138/500 | Train Loss: 0.116285 | Val Loss: 0.145826\n",
      "Epoch 139/500 | Train Loss: 0.103060 | Val Loss: 0.119599\n",
      "Validation loss improved from 0.119804 to 0.119599. Saving model to best_model_weights.pth\n",
      "Epoch 140/500 | Train Loss: 0.103179 | Val Loss: 0.167709\n",
      "Epoch 141/500 | Train Loss: 0.108336 | Val Loss: 0.120395\n",
      "Epoch 142/500 | Train Loss: 0.099548 | Val Loss: 0.119846\n",
      "Epoch 143/500 | Train Loss: 0.107931 | Val Loss: 0.140713\n",
      "Epoch 144/500 | Train Loss: 0.123076 | Val Loss: 0.273907\n",
      "Epoch 145/500 | Train Loss: 0.123563 | Val Loss: 0.312372\n",
      "Epoch 146/500 | Train Loss: 0.118111 | Val Loss: 0.145062\n",
      "Epoch 147/500 | Train Loss: 0.104078 | Val Loss: 0.119307\n",
      "Validation loss improved from 0.119599 to 0.119307. Saving model to best_model_weights.pth\n",
      "Epoch 148/500 | Train Loss: 0.101249 | Val Loss: 0.132001\n",
      "Epoch 149/500 | Train Loss: 0.098894 | Val Loss: 0.120598\n",
      "Epoch 150/500 | Train Loss: 0.102061 | Val Loss: 0.166619\n",
      "Epoch 151/500 | Train Loss: 0.110064 | Val Loss: 0.119120\n",
      "Validation loss improved from 0.119307 to 0.119120. Saving model to best_model_weights.pth\n",
      "Epoch 152/500 | Train Loss: 0.105306 | Val Loss: 0.131236\n",
      "Epoch 153/500 | Train Loss: 0.114052 | Val Loss: 0.122716\n",
      "Epoch 154/500 | Train Loss: 0.101110 | Val Loss: 0.122897\n",
      "Epoch 155/500 | Train Loss: 0.103916 | Val Loss: 0.140237\n",
      "Epoch 156/500 | Train Loss: 0.113384 | Val Loss: 0.119095\n",
      "Validation loss improved from 0.119120 to 0.119095. Saving model to best_model_weights.pth\n",
      "Epoch 157/500 | Train Loss: 0.108202 | Val Loss: 0.138160\n",
      "Epoch 158/500 | Train Loss: 0.105137 | Val Loss: 0.258675\n",
      "Epoch 159/500 | Train Loss: 0.115163 | Val Loss: 0.173719\n",
      "Epoch 160/500 | Train Loss: 0.118265 | Val Loss: 0.120181\n",
      "Epoch 161/500 | Train Loss: 0.118245 | Val Loss: 0.140831\n",
      "Epoch 162/500 | Train Loss: 0.109140 | Val Loss: 0.120441\n",
      "Epoch 163/500 | Train Loss: 0.104755 | Val Loss: 0.122755\n",
      "Epoch 164/500 | Train Loss: 0.109680 | Val Loss: 0.119209\n",
      "Epoch 165/500 | Train Loss: 0.111995 | Val Loss: 0.121870\n",
      "Epoch 166/500 | Train Loss: 0.140277 | Val Loss: 0.183073\n",
      "Epoch 167/500 | Train Loss: 0.171952 | Val Loss: 0.130044\n",
      "Epoch 168/500 | Train Loss: 0.101659 | Val Loss: 0.119972\n",
      "Epoch 169/500 | Train Loss: 0.107157 | Val Loss: 0.157726\n",
      "Epoch 170/500 | Train Loss: 0.114478 | Val Loss: 0.149195\n",
      "Epoch 171/500 | Train Loss: 0.145287 | Val Loss: 0.139369\n",
      "Epoch 172/500 | Train Loss: 0.143134 | Val Loss: 0.123035\n",
      "Epoch 173/500 | Train Loss: 0.115021 | Val Loss: 0.120624\n",
      "Epoch 174/500 | Train Loss: 0.214526 | Val Loss: 0.134777\n",
      "Epoch 175/500 | Train Loss: 0.102454 | Val Loss: 0.136469\n",
      "Epoch 176/500 | Train Loss: 0.161467 | Val Loss: 0.203676\n",
      "Epoch 177/500 | Train Loss: 0.127327 | Val Loss: 0.129672\n",
      "Epoch 178/500 | Train Loss: 0.163100 | Val Loss: 0.164357\n",
      "Epoch 179/500 | Train Loss: 0.146694 | Val Loss: 0.187204\n",
      "Epoch 180/500 | Train Loss: 0.135489 | Val Loss: 0.143594\n",
      "Epoch 181/500 | Train Loss: 0.112803 | Val Loss: 0.168373\n",
      "Epoch 182/500 | Train Loss: 0.115118 | Val Loss: 0.144789\n",
      "Epoch 183/500 | Train Loss: 0.125037 | Val Loss: 0.117849\n",
      "Validation loss improved from 0.119095 to 0.117849. Saving model to best_model_weights.pth\n",
      "Epoch 184/500 | Train Loss: 0.147989 | Val Loss: 0.118338\n",
      "Epoch 185/500 | Train Loss: 0.140269 | Val Loss: 0.137338\n",
      "Epoch 186/500 | Train Loss: 0.135451 | Val Loss: 0.213400\n",
      "Epoch 187/500 | Train Loss: 0.188712 | Val Loss: 0.154890\n",
      "Epoch 188/500 | Train Loss: 0.178565 | Val Loss: 0.139013\n",
      "Epoch 189/500 | Train Loss: 0.199959 | Val Loss: 0.289572\n",
      "Epoch 190/500 | Train Loss: 0.175909 | Val Loss: 0.791391\n",
      "Epoch 191/500 | Train Loss: 0.350736 | Val Loss: 0.247783\n",
      "Epoch 192/500 | Train Loss: 0.173485 | Val Loss: 0.215580\n",
      "Epoch 193/500 | Train Loss: 0.213593 | Val Loss: 0.119863\n",
      "Epoch 194/500 | Train Loss: 0.125482 | Val Loss: 0.131339\n",
      "Epoch 195/500 | Train Loss: 0.149565 | Val Loss: 0.124791\n",
      "Epoch 196/500 | Train Loss: 0.164987 | Val Loss: 0.599402\n",
      "Epoch 197/500 | Train Loss: 0.382032 | Val Loss: 0.134536\n",
      "Epoch 198/500 | Train Loss: 0.173184 | Val Loss: 0.213528\n",
      "Epoch 199/500 | Train Loss: 0.247421 | Val Loss: 0.433145\n",
      "Epoch 200/500 | Train Loss: 0.186918 | Val Loss: 0.121608\n",
      "Epoch 201/500 | Train Loss: 0.126865 | Val Loss: 0.119281\n",
      "Epoch 202/500 | Train Loss: 0.141815 | Val Loss: 0.136239\n",
      "Epoch 203/500 | Train Loss: 0.148621 | Val Loss: 0.316015\n",
      "Epoch 204/500 | Train Loss: 0.127251 | Val Loss: 0.120297\n",
      "Epoch 205/500 | Train Loss: 0.161730 | Val Loss: 0.139509\n",
      "Epoch 206/500 | Train Loss: 0.158969 | Val Loss: 0.259261\n",
      "Epoch 207/500 | Train Loss: 0.160185 | Val Loss: 0.186057\n",
      "Epoch 208/500 | Train Loss: 0.171451 | Val Loss: 0.167556\n",
      "Epoch 209/500 | Train Loss: 0.143925 | Val Loss: 0.183820\n",
      "Epoch 210/500 | Train Loss: 0.324008 | Val Loss: 1.128623\n",
      "Epoch 211/500 | Train Loss: 0.341477 | Val Loss: 0.179910\n",
      "Epoch 212/500 | Train Loss: 0.119513 | Val Loss: 0.261652\n",
      "Epoch 213/500 | Train Loss: 0.148254 | Val Loss: 0.127233\n",
      "Epoch 214/500 | Train Loss: 0.161634 | Val Loss: 0.399858\n",
      "Epoch 215/500 | Train Loss: 0.218675 | Val Loss: 0.369842\n",
      "Epoch 216/500 | Train Loss: 0.134296 | Val Loss: 0.346714\n",
      "Epoch 217/500 | Train Loss: 0.276621 | Val Loss: 0.578780\n",
      "Epoch 218/500 | Train Loss: 0.232330 | Val Loss: 0.142306\n",
      "Epoch 219/500 | Train Loss: 0.148687 | Val Loss: 0.437502\n",
      "Epoch 220/500 | Train Loss: 0.223197 | Val Loss: 0.115750\n",
      "Validation loss improved from 0.117849 to 0.115750. Saving model to best_model_weights.pth\n",
      "Epoch 221/500 | Train Loss: 0.169342 | Val Loss: 0.133476\n",
      "Epoch 222/500 | Train Loss: 0.195395 | Val Loss: 0.252140\n",
      "Epoch 223/500 | Train Loss: 0.343525 | Val Loss: 0.669697\n",
      "Epoch 224/500 | Train Loss: 0.272768 | Val Loss: 0.136842\n",
      "Epoch 225/500 | Train Loss: 0.154043 | Val Loss: 0.119648\n",
      "Epoch 226/500 | Train Loss: 0.126346 | Val Loss: 0.129721\n",
      "Epoch 227/500 | Train Loss: 0.140916 | Val Loss: 0.190792\n",
      "Epoch 228/500 | Train Loss: 0.140555 | Val Loss: 0.113872\n",
      "Validation loss improved from 0.115750 to 0.113872. Saving model to best_model_weights.pth\n",
      "Epoch 229/500 | Train Loss: 0.192781 | Val Loss: 0.181011\n",
      "Epoch 230/500 | Train Loss: 0.175571 | Val Loss: 0.117011\n",
      "Epoch 231/500 | Train Loss: 0.203161 | Val Loss: 0.142404\n",
      "Epoch 232/500 | Train Loss: 0.107282 | Val Loss: 0.266978\n",
      "Epoch 233/500 | Train Loss: 0.208114 | Val Loss: 0.445614\n",
      "Epoch 234/500 | Train Loss: 0.194776 | Val Loss: 0.363553\n",
      "Epoch 235/500 | Train Loss: 0.166639 | Val Loss: 0.112957\n",
      "Validation loss improved from 0.113872 to 0.112957. Saving model to best_model_weights.pth\n",
      "Epoch 236/500 | Train Loss: 0.154406 | Val Loss: 0.419362\n",
      "Epoch 237/500 | Train Loss: 0.160374 | Val Loss: 0.143881\n",
      "Epoch 238/500 | Train Loss: 0.137163 | Val Loss: 0.153923\n",
      "Epoch 239/500 | Train Loss: 0.121560 | Val Loss: 0.147112\n",
      "Epoch 240/500 | Train Loss: 0.114605 | Val Loss: 0.177537\n",
      "Epoch 241/500 | Train Loss: 0.182408 | Val Loss: 0.158245\n",
      "Epoch 242/500 | Train Loss: 0.233538 | Val Loss: 0.195260\n",
      "Epoch 243/500 | Train Loss: 0.312734 | Val Loss: 0.141748\n",
      "Epoch 244/500 | Train Loss: 0.116647 | Val Loss: 0.164005\n",
      "Epoch 245/500 | Train Loss: 0.139586 | Val Loss: 0.264723\n",
      "Epoch 246/500 | Train Loss: 0.214295 | Val Loss: 0.127585\n",
      "Epoch 247/500 | Train Loss: 0.123415 | Val Loss: 0.536478\n",
      "Epoch 248/500 | Train Loss: 0.259485 | Val Loss: 0.115391\n",
      "Epoch 249/500 | Train Loss: 0.164333 | Val Loss: 0.120572\n",
      "Epoch 250/500 | Train Loss: 0.139931 | Val Loss: 0.127855\n",
      "Epoch 251/500 | Train Loss: 0.132157 | Val Loss: 0.134059\n",
      "Epoch 252/500 | Train Loss: 0.104352 | Val Loss: 0.109889\n",
      "Validation loss improved from 0.112957 to 0.109889. Saving model to best_model_weights.pth\n",
      "Epoch 253/500 | Train Loss: 0.124350 | Val Loss: 0.335683\n",
      "Epoch 254/500 | Train Loss: 0.191698 | Val Loss: 0.250690\n",
      "Epoch 255/500 | Train Loss: 0.201412 | Val Loss: 0.143904\n",
      "Epoch 256/500 | Train Loss: 0.124479 | Val Loss: 0.109560\n",
      "Validation loss improved from 0.109889 to 0.109560. Saving model to best_model_weights.pth\n",
      "Epoch 257/500 | Train Loss: 0.105385 | Val Loss: 0.141429\n",
      "Epoch 258/500 | Train Loss: 0.118012 | Val Loss: 0.113252\n",
      "Epoch 259/500 | Train Loss: 0.166735 | Val Loss: 0.109752\n",
      "Epoch 260/500 | Train Loss: 0.135945 | Val Loss: 0.158261\n",
      "Epoch 261/500 | Train Loss: 0.142677 | Val Loss: 0.179108\n",
      "Epoch 262/500 | Train Loss: 0.128726 | Val Loss: 0.287383\n",
      "Epoch 263/500 | Train Loss: 0.118315 | Val Loss: 0.112425\n",
      "Epoch 264/500 | Train Loss: 0.221153 | Val Loss: 0.109089\n",
      "Validation loss improved from 0.109560 to 0.109089. Saving model to best_model_weights.pth\n",
      "Epoch 265/500 | Train Loss: 0.150997 | Val Loss: 0.413976\n",
      "Epoch 266/500 | Train Loss: 2.765183 | Val Loss: 8.197364\n",
      "Epoch 267/500 | Train Loss: 1.316801 | Val Loss: 0.132618\n",
      "Epoch 268/500 | Train Loss: 0.110385 | Val Loss: 0.126420\n",
      "Epoch 269/500 | Train Loss: 0.098655 | Val Loss: 0.114002\n",
      "Epoch 270/500 | Train Loss: 0.091935 | Val Loss: 0.113010\n",
      "Epoch 271/500 | Train Loss: 0.107965 | Val Loss: 0.112713\n",
      "Epoch 272/500 | Train Loss: 0.090697 | Val Loss: 0.118359\n",
      "Epoch 273/500 | Train Loss: 0.095297 | Val Loss: 0.114796\n",
      "Epoch 274/500 | Train Loss: 0.107586 | Val Loss: 0.126636\n",
      "Epoch 275/500 | Train Loss: 0.098668 | Val Loss: 0.113998\n",
      "Epoch 276/500 | Train Loss: 0.116726 | Val Loss: 0.111373\n",
      "Epoch 277/500 | Train Loss: 0.105780 | Val Loss: 0.115332\n",
      "Epoch 278/500 | Train Loss: 0.100429 | Val Loss: 0.111371\n",
      "Epoch 279/500 | Train Loss: 0.104397 | Val Loss: 0.113324\n",
      "Epoch 280/500 | Train Loss: 0.093764 | Val Loss: 0.148329\n",
      "Epoch 281/500 | Train Loss: 0.113536 | Val Loss: 0.112131\n",
      "Epoch 282/500 | Train Loss: 0.094384 | Val Loss: 0.109399\n",
      "Epoch 283/500 | Train Loss: 0.111099 | Val Loss: 0.186188\n",
      "Epoch 284/500 | Train Loss: 0.122887 | Val Loss: 0.108791\n",
      "Validation loss improved from 0.109089 to 0.108791. Saving model to best_model_weights.pth\n",
      "Epoch 285/500 | Train Loss: 0.141327 | Val Loss: 0.117964\n",
      "Epoch 286/500 | Train Loss: 0.089887 | Val Loss: 0.128981\n",
      "Epoch 287/500 | Train Loss: 0.111809 | Val Loss: 0.111012\n",
      "Epoch 288/500 | Train Loss: 0.092497 | Val Loss: 0.145151\n",
      "Epoch 289/500 | Train Loss: 0.142299 | Val Loss: 0.131457\n",
      "Epoch 290/500 | Train Loss: 0.093257 | Val Loss: 0.106053\n",
      "Validation loss improved from 0.108791 to 0.106053. Saving model to best_model_weights.pth\n",
      "Epoch 291/500 | Train Loss: 0.131318 | Val Loss: 0.125590\n",
      "Epoch 292/500 | Train Loss: 0.150628 | Val Loss: 0.255837\n",
      "Epoch 293/500 | Train Loss: 0.124839 | Val Loss: 0.128427\n",
      "Epoch 294/500 | Train Loss: 0.105555 | Val Loss: 0.563269\n",
      "Epoch 295/500 | Train Loss: 0.186753 | Val Loss: 0.473184\n",
      "Epoch 296/500 | Train Loss: 0.896450 | Val Loss: 10388.085710\n",
      "Epoch 297/500 | Train Loss: 1101019.827893 | Val Loss: 751.090911\n",
      "Epoch 298/500 | Train Loss: 1416.205638 | Val Loss: 980.814430\n",
      "Epoch 299/500 | Train Loss: 414.798269 | Val Loss: 79.369289\n",
      "Epoch 300/500 | Train Loss: 31.182049 | Val Loss: 21.249624\n",
      "Epoch 301/500 | Train Loss: 22.995413 | Val Loss: 10.420335\n",
      "Epoch 302/500 | Train Loss: 4.662698 | Val Loss: 1.059995\n",
      "Epoch 303/500 | Train Loss: 0.508204 | Val Loss: 0.228895\n",
      "Epoch 304/500 | Train Loss: 0.173429 | Val Loss: 0.187008\n",
      "Epoch 305/500 | Train Loss: 0.153363 | Val Loss: 0.187164\n",
      "Epoch 306/500 | Train Loss: 0.152632 | Val Loss: 0.187270\n",
      "Epoch 307/500 | Train Loss: 0.152518 | Val Loss: 0.187491\n",
      "Epoch 308/500 | Train Loss: 0.152482 | Val Loss: 0.187565\n",
      "Epoch 309/500 | Train Loss: 0.152527 | Val Loss: 0.187599\n",
      "Epoch 310/500 | Train Loss: 0.152551 | Val Loss: 0.187554\n",
      "Epoch 311/500 | Train Loss: 0.152522 | Val Loss: 0.187640\n",
      "Epoch 312/500 | Train Loss: 0.152562 | Val Loss: 0.187801\n",
      "Epoch 313/500 | Train Loss: 0.152553 | Val Loss: 0.187336\n",
      "Epoch 314/500 | Train Loss: 0.152546 | Val Loss: 0.187462\n",
      "Epoch 315/500 | Train Loss: 0.152594 | Val Loss: 0.187734\n",
      "Epoch 316/500 | Train Loss: 0.152501 | Val Loss: 0.187865\n",
      "Epoch 317/500 | Train Loss: 0.152701 | Val Loss: 0.187937\n",
      "Epoch 318/500 | Train Loss: 0.152504 | Val Loss: 0.187343\n",
      "Epoch 319/500 | Train Loss: 0.152531 | Val Loss: 0.187287\n",
      "Epoch 320/500 | Train Loss: 0.152569 | Val Loss: 0.187466\n",
      "Epoch 321/500 | Train Loss: 0.152518 | Val Loss: 0.187284\n",
      "Epoch 322/500 | Train Loss: 0.152603 | Val Loss: 0.187600\n",
      "Epoch 323/500 | Train Loss: 0.152603 | Val Loss: 0.187425\n",
      "Epoch 324/500 | Train Loss: 0.152490 | Val Loss: 0.187698\n",
      "Epoch 325/500 | Train Loss: 0.152750 | Val Loss: 0.187629\n",
      "Epoch 326/500 | Train Loss: 0.152828 | Val Loss: 0.187273\n",
      "Epoch 327/500 | Train Loss: 0.152661 | Val Loss: 0.187803\n",
      "Epoch 328/500 | Train Loss: 0.152551 | Val Loss: 0.187733\n",
      "Epoch 329/500 | Train Loss: 0.152570 | Val Loss: 0.188007\n",
      "Epoch 330/500 | Train Loss: 0.152613 | Val Loss: 0.187489\n",
      "Epoch 331/500 | Train Loss: 0.152492 | Val Loss: 0.187809\n",
      "Epoch 332/500 | Train Loss: 0.152727 | Val Loss: 0.187931\n",
      "Epoch 333/500 | Train Loss: 0.152548 | Val Loss: 0.187313\n",
      "Epoch 334/500 | Train Loss: 0.152538 | Val Loss: 0.187462\n",
      "Epoch 335/500 | Train Loss: 0.152589 | Val Loss: 0.187499\n",
      "Epoch 336/500 | Train Loss: 0.152622 | Val Loss: 0.187412\n",
      "Epoch 337/500 | Train Loss: 0.152577 | Val Loss: 0.187765\n",
      "Epoch 338/500 | Train Loss: 0.152612 | Val Loss: 0.187713\n",
      "Epoch 339/500 | Train Loss: 0.152687 | Val Loss: 0.187852\n",
      "Epoch 340/500 | Train Loss: 0.152745 | Val Loss: 0.188117\n",
      "Early stopping triggered after 50 epochs without improvement.\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Model mathematically represented, using Adam optimizer, default hyperparameters:\n",
    "$$\n",
    "\\begin{align}\n",
    "% Input\n",
    "\\mathbf{a}^{(0)} &= \\mathbf{x}, \\quad \\text{where } \\mathbf{x} \\in \\mathbb{R}^{260} \\\\\n",
    "% Layer 1\n",
    "\\mathbf{z}^{(1)} &= W^{(1)}\\mathbf{a}^{(0)} + \\mathbf{b}^{(1)}, \\quad \\text{where } W^{(1)} \\in \\mathbb{R}^{128 \\times 260}, \\mathbf{b}^{(1)} \\in \\mathbb{R}^{128} \\\\\n",
    "\\mathbf{a}^{(1)} &= \\text{ReLU}(\\mathbf{z}^{(1)}), \\quad \\mathbf{a}^{(1)} \\in \\mathbb{R}^{128} \\\\\n",
    "% Layer 2\n",
    "\\mathbf{z}^{(2)} &= W^{(2)}\\mathbf{a}^{(1)} + \\mathbf{b}^{(2)}, \\quad \\text{where } W^{(2)} \\in \\mathbb{R}^{64 \\times 128}, \\mathbf{b}^{(2)} \\in \\mathbb{R}^{64} \\\\\n",
    "\\mathbf{a}^{(2)} &= \\text{ReLU}(\\mathbf{z}^{(2)}), \\quad \\mathbf{a}^{(2)} \\in \\mathbb{R}^{64} \\\\\n",
    "% Layer 3\n",
    "\\mathbf{z}^{(3)} &= W^{(3)}\\mathbf{a}^{(2)} + \\mathbf{b}^{(3)}, \\quad \\text{where } W^{(3)} \\in \\mathbb{R}^{16 \\times 64}, \\mathbf{b}^{(3)} \\in \\mathbb{R}^{16} \\\\\n",
    "\\mathbf{a}^{(3)} &= \\text{ReLU}(\\mathbf{z}^{(3)}), \\quad \\mathbf{a}^{(3)} \\in \\mathbb{R}^{16} \\\\\n",
    "% Layer 4 (Output)\n",
    "\\mathbf{z}^{(4)} &= W^{(4)}\\mathbf{a}^{(3)} + b^{(4)}, \\quad \\text{where } W^{(4)} \\in \\mathbb{R}^{1 \\times 16}, \\mathbf{b}^{(4)} \\in \\mathbb{R} \\\\\n",
    "\\hat{y} &= \\mathbf{z}^{(4)}, \\quad \\hat{y} \\in \\mathbb{R} \\\\\n",
    "% Loss\n",
    "\\mathcal{L} &= (\\hat{y} - y_{\\text{target}})^2\n",
    "\\end{align}\n",
    "$$"
   ],
   "id": "24d8d53978c00922"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T15:03:15.820321Z",
     "start_time": "2025-05-19T15:03:15.811551Z"
    }
   },
   "cell_type": "code",
   "source": [
    "try:\n",
    "    weights = torch.load(best_model_path, map_location=torch.device('cpu'))\n",
    "    print(\"Successfully loaded entire model.\")\n",
    "    model.path = weights\n",
    "    # You can now inspect the model structure\n",
    "    print(\"\\nModel structure:\")\n",
    "    print(model)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading .pth file: {e}\")"
   ],
   "id": "ce84266fd5bb925d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded entire model.\n",
      "\n",
      "Model structure:\n",
      "OrderedDict({'0.weight': tensor([[-0.3538, -0.4171, -0.3354,  ...,  0.3727,  0.3174, -0.1427],\n",
      "        [-0.3604, -0.3398, -0.3795,  ..., -0.3307,  0.3510, -0.3453],\n",
      "        [-0.3268, -0.3873, -0.3551,  ...,  0.3550,  0.3401, -0.2410],\n",
      "        ...,\n",
      "        [-0.4087, -0.3104, -0.3129,  ..., -0.4217,  0.4192, -0.3423],\n",
      "        [-0.0350, -0.0011, -0.0502,  ..., -0.0418, -0.0619, -0.0198],\n",
      "        [-0.3049, -0.3309, -0.3674,  ...,  0.3650,  0.3439, -0.1259]]), '0.bias': tensor([-4.2232e-01, -3.2606e-01, -3.8081e-01, -3.9244e-01, -1.1622e-02,\n",
      "        -5.4648e-02, -3.1089e-01, -5.6269e-03, -4.0108e-01, -3.2437e-01,\n",
      "        -3.5242e-02, -3.5957e-01, -2.9641e-01, -5.5680e-02,  5.9336e-02,\n",
      "        -4.1915e-01, -3.4381e-01,  3.4212e-02, -3.4749e-01, -3.5518e-01,\n",
      "        -3.5186e-03, -2.4463e-02, -2.3829e-01, -3.7933e-01,  5.7697e-02,\n",
      "        -2.9904e-01, -3.8016e-01, -2.4829e-03, -3.1815e-01,  5.8168e-02,\n",
      "         1.5001e-01, -2.6798e-01, -3.8017e-01, -3.6685e-01, -3.8353e-01,\n",
      "        -3.3009e-01, -4.2280e-03, -2.5633e-02, -3.5765e-01, -3.6872e-01,\n",
      "        -2.9885e-01,  3.8605e-02, -3.5771e-01, -3.2263e-01, -3.9065e-01,\n",
      "        -3.4001e-01,  2.7884e-02,  9.4239e-03, -1.5449e-01,  1.2452e-02,\n",
      "         2.1923e-02, -2.9323e-01,  5.1377e-02,  2.1586e-01, -4.0772e-01,\n",
      "        -3.0772e-01, -3.1249e-01,  4.0713e-02,  1.2219e-02,  2.2533e-02,\n",
      "        -3.1073e-01, -2.8780e-01, -3.0679e-01, -2.6607e-02, -7.2815e-04,\n",
      "        -3.5312e-01, -3.8444e-01,  4.1336e-02,  8.5039e-03,  5.9337e-03,\n",
      "        -3.4351e-01, -3.6525e-01, -3.2151e-01,  5.1727e-02, -5.1224e-02,\n",
      "        -4.2387e-01, -5.6309e-02,  5.9463e-02, -3.2370e-01,  4.7310e-04,\n",
      "        -3.8332e-01, -3.4615e-01, -3.2433e-01,  2.3497e-02, -2.9209e-01,\n",
      "        -3.3599e-02,  5.2963e-02, -2.2969e-01,  1.0253e-03, -2.7027e-01,\n",
      "         6.3084e-02, -3.1321e-01,  6.0055e-02,  2.5620e-02, -1.5281e-02,\n",
      "        -2.8975e-01, -4.0429e-01, -3.6480e-01,  8.3949e-03, -3.4107e-01,\n",
      "        -3.8813e-01,  3.7639e-02, -3.8967e-01, -4.3317e-01, -6.0200e-02,\n",
      "        -1.2531e-02, -1.5521e-02, -4.0806e-01, -6.2237e-02, -9.1360e-03,\n",
      "        -3.5517e-01, -6.1760e-03, -3.3046e-01,  3.6104e-02, -5.4516e-02,\n",
      "         8.9556e-02, -4.0903e-01, -4.0023e-01, -2.5918e-01,  7.5088e-02,\n",
      "        -2.7377e-02, -4.3466e-01, -3.8735e-01, -5.3802e-02, -4.3084e-01,\n",
      "         2.5305e-01, -1.9172e-02, -1.8997e-02, -3.8891e-01,  2.9085e-02,\n",
      "        -2.5869e-02, -3.7101e-01, -3.7581e-01, -1.1191e-02, -3.4915e-01,\n",
      "        -3.2217e-01, -4.1141e-01,  7.2723e-03, -3.9868e-01, -3.6236e-01,\n",
      "        -4.2444e-01, -3.1250e-01, -2.5600e-01, -2.8670e-01, -3.4731e-02,\n",
      "        -3.4108e-01, -3.2101e-01, -3.4083e-01, -3.6304e-02, -3.5252e-01,\n",
      "         1.5080e-02, -3.3745e-01, -2.7302e-01,  1.5371e-01,  7.6553e-03,\n",
      "        -3.1653e-01, -3.4554e-01, -4.2256e-01, -2.9126e-01, -3.6239e-01,\n",
      "         5.8391e-02,  2.1807e-02, -3.8903e-01,  7.7175e-02, -1.5576e-02,\n",
      "        -3.5277e-01, -5.6722e-02, -2.6642e-01, -3.4163e-01, -3.2288e-01,\n",
      "        -3.6049e-01, -5.7149e-02, -3.6301e-01,  4.1378e-02, -3.8690e-01,\n",
      "        -3.2111e-01, -3.0366e-01, -3.9482e-01, -3.2127e-01, -2.6932e-02,\n",
      "        -3.1480e-01, -4.4996e-02, -4.1646e-01,  4.3537e-02, -3.3981e-01,\n",
      "        -2.3348e-01, -3.5005e-02, -2.8179e-01, -4.2241e-01,  1.5408e-02,\n",
      "        -3.7442e-01, -2.3845e-02,  1.3412e-02, -1.3746e-01, -3.7323e-01,\n",
      "         5.2349e-02,  1.4016e-02, -1.1454e-02, -1.8616e-03,  3.6418e-04,\n",
      "        -3.1804e-01, -7.8238e-03, -6.1695e-02, -3.3191e-01,  1.4631e-02,\n",
      "        -3.2776e-01, -3.7181e-01, -4.5242e-02, -1.9691e-02, -3.0887e-01,\n",
      "         5.7521e-02, -4.0897e-01,  4.3537e-02, -3.2749e-01, -3.0413e-01,\n",
      "        -3.2875e-02,  4.5612e-02, -4.9158e-02, -2.7546e-03, -3.5008e-01,\n",
      "         1.7301e-01, -3.2972e-01, -2.9246e-01, -3.5280e-01,  2.9937e-02,\n",
      "        -3.4551e-01, -3.1525e-01, -3.8525e-01, -3.2526e-01,  7.2897e-04,\n",
      "        -3.7870e-01, -3.7390e-01, -3.5721e-01, -5.0780e-02,  3.9164e-03,\n",
      "        -3.7207e-01,  4.6397e-02, -4.0909e-01, -3.6899e-01, -3.5044e-01,\n",
      "        -4.2423e-01,  2.6630e-01, -3.2675e-01, -2.6870e-02, -3.7787e-01,\n",
      "        -3.3571e-01, -3.7401e-01, -1.9046e-02, -3.5728e-01, -6.3798e-02,\n",
      "        -3.4390e-01, -3.7205e-01, -8.2673e-02, -2.8129e-01, -3.2709e-01,\n",
      "        -3.9219e-01, -2.9471e-01,  1.7241e-01, -2.6336e-02,  1.3814e-02,\n",
      "        -3.5580e-01,  2.1404e-02, -3.5619e-01, -3.5033e-01, -2.7952e-01,\n",
      "        -4.1749e-01, -4.1674e-01, -4.0975e-01, -3.2816e-01, -1.7028e-02,\n",
      "        -1.1098e-02,  1.6729e-02, -6.3229e-02, -3.9047e-01, -3.3878e-01,\n",
      "        -4.2583e-01, -2.6326e-01,  5.8043e-02, -4.0668e-01, -4.7274e-02,\n",
      "        -3.7831e-01, -3.0177e-02,  4.3778e-02, -3.2598e-01, -3.3546e-01,\n",
      "        -2.8470e-01,  3.0405e-02,  4.5567e-02, -3.4877e-01, -4.4558e-02,\n",
      "        -3.3187e-02,  5.6490e-02, -3.6978e-01, -4.3582e-02,  4.8791e-02,\n",
      "        -3.4506e-01, -3.5717e-01,  2.7959e-01, -2.6682e-01, -3.1953e-01,\n",
      "        -4.6975e-02, -3.1522e-01]), '2.weight': tensor([[ 0.4903,  0.1898, -0.0043,  ...,  0.1429,  0.4696, -0.0285],\n",
      "        [ 0.1201,  0.2183,  0.0643,  ...,  0.2459,  0.1130,  0.1290],\n",
      "        [ 0.5700,  0.1728,  0.0143,  ...,  0.1724,  0.5369, -0.0450],\n",
      "        ...,\n",
      "        [ 0.2864,  0.4734, -0.2338,  ...,  0.5047,  0.2829, -0.2650],\n",
      "        [ 0.2193,  0.4797, -0.2032,  ...,  0.4601,  0.2945, -0.2845],\n",
      "        [ 0.4590,  0.3251, -0.3721,  ...,  0.3566,  0.5397, -0.3770]]), '2.bias': tensor([-0.5124, -0.0823, -0.4696, -0.4268, -0.5414, -0.1661, -0.1470, -0.0837,\n",
      "        -0.2410, -0.5429, -0.0675, -0.2483, -0.2835, -0.0857,  0.0910, -0.2926,\n",
      "        -0.2567, -0.1707, -0.5295, -0.3209, -0.1383, -0.2818, -0.0371, -0.0288,\n",
      "        -0.0715, -0.4176, -0.2501, -0.5391, -0.3422, -0.0809, -0.1200, -0.4146,\n",
      "        -0.1160, -0.3334, -0.0738, -0.4525, -0.4675, -0.4585, -0.2733, -0.2361,\n",
      "        -0.1571, -0.4647, -0.1533, -0.3073, -0.4042, -0.1177, -0.5352, -0.0935,\n",
      "        -0.0783, -0.1764, -0.4872, -0.4873, -0.4848, -0.2106, -0.0962, -0.1327,\n",
      "        -0.0539, -0.4438, -0.4000, -0.1035, -0.5026,  0.2018, -0.4431, -0.2954,\n",
      "        -0.4490, -0.4318, -0.4215, -0.4915, -0.2519, -0.4407, -0.1196, -0.4230,\n",
      "        -0.2770, -0.3281, -0.4735]), '4.weight': tensor([[-0.0733, -0.5375, -0.0209,  ..., -0.2077, -0.1380, -0.1896],\n",
      "        [ 0.1947, -0.0025,  0.1511,  ...,  0.3695,  0.2410, -0.3057],\n",
      "        [ 0.1614, -0.0577,  0.1452,  ...,  0.1490,  0.2990, -0.3013],\n",
      "        ...,\n",
      "        [-0.1615,  0.2146, -0.2991,  ...,  0.0490,  0.1480, -0.2997],\n",
      "        [-0.1870, -0.3471, -0.1457,  ..., -0.2899, -0.3602, -0.1728],\n",
      "        [ 0.1691, -0.2835,  0.2093,  ...,  0.1360, -0.0124, -0.2268]]), '4.bias': tensor([-0.5389, -0.3165, -0.3289, -0.4242, -0.4459, -0.5292,  0.0275, -0.3636,\n",
      "         0.1979, -0.1888, -0.2898, -0.2683, -0.3508, -0.1834, -0.1168, -0.2294,\n",
      "        -0.4382, -0.3522]), '6.weight': tensor([[-0.3100, -0.0334, -0.0494, -0.2426,  0.0506,  0.3121,  0.3952,  0.2661,\n",
      "          0.4052, -0.3256, -0.2347, -0.2392, -0.2698, -0.3953, -0.3028, -0.3114,\n",
      "         -0.2487,  0.1106]]), '6.bias': tensor([0.4344])})\n",
      "Error loading .pth file: 'collections.OrderedDict' object has no attribute 'named_parameters'\n"
     ]
    }
   ],
   "execution_count": 43
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
