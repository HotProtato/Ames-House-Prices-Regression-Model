{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-04T22:58:02.350284Z",
     "start_time": "2025-06-04T22:58:02.245485Z"
    }
   },
   "source": [
    "\n",
    "\n",
    "from data_management.preprocess_data import DataPreprocessor\n",
    "# Imports\n",
    "import numpy as np\n",
    "from utils import helpers\n",
    "\n",
    "data_preprocessor = DataPreprocessor()\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = data_preprocessor.preprocess_data(lot_frontage_threshold=13)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated Global Median Ratio: 0.7235 (from 951 samples)\n",
      "Calculating for group level: 3way (['MSZoning', 'BldgType', 'LotShape'])\n",
      " -> Found 39 groups for 3way\n",
      "Calculating for group level: 2way_ZS (['MSZoning', 'LotShape'])\n",
      " -> Found 16 groups for 2way_ZS\n",
      "Calculating for group level: 2way_ZB (['MSZoning', 'BldgType'])\n",
      " -> Found 19 groups for 2way_ZB\n",
      "Calculating for group level: 2way_BS (['BldgType', 'LotShape'])\n",
      " -> Found 14 groups for 2way_BS\n",
      "Calculating for group level: 1way_Z (['MSZoning'])\n",
      " -> Found 5 groups for 1way_Z\n",
      "Calculating for group level: 1way_B (['BldgType'])\n",
      " -> Found 5 groups for 1way_B\n",
      "Calculating for group level: 1way_S (['LotShape'])\n",
      " -> Found 4 groups for 1way_S\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T22:58:02.354129Z",
     "start_time": "2025-06-04T22:58:02.350790Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "log_then_minmax = Pipeline([\n",
    "    ('log_transform', FunctionTransformer(np.log1p)), # Example log transform\n",
    "    ('min_max_scaler', MinMaxScaler())\n",
    "])\n",
    "\n",
    "ordinal_then_minmax_pipeline = Pipeline([\n",
    "    ('ordinal_encode', OrdinalEncoder(\n",
    "        categories=helpers.get_ordinal_cats_ordered(), # Make sure this returns the correct list of lists for categories\n",
    "        handle_unknown='use_encoded_value',\n",
    "        unknown_value=-1 # Or np.nan, but -1 works fine with MinMaxScaler\n",
    "    )),\n",
    "    ('minmax_scale_ordinal', MinMaxScaler()) # Scale the 0,1,2... output of OrdinalEncoder to [0,1]\n",
    "])\n",
    "\n",
    "model_pipeline = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('log_num', log_then_minmax, helpers.get_log_minmax_cols()),\n",
    "            ('ord', ordinal_then_minmax_pipeline, helpers.get_categorical_cols_ordinal()),\n",
    "            ('num', MinMaxScaler(), helpers.get_minmax_cols())\n",
    "        ],\n",
    "        remainder='passthrough',\n",
    "        sparse_threshold=1\n",
    "    )"
   ],
   "id": "56c9bf0e55d18057",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T22:58:02.398050Z",
     "start_time": "2025-06-04T22:58:02.354636Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "# Deep learning model:\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "model_pipeline.fit(X_train, Y_train)\n",
    "X_train = model_pipeline.transform(X_train)\n",
    "X_test = model_pipeline.transform(X_test)\n",
    "\n",
    "# to remove after sensitivty analysis\n",
    "to_remove_1 = [\n",
    "    # Starting from the bottom of your list and going up to importance 0.00010\n",
    "    \"remainder__ohe__Exterior1st_VinylSd\",   # Mean: -0.00004\n",
    "    \"remainder__ohe__Foundation_Slab\",       # Mean: -0.00001\n",
    "    \"remainder__ohe__LandContour_Low\",       # Mean: -0.00001\n",
    "    \"ord__BsmtCond\",                         # Mean: -0.00001\n",
    "    \"remainder__ohe__MSSubClass_90\",         # Mean: -0.00001\n",
    "    \"ord__GarageFinish\",                     # Mean: -0.00001\n",
    "    \"remainder__ohe__BldgType_Duplex\",       # Mean: -0.00000\n",
    "    \"remainder__ohe__RoofStyle_Hip\",         # Mean: -0.00000\n",
    "    \"ord__ExterQual\",                        # Mean: -0.00000\n",
    "    \"log_num__MasVnrArea\",                   # Mean: -0.00000\n",
    "    \"remainder__ohe__MSSubClass_75\",         # Mean: -0.00000\n",
    "    \"remainder__ohe__Neighborhood_OldTown\",  # Mean: -0.00000\n",
    "    \"remainder__ohe__BedroomAbvGr_4\",        # Mean:  0.00000 (from previous list, check current)\n",
    "    \"remainder__ohe__Neighborhood_IDOTRR\",   # Mean:  0.00000\n",
    "    \"remainder__ohe__SaleType_CWD\",          # Mean:  0.00000\n",
    "    \"remainder__ohe__Exterior1st_MetalSd\",   # Mean:  0.00001 (from previous list, check current)\n",
    "    \"remainder__ohe__FullBath_2\",            # Mean:  0.00001 (from previous list, check current)\n",
    "    \"remainder__ohe__RoofStyle_Gable\",       # Mean:  0.00001 (from previous list, check current)\n",
    "    \"log_num__MiscVal\",                      # Mean:  0.00001\n",
    "    \"remainder__ohe__RoofStyle_Gambrel\",     # Mean:  0.00001\n",
    "    \"remainder__ohe__Exterior2nd_Brk Cmn\",   # Mean:  0.00001\n",
    "    \"remainder__ohe__HalfBath_2\",            # Mean:  0.00001\n",
    "    \"remainder__ohe__Neighborhood_Gilbert\",  # Mean:  0.00001\n",
    "    \"ord__ExterCond\",                        # Mean:  0.00001\n",
    "    \"remainder__ohe__SaleType_ConLI\",        # Mean:  0.00001\n",
    "    \"remainder__ohe__Exterior1st_Stucco\",    # Mean:  0.00001\n",
    "    \"remainder__ohe__BsmtFinType2_GLQ\",      # Mean:  0.00001\n",
    "    \"remainder__ohe__Exterior2nd_Wd Shng\",   # Mean:  0.00001\n",
    "    \"remainder__ohe__BsmtFinType2_LwQ\",      # Mean:  0.00001\n",
    "    \"remainder__ohe__BsmtHalfBath_1\",        # Mean:  0.00001\n",
    "    \"num__L1_I_PR\",                          # Mean:  0.00001\n",
    "    \"num__L1_I_HPI_MA3\",                     # Mean:  0.00001\n",
    "    \"remainder__ohe__Electrical_FuseP\",      # Mean:  0.00001\n",
    "    \"remainder__ohe__Foundation_Stone\",      # Mean:  0.00001\n",
    "    \"remainder__ohe__MSSubClass_40\",         # Mean:  0.00001\n",
    "    \"remainder__ohe__Exterior2nd_Stone\",     # Mean:  0.00001\n",
    "    \"remainder__ohe__BsmtFullBath_3\",        # Mean:  0.00001 (from previous list, check current)\n",
    "    \"remainder__ohe__SaleType_Con\",          # Mean:  0.00002\n",
    "    \"remainder__ohe__Exterior1st_CemntBd\",   # Mean:  0.00002\n",
    "    \"remainder__ohe__BsmtFinType2_Rec\",      # Mean:  0.00002\n",
    "    \"remainder__ohe__Foundation_Wood\",       # Mean:  0.00002\n",
    "    \"remainder__ohe__Exterior2nd_BrkFace\",   # Mean:  0.00002\n",
    "    \"remainder__ohe__BsmtExposure_Mn\",       # Mean:  0.00002\n",
    "    \"remainder__ohe__BldgType_Twnhs\",          # Mean:  0.00002\n",
    "    \"remainder__ohe__Neighborhood_SawyerW\",  # Mean:  0.00002\n",
    "    \"remainder__ohe__MiscFeature_Othr\",      # Mean:  0.00002\n",
    "    \"remainder__ohe__BedroomAbvGr_6\",        # Mean:  0.00002\n",
    "    \"remainder__ohe__Exterior2nd_Stucco\",    # Mean:  0.00002\n",
    "    \"remainder__ohe__Neighborhood_NPkVill\",  # Mean:  0.00002\n",
    "    \"log_num__EnclosedPorch\",                # Mean:  0.00003\n",
    "    \"remainder__ohe__MSSubClass_85\",         # Mean:  0.00003\n",
    "    \"remainder__ohe__HouseStyle_2.5Fin\",     # Mean:  0.00003\n",
    "    \"remainder__ohe__Neighborhood_Timber\",   # Mean:  0.00003\n",
    "    \"remainder__ohe__BsmtFinType2_GLQ\",      # Mean:  0.00003 (Appears again, likely meant a different feature or copy-paste from previous list)\n",
    "    \"remainder__ohe__Condition1_RRNn\",       # Mean:  0.00003\n",
    "    \"remainder__ohe__BldgType_TwnhsE\",       # Mean:  0.00003\n",
    "    \"remainder__ohe__BsmtFinType1_GLQ\",      # Mean:  0.00003\n",
    "    \"remainder__ohe__Condition1_RRAe\",       # Mean:  0.00003\n",
    "    \"remainder__ohe__Neighborhood_BrDale\",   # Mean:  0.00003\n",
    "    \"remainder__ohe__BsmtHalfBath_1\",        # Mean:  0.00003 (Appears again)\n",
    "    \"remainder__ohe__BsmtHalfBath_2\",        # Mean:  0.00003\n",
    "    \"remainder__ohe__Street_Pave\",           # Mean:  0.00004\n",
    "    \"remainder__ohe__Exterior1st_WdShing\",   # Mean:  0.00004\n",
    "    \"remainder__ohe__BedroomAbvGr_8\",        # Mean:  0.00004\n",
    "    \"remainder__ohe__Alley_None\",            # Mean:  0.00004\n",
    "    \"num__L1_I_PR\",                          # Mean:  0.00004\n",
    "    \"ord__Fence\",                            # Mean:  0.00004\n",
    "    \"ord__FireplaceQu\",                      # Mean:  0.00004\n",
    "    \"remainder__ohe__Electrical_SBrkr\",      # Mean:  0.00005\n",
    "    \"remainder__ohe__BsmtExposure_Mn\",       # Mean:  0.00005 (Appears again)\n",
    "    \"remainder__ohe__Exterior2nd_Stucco\",    # Mean:  0.00005 (Appears again)\n",
    "    \"remainder__ohe__BsmtFinType2_Rec\",      # Mean:  0.00005 (Appears again)\n",
    "    \"remainder__ohe__Condition1_RRAe\",       # Mean:  0.00005 (Appears again)\n",
    "    \"remainder__ohe__SaleCondition_AdjLand\", # Mean:  0.00005\n",
    "    \"remainder__ohe__Exterior2nd_Stone\",     # Mean:  0.00005\n",
    "    \"num__L1_I_HPI\",                         # Mean:  0.00005\n",
    "    \"remainder__ohe__KitchenAbvGr_3\",        # Mean:  0.00005\n",
    "    \"remainder__ohe__Exterior1st_Plywood\",   # Mean:  0.00005\n",
    "    \"remainder__ohe__Neighborhood_NWAmes\",   # Mean:  0.00005\n",
    "    \"remainder__ohe__Electrical_FuseF\",      # Mean:  0.00005\n",
    "    \"remainder__ohe__Exterior1st_MetalSd\",   # Mean:  0.00005\n",
    "    \"remainder__MoSold_sin\",                 # Mean:  0.00006\n",
    "    \"remainder__ohe__BsmtFinType1_BLQ\",      # Mean:  0.00006 (Appears again)\n",
    "    \"ord__HeatingQC\",                        # Mean:  0.00006\n",
    "    \"remainder__ohe__BsmtFinType1_Rec\",      # Mean:  0.00006 (Appears again)\n",
    "    \"remainder__ohe__Exterior1st_Stucco\",    # Mean:  0.00006 (Appears again)\n",
    "    \"num__L1_I_HPI_MA3\",                     # Mean:  0.00006 (from previous list, check current. Your new list has it at 0.00020)\n",
    "    \"remainder__ohe__Exterior2nd_AsphShn\",   # Mean:  0.00006\n",
    "    \"remainder__ohe__Exterior1st_CemntBd\",   # Mean:  0.00006\n",
    "    \"log_num__BsmtUnfSF\",                    # Mean:  0.00006\n",
    "    \"remainder__ohe__BedroomAbvGr_5\",        # Mean:  0.00007\n",
    "    \"num__YrSold\",                           # Mean:  0.00007\n",
    "    \"remainder__ohe__Neighborhood_CollgCr\",  # Mean:  0.00007\n",
    "    \"remainder__ohe__LotConfig_Inside\",      # Mean:  0.00007\n",
    "    \"remainder__ohe__BldgType_TwnhsE\",       # Mean:  0.00007\n",
    "    \"remainder__HasPool\",                    # Mean:  0.00007\n",
    "    \"remainder__ohe__Heating_Grav\",          # Mean:  0.00007\n",
    "    \"remainder__ohe__SaleCondition_Family\",  # Mean:  0.00007\n",
    "    \"num__L1_I_PR_MA3\",                      # Mean:  0.00007\n",
    "    \"remainder__ohe__MSSubClass_70\",         # Mean:  0.00008\n",
    "    \"remainder__ohe__Exterior2nd_Wd Shng\",   # Mean:  0.00008\n",
    "    \"num__L1_I_UR_MA3\",                      # Mean:  0.00008\n",
    "    \"remainder__ohe__BldgType_Duplex\",       # Mean:  0.00008\n",
    "    \"remainder__ohe__MSSubClass_75\",         # Mean:  0.00008\n",
    "    \"remainder__ohe__Exterior1st_HdBoard\",   # Mean:  0.00009\n",
    "    \"remainder__ohe__SaleType_Oth\",          # Mean:  0.00009\n",
    "    \"remainder__ohe__KitchenAbvGr_3\",        # Mean:  0.00009 (Appears again)\n",
    "    \"log_num__LowQualFinSF\",                 # Mean:  0.00009\n",
    "    \"remainder__ohe__Neighborhood_Sawyer\",   # Mean:  0.00009\n",
    "    \"num__L1_I_PR_MA6\",                      # Mean:  0.00009\n",
    "    \"remainder__ohe__Neighborhood_Mitchel\",  # Mean:  0.00009 (Appears again)\n",
    "    \"ord__HeatingQC\",                        # Mean:  0.00009 (Appears again)\n",
    "    \"remainder__ohe__SaleType_WD\",           # Mean:  0.00010\n",
    "    \"remainder__ohe__Exterior1st_Plywood\",   # Mean:  0.00010 (Appears again)\n",
    "    \"remainder__ohe__MSSubClass_80\",         # Mean:  0.00010\n",
    "    \"remainder__ohe__Exterior2nd_VinylSd\",   # Mean:  0.00010\n",
    "    \"remainder__ohe__Neighborhood_Veenker\",  # Mean:  0.00010\n",
    "    \"remainder__ohe__Exterior2nd_CmentBd\",   # Mean:  0.00010\n",
    "    \"remainder__ohe__BsmtFinType1_LwQ\",\n",
    "    \"remainder__ohe__SaleType_ConLw\",\n",
    "    \"ord__LotShape\",\n",
    "    \"ord__LandSlope\",\n",
    "    \"remainder__ohe__MSSubClass_180\",\n",
    "    \"remainder__ohe__Heating_OthW\",\n",
    "    \"remainder__ohe__MSSubClass_120\",\n",
    "    \"remainder__ohe__Exterior2nd_MetalSd\",\n",
    "    \"remainder__ohe__Neighborhood_NAmes\",\n",
    "    \"remainder__ohe__BedroomAbvGr_2\",\n",
    "    \"remainder__ohe__BedroomAbvGr_1\",\n",
    "    \"remainder__ohe__Exterior2nd_ImStucc\",\n",
    "    \"remainder__ohe__BsmtFinType2_BLQ\",\n",
    "    \"remainder__ohe__RoofStyle_Mansard\",\n",
    "    \"ord__GarageCond\",\n",
    "    \"remainder__ohe__LotConfig_FR3\"\n",
    "]\n",
    "\n",
    "\n",
    "X_train = X_train.drop(to_remove_1, axis=1)\n",
    "X_test = X_test.drop(to_remove_1, axis=1)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# float64 acceptable for EDA, float32 preferred for training.\n",
    "X_train = torch.tensor(X_train.values, device=device, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test.values, device=device, dtype=torch.float32)\n",
    "Y_train = torch.tensor(Y_train.values, device=device, dtype=torch.float32)\n",
    "Y_test = torch.tensor(Y_test.values, device=device, dtype=torch.float32)\n",
    "\n",
    "\n",
    "try:\n",
    "    train_dataset = TensorDataset(X_train, Y_train)\n",
    "    val_dataset = TensorDataset(X_test, Y_test)\n",
    "    print(f\"Train dataset length: {len(train_dataset)}\")\n",
    "    print(f\"Validation dataset length: {len(val_dataset)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating TensorDataset: {e}\")\n",
    "    # Likely length mismatch between X and y tensors if error here\n",
    "\n",
    "#batch_size = 48 # Batch size lowered, due to sample size being less than ideal.\n",
    "#train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) # Shuffle training data\n",
    "#val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False) # No need to shuffle validation\n",
    "\n",
    "loss_func = nn.MSELoss()\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "id": "7ef03f6361cd05",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset length: 1168\n",
      "Validation dataset length: 292\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T22:58:02.400158Z",
     "start_time": "2025-06-04T22:58:02.398557Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "d88c3063ecf09279",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "import optuna\n",
    "num_epochs = 400 # Hyperparameter: How many times to iterate over the dataset\n",
    "\n",
    "def get_activation_instance(activation_choice_name):\n",
    "    \"\"\"Helper function to get an activation function instance.\"\"\"\n",
    "    if activation_choice_name == 'sigmoid':\n",
    "        return nn.Sigmoid()\n",
    "    elif activation_choice_name == 'tanh':\n",
    "        return nn.Tanh()\n",
    "    elif activation_choice_name == 'leaky_relu':\n",
    "        return nn.LeakyReLU()\n",
    "    elif activation_choice_name == 'elu':\n",
    "        return nn.ELU()\n",
    "    elif activation_choice_name == 'silu': # Swish\n",
    "        return nn.SiLU()\n",
    "    else: # Default to ReLU\n",
    "        return nn.ReLU()\n",
    "\n",
    "def objective(trial):\n",
    "    # --- Adam Optimizer Hyperparameters ---\n",
    "    # Focused LR based on recent best findings\n",
    "    adam_lr = trial.suggest_float(\"adam_lr\", 0.001, 0.015, log=True) \n",
    "    \n",
    "    # Betas around previously found good values\n",
    "    adam_beta1 = trial.suggest_float(\"adam_beta1\", 0.8, 0.95) \n",
    "    adam_beta2 = trial.suggest_float(\"adam_beta2\", 0.93, 0.999) # Slightly wider upper for beta2\n",
    "    \n",
    "    adam_epsilon = trial.suggest_float(\"adam_epsilon\", 1e-9, 1e-7, log=True)\n",
    "\n",
    "    # --- Architecture Hyperparameters ---\n",
    "    # Assuming a 3-hidden-layer network (Input -> H1 -> H2 -> H3 -> Output)\n",
    "    \n",
    "    input_features = X_train.shape[1] # Assuming X_train is a global tensor\n",
    "\n",
    "    # Neurons for Hidden Layer 1 (H1)\n",
    "    # Explore smaller sizes more, given recent best trial\n",
    "    neurons_h1_choices = [32, 48, 64, 80, 96, 128, 160, 192, 240] \n",
    "    # Filter choices to be <= 1.5 * input_features to keep it somewhat relative if input_features is small\n",
    "    neurons_h1_choices_filtered = [n for n in neurons_h1_choices if n <= input_features * 1.5 or n <= 64] # Ensure some smaller options\n",
    "    if not neurons_h1_choices_filtered: # Fallback if input_features is tiny\n",
    "        neurons_h1_choices_filtered = [32, 48, 64]\n",
    "    neurons_h1 = trial.suggest_categorical('neurons_h1', neurons_h1_choices_filtered)\n",
    "    \n",
    "    # Neurons for Hidden Layer 2 (H2) - descending from H1\n",
    "    low_h2 = max(16, int(neurons_h1 * 0.25))\n",
    "    high_h2 = neurons_h1\n",
    "    if low_h2 > high_h2: low_h2 = high_h2 \n",
    "    step_h2 = max(1, (high_h2 - low_h2) // 4) if high_h2 > low_h2 else 1 \n",
    "    if high_h2 == low_h2: \n",
    "        neurons_h2 = high_h2\n",
    "    else:\n",
    "        # Ensure step is at least 1 and high is greater than low for suggest_int\n",
    "        step_h2_final = max(1, step_h2) if high_h2 > low_h2 else 1\n",
    "        if high_h2 < low_h2 + step_h2_final and high_h2 > low_h2 : # If range is too small for step\n",
    "            step_h2_final = 1\n",
    "        neurons_h2 = trial.suggest_int('neurons_h2', low=low_h2, high=high_h2, step=step_h2_final)\n",
    "\n",
    "\n",
    "    # Neurons for Hidden Layer 3 (H3) - descending from H2\n",
    "    low_h3 = max(8, int(neurons_h2 * 0.25))\n",
    "    high_h3 = neurons_h2\n",
    "    if low_h3 > high_h3: low_h3 = high_h3\n",
    "    step_h3 = max(1, (high_h3 - low_h3) // 4) if high_h3 > low_h3 else 1\n",
    "    if high_h3 == low_h3:\n",
    "        neurons_h3 = high_h3\n",
    "    else:\n",
    "        step_h3_final = max(1, step_h3) if high_h3 > low_h3 else 1\n",
    "        if high_h3 < low_h3 + step_h3_final and high_h3 > low_h3:\n",
    "            step_h3_final = 1\n",
    "        neurons_h3 = trial.suggest_int('neurons_h3', low=low_h3, high=high_h3, step=step_h3_final)\n",
    "\n",
    "\n",
    "    # Activation choices for the hidden layers\n",
    "    activation_choice_h1_name = trial.suggest_categorical('activation_h1', ['relu', 'leaky_relu', 'elu', 'silu', 'tanh'])\n",
    "    activation_choice_h2_name = trial.suggest_categorical('activation_h2', ['relu', 'leaky_relu', 'elu', 'silu', 'tanh'])\n",
    "    activation_choice_h3_name = trial.suggest_categorical('activation_h3', ['relu', 'leaky_relu', 'elu', 'silu', 'tanh'])\n",
    "\n",
    "    # Build the model layers dynamically\n",
    "    model_layers = []\n",
    "    model_layers.extend([nn.Linear(input_features, neurons_h1), get_activation_instance(activation_choice_h1_name)])\n",
    "    model_layers.extend([nn.Linear(neurons_h1, neurons_h2), get_activation_instance(activation_choice_h2_name)])\n",
    "    model_layers.extend([nn.Linear(neurons_h2, neurons_h3), get_activation_instance(activation_choice_h3_name)])\n",
    "    model_layers.append(nn.Linear(neurons_h3, 1)) \n",
    "    \n",
    "    model = nn.Sequential(*model_layers)\n",
    "    model.to(device) \n",
    "    \n",
    "    _optimizer = torch.optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=adam_lr,\n",
    "        betas=(adam_beta1, adam_beta2),\n",
    "        eps=adam_epsilon\n",
    "    )\n",
    "    \n",
    "    best_val_loss_this_trial = float('inf') \n",
    "    epochs_without_improvement = 0\n",
    "    patience_for_trial_early_stop = 75 # Reduced patience for 400 epochs\n",
    "\n",
    "    for epoch in range(num_epochs): # num_epochs defined globally (e.g., 400)\n",
    "        model.train() \n",
    "        running_train_loss = 0.0\n",
    "        for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "            features, targets = features.to(device), targets.to(device)\n",
    "            _optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            targets_reshaped = targets.unsqueeze(1) if targets.ndim == 1 else targets\n",
    "            loss = loss_func(outputs, targets_reshaped)\n",
    "            \n",
    "            if torch.isnan(loss) or torch.isinf(loss): \n",
    "                print(f\"Trial {trial.number}, Epoch {epoch+1}: NaN or Inf loss detected in training. Returning high value.\")\n",
    "                return float('inf') \n",
    "                \n",
    "            loss.backward()\n",
    "            _optimizer.step()\n",
    "            running_train_loss += loss.item() * features.size(0)\n",
    "\n",
    "        model.eval() \n",
    "        running_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for features, targets in val_loader:\n",
    "                features, targets = features.to(device), targets.to(device)\n",
    "                outputs = model(features)\n",
    "                targets_reshaped = targets.unsqueeze(1) if targets.ndim == 1 else targets\n",
    "                loss = loss_func(outputs, targets_reshaped)\n",
    "\n",
    "                if torch.isnan(loss) or torch.isinf(loss): \n",
    "                    print(f\"Trial {trial.number}, Epoch {epoch+1}: NaN or Inf loss detected in validation. Returning high value.\")\n",
    "                    return float('inf') \n",
    "\n",
    "                running_val_loss += loss.item() * features.size(0)\n",
    "        \n",
    "        current_epoch_val_loss = running_val_loss / len(val_loader.dataset)\n",
    "\n",
    "        if current_epoch_val_loss < best_val_loss_this_trial:\n",
    "            best_val_loss_this_trial = current_epoch_val_loss\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        trial.report(current_epoch_val_loss, epoch) \n",
    "        if trial.should_prune():\n",
    "            print(f\"Trial {trial.number} pruned at epoch {epoch+1} by Optuna pruner.\")\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "            \n",
    "        if epochs_without_improvement >= patience_for_trial_early_stop:\n",
    "            print(f\"Trial {trial.number} stopped early at epoch {epoch+1} (in-trial patience {patience_for_trial_early_stop}).\")\n",
    "            break \n",
    "            \n",
    "    return best_val_loss_this_trial \n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction='minimize', \n",
    "    pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=150, interval_steps=10) \n",
    "    # n_startup_trials: Don't prune first 5 trials.\n",
    "    # n_warmup_steps: Don't prune a trial before it has completed 50 epochs.\n",
    "    # interval_steps: Check for pruning every 10 epochs after warmup.\n",
    ")\n",
    "\n",
    "# 2. Run the optimization.\n",
    "#    Optuna will call your 'objective' function 'n_trials' times.\n",
    "#    Each time, it passes a 'trial' object to your function.\n",
    "study.optimize(objective, n_trials=500)\n",
    "\n",
    "# 3. Get the best results.\n",
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(study.get_trials(states=[optuna.trial.TrialState.PRUNED])))\n",
    "print(\"  Number of complete trials: \", len(study.get_trials(states=[optuna.trial.TrialState.COMPLETE])))\n",
    "\n",
    "print(\"\\nBest trial:\")\n",
    "best_trial = study.best_trial\n",
    "\n",
    "print(\"  Value (Min Validation Loss): \", best_trial.value)\n",
    "\n",
    "print(\"  Best hyperparameters: \")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "    # Print progress (e.g., every epoch or every few epochs)\n",
    "#    print(f'Epoch {epoch+1}/{num_epochs} | Train Loss: {epoch_train_loss:.6f} | Val Loss: {epoch_val_loss:.6f}')"
   ],
   "id": "2ff2b9eaf3fde467"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T22:58:08.164360Z",
     "start_time": "2025-06-04T22:58:02.400659Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_epochs = 400 # Hyperparameter: How many times to iterate over the dataset\n",
    "\n",
    "best_validation_loss = float('inf')\n",
    "best_model_path = \"best_model_weights.pth\"\n",
    "patience_epochs = 150\n",
    "totalEpochs = 0\n",
    "print(\"\\nStarting Training...\")\n",
    "while best_validation_loss > float(0.015) and totalEpochs < 1200:\n",
    "    patience_counter = 0\n",
    "    updates = 0\n",
    "    model, optimizer = helpers.get_model_and_optim(X_train.shape[1])\n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        # --- Training Phase ---\n",
    "        model.train() # Set model to training mode (enables dropout, batchnorm updates)\n",
    "        running_train_loss = 0.0\n",
    "        for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "            # Move batch data to the target device (GPU or CPU)\n",
    "            features, targets = features.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            # Reshape the targets tensor to match the outputs shape ([batch_size, 1])\n",
    "            targets_reshaped = targets.unsqueeze(1)\n",
    "            loss = loss_func(outputs, targets_reshaped)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            running_train_loss += loss.item() * features.size(0)\n",
    "            totalEpochs += 1\n",
    "    \n",
    "        epoch_train_loss = running_train_loss / len(train_loader.dataset)\n",
    "    \n",
    "        # --- Validation Phase ---\n",
    "        model.eval() # Set model to evaluation mode (disables dropout, batchnorm updates)\n",
    "        running_val_loss = 0.0\n",
    "    \n",
    "        with torch.no_grad(): # No need to calculate gradients during validation\n",
    "            for features, targets in val_loader:\n",
    "                features, targets = features.to(device), targets.to(device)\n",
    "                outputs = model(features)\n",
    "                targets_reshaped = targets.unsqueeze(1)\n",
    "                loss = loss_func(outputs, targets_reshaped)\n",
    "                running_val_loss += loss.item() * features.size(0)\n",
    "    \n",
    "        epoch_val_loss = running_val_loss / len(val_loader.dataset)\n",
    "    \n",
    "        # Print progress (e.g., every epoch or every few epochs)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs} | Train Loss: {epoch_train_loss:.6f} | Val Loss: {epoch_val_loss:.6f}')\n",
    "        if epoch_val_loss < best_validation_loss:\n",
    "            print(f\"Validation loss improved from {best_validation_loss:.6f} to {epoch_val_loss:.6f}. Saving model to {best_model_path}\")\n",
    "            best_validation_loss = epoch_val_loss\n",
    "            torch.save(model.state_dict(), best_model_path) # Save the model's weights\n",
    "            patience_counter = 0  # Reset patience since we found a better model\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            updates += 1\n",
    "        \n",
    "        if patience_counter >= patience_epochs:\n",
    "            print(f\"Early stopping triggered after {patience_epochs} epochs without improvement.\")\n",
    "            break\n",
    "\n",
    "print(f\"The bestest best validation loss: {best_validation_loss:.6f}\")\n"
   ],
   "id": "cb968c92705e8051",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Training...\n",
      "Epoch 1/400 | Train Loss: 90.206425 | Val Loss: 5.843169\n",
      "Validation loss improved from inf to 5.843169. Saving model to best_model_weights.pth\n",
      "Epoch 2/400 | Train Loss: 4.095268 | Val Loss: 2.293256\n",
      "Validation loss improved from 5.843169 to 2.293256. Saving model to best_model_weights.pth\n",
      "Epoch 3/400 | Train Loss: 0.704974 | Val Loss: 0.444810\n",
      "Validation loss improved from 2.293256 to 0.444810. Saving model to best_model_weights.pth\n",
      "Epoch 4/400 | Train Loss: 0.241751 | Val Loss: 0.191608\n",
      "Validation loss improved from 0.444810 to 0.191608. Saving model to best_model_weights.pth\n",
      "Epoch 5/400 | Train Loss: 0.147256 | Val Loss: 0.123218\n",
      "Validation loss improved from 0.191608 to 0.123218. Saving model to best_model_weights.pth\n",
      "Epoch 6/400 | Train Loss: 0.095979 | Val Loss: 0.083716\n",
      "Validation loss improved from 0.123218 to 0.083716. Saving model to best_model_weights.pth\n",
      "Epoch 7/400 | Train Loss: 0.064601 | Val Loss: 0.058673\n",
      "Validation loss improved from 0.083716 to 0.058673. Saving model to best_model_weights.pth\n",
      "Epoch 8/400 | Train Loss: 0.048763 | Val Loss: 0.048120\n",
      "Validation loss improved from 0.058673 to 0.048120. Saving model to best_model_weights.pth\n",
      "Epoch 9/400 | Train Loss: 0.042128 | Val Loss: 0.041548\n",
      "Validation loss improved from 0.048120 to 0.041548. Saving model to best_model_weights.pth\n",
      "Epoch 10/400 | Train Loss: 0.043247 | Val Loss: 0.049621\n",
      "Epoch 11/400 | Train Loss: 0.032498 | Val Loss: 0.031580\n",
      "Validation loss improved from 0.041548 to 0.031580. Saving model to best_model_weights.pth\n",
      "Epoch 12/400 | Train Loss: 0.028323 | Val Loss: 0.024291\n",
      "Validation loss improved from 0.031580 to 0.024291. Saving model to best_model_weights.pth\n",
      "Epoch 13/400 | Train Loss: 0.029222 | Val Loss: 0.028377\n",
      "Epoch 14/400 | Train Loss: 0.024286 | Val Loss: 0.021463\n",
      "Validation loss improved from 0.024291 to 0.021463. Saving model to best_model_weights.pth\n",
      "Epoch 15/400 | Train Loss: 0.020013 | Val Loss: 0.028375\n",
      "Epoch 16/400 | Train Loss: 0.027927 | Val Loss: 0.024659\n",
      "Epoch 17/400 | Train Loss: 0.018412 | Val Loss: 0.030953\n",
      "Epoch 18/400 | Train Loss: 0.018290 | Val Loss: 0.018332\n",
      "Validation loss improved from 0.021463 to 0.018332. Saving model to best_model_weights.pth\n",
      "Epoch 19/400 | Train Loss: 0.025826 | Val Loss: 0.017981\n",
      "Validation loss improved from 0.018332 to 0.017981. Saving model to best_model_weights.pth\n",
      "Epoch 20/400 | Train Loss: 0.021447 | Val Loss: 0.018317\n",
      "Epoch 21/400 | Train Loss: 0.019521 | Val Loss: 0.073427\n",
      "Epoch 22/400 | Train Loss: 0.026373 | Val Loss: 0.045642\n",
      "Epoch 23/400 | Train Loss: 0.018445 | Val Loss: 0.017397\n",
      "Validation loss improved from 0.017981 to 0.017397. Saving model to best_model_weights.pth\n",
      "Epoch 24/400 | Train Loss: 0.019231 | Val Loss: 0.017832\n",
      "Epoch 25/400 | Train Loss: 0.023001 | Val Loss: 0.017505\n",
      "Epoch 26/400 | Train Loss: 0.019423 | Val Loss: 0.017173\n",
      "Validation loss improved from 0.017397 to 0.017173. Saving model to best_model_weights.pth\n",
      "Epoch 27/400 | Train Loss: 0.016979 | Val Loss: 0.016873\n",
      "Validation loss improved from 0.017173 to 0.016873. Saving model to best_model_weights.pth\n",
      "Epoch 28/400 | Train Loss: 0.014910 | Val Loss: 0.016095\n",
      "Validation loss improved from 0.016873 to 0.016095. Saving model to best_model_weights.pth\n",
      "Epoch 29/400 | Train Loss: 0.019696 | Val Loss: 0.017030\n",
      "Epoch 30/400 | Train Loss: 0.015411 | Val Loss: 0.027771\n",
      "Epoch 31/400 | Train Loss: 0.015249 | Val Loss: 0.017524\n",
      "Epoch 32/400 | Train Loss: 0.024178 | Val Loss: 0.038266\n",
      "Epoch 33/400 | Train Loss: 0.020305 | Val Loss: 0.020470\n",
      "Epoch 34/400 | Train Loss: 0.014443 | Val Loss: 0.021218\n",
      "Epoch 35/400 | Train Loss: 0.023767 | Val Loss: 0.044520\n",
      "Epoch 36/400 | Train Loss: 0.019127 | Val Loss: 0.016043\n",
      "Validation loss improved from 0.016095 to 0.016043. Saving model to best_model_weights.pth\n",
      "Epoch 37/400 | Train Loss: 0.015688 | Val Loss: 0.015910\n",
      "Validation loss improved from 0.016043 to 0.015910. Saving model to best_model_weights.pth\n",
      "Epoch 38/400 | Train Loss: 0.013870 | Val Loss: 0.017210\n",
      "Epoch 39/400 | Train Loss: 0.017921 | Val Loss: 0.031401\n",
      "Epoch 40/400 | Train Loss: 0.017462 | Val Loss: 0.017721\n",
      "Epoch 41/400 | Train Loss: 0.012913 | Val Loss: 0.023105\n",
      "Epoch 42/400 | Train Loss: 0.016191 | Val Loss: 0.033311\n",
      "Epoch 43/400 | Train Loss: 0.018778 | Val Loss: 0.016533\n",
      "Epoch 44/400 | Train Loss: 0.014097 | Val Loss: 0.021983\n",
      "Epoch 45/400 | Train Loss: 0.013113 | Val Loss: 0.029148\n",
      "Epoch 46/400 | Train Loss: 0.017463 | Val Loss: 0.020354\n",
      "Epoch 47/400 | Train Loss: 0.015083 | Val Loss: 0.020341\n",
      "Epoch 48/400 | Train Loss: 0.014330 | Val Loss: 0.015359\n",
      "Validation loss improved from 0.015910 to 0.015359. Saving model to best_model_weights.pth\n",
      "Epoch 49/400 | Train Loss: 0.015770 | Val Loss: 0.017765\n",
      "Epoch 50/400 | Train Loss: 0.012133 | Val Loss: 0.034635\n",
      "Epoch 51/400 | Train Loss: 0.022824 | Val Loss: 0.030387\n",
      "Epoch 52/400 | Train Loss: 0.016830 | Val Loss: 0.018960\n",
      "Epoch 53/400 | Train Loss: 0.011744 | Val Loss: 0.017750\n",
      "Epoch 54/400 | Train Loss: 0.016932 | Val Loss: 0.031764\n",
      "Epoch 55/400 | Train Loss: 0.015140 | Val Loss: 0.023884\n",
      "Epoch 56/400 | Train Loss: 0.015063 | Val Loss: 0.015768\n",
      "Epoch 57/400 | Train Loss: 0.013064 | Val Loss: 0.023035\n",
      "Epoch 58/400 | Train Loss: 0.011569 | Val Loss: 0.018196\n",
      "Epoch 59/400 | Train Loss: 0.014811 | Val Loss: 0.022478\n",
      "Epoch 60/400 | Train Loss: 0.015744 | Val Loss: 0.032936\n",
      "Epoch 61/400 | Train Loss: 0.017675 | Val Loss: 0.028911\n",
      "Epoch 62/400 | Train Loss: 0.016014 | Val Loss: 0.021021\n",
      "Epoch 63/400 | Train Loss: 0.013796 | Val Loss: 0.029991\n",
      "Epoch 64/400 | Train Loss: 0.013026 | Val Loss: 0.018131\n",
      "Epoch 65/400 | Train Loss: 0.021303 | Val Loss: 0.015826\n",
      "Epoch 66/400 | Train Loss: 0.013598 | Val Loss: 0.035340\n",
      "Epoch 67/400 | Train Loss: 0.017147 | Val Loss: 0.016285\n",
      "Epoch 68/400 | Train Loss: 0.012281 | Val Loss: 0.016184\n",
      "Epoch 69/400 | Train Loss: 0.016341 | Val Loss: 0.017535\n",
      "Epoch 70/400 | Train Loss: 0.011961 | Val Loss: 0.015388\n",
      "Epoch 71/400 | Train Loss: 0.011197 | Val Loss: 0.016434\n",
      "Epoch 72/400 | Train Loss: 0.010994 | Val Loss: 0.023136\n",
      "Epoch 73/400 | Train Loss: 0.014256 | Val Loss: 0.031045\n",
      "Epoch 74/400 | Train Loss: 0.010814 | Val Loss: 0.027433\n",
      "Epoch 75/400 | Train Loss: 0.012492 | Val Loss: 0.016864\n",
      "Epoch 76/400 | Train Loss: 0.016853 | Val Loss: 0.017227\n",
      "Epoch 77/400 | Train Loss: 0.011854 | Val Loss: 0.015687\n",
      "Epoch 78/400 | Train Loss: 0.013625 | Val Loss: 0.016350\n",
      "Epoch 79/400 | Train Loss: 0.014038 | Val Loss: 0.018705\n",
      "Epoch 80/400 | Train Loss: 0.014088 | Val Loss: 0.023051\n",
      "Epoch 81/400 | Train Loss: 0.012385 | Val Loss: 0.023486\n",
      "Epoch 82/400 | Train Loss: 0.011817 | Val Loss: 0.017671\n",
      "Epoch 83/400 | Train Loss: 0.011067 | Val Loss: 0.015564\n",
      "Epoch 84/400 | Train Loss: 0.012743 | Val Loss: 0.031741\n",
      "Epoch 85/400 | Train Loss: 0.015887 | Val Loss: 0.025223\n",
      "Epoch 86/400 | Train Loss: 0.013582 | Val Loss: 0.042837\n",
      "Epoch 87/400 | Train Loss: 0.018613 | Val Loss: 0.027673\n",
      "Epoch 88/400 | Train Loss: 0.012055 | Val Loss: 0.015822\n",
      "Epoch 89/400 | Train Loss: 0.010774 | Val Loss: 0.022795\n",
      "Epoch 90/400 | Train Loss: 0.011904 | Val Loss: 0.022783\n",
      "Epoch 91/400 | Train Loss: 0.014545 | Val Loss: 0.016428\n",
      "Epoch 92/400 | Train Loss: 0.011064 | Val Loss: 0.020482\n",
      "Epoch 93/400 | Train Loss: 0.010991 | Val Loss: 0.018687\n",
      "Epoch 94/400 | Train Loss: 0.014069 | Val Loss: 0.021787\n",
      "Epoch 95/400 | Train Loss: 0.013469 | Val Loss: 0.021923\n",
      "Epoch 96/400 | Train Loss: 0.011171 | Val Loss: 0.017270\n",
      "Epoch 97/400 | Train Loss: 0.011305 | Val Loss: 0.016216\n",
      "Epoch 98/400 | Train Loss: 0.014557 | Val Loss: 0.027217\n",
      "Epoch 99/400 | Train Loss: 0.013373 | Val Loss: 0.015888\n",
      "Epoch 100/400 | Train Loss: 0.011831 | Val Loss: 0.017780\n",
      "Epoch 101/400 | Train Loss: 0.012151 | Val Loss: 0.017282\n",
      "Epoch 102/400 | Train Loss: 0.011394 | Val Loss: 0.021387\n",
      "Epoch 103/400 | Train Loss: 0.018091 | Val Loss: 0.023664\n",
      "Epoch 104/400 | Train Loss: 0.017207 | Val Loss: 0.018069\n",
      "Epoch 105/400 | Train Loss: 0.010495 | Val Loss: 0.026977\n",
      "Epoch 106/400 | Train Loss: 0.013215 | Val Loss: 0.016575\n",
      "Epoch 107/400 | Train Loss: 0.010889 | Val Loss: 0.018473\n",
      "Epoch 108/400 | Train Loss: 0.010883 | Val Loss: 0.034563\n",
      "Epoch 109/400 | Train Loss: 0.014587 | Val Loss: 0.032769\n",
      "Epoch 110/400 | Train Loss: 0.012953 | Val Loss: 0.016978\n",
      "Epoch 111/400 | Train Loss: 0.008933 | Val Loss: 0.016706\n",
      "Epoch 112/400 | Train Loss: 0.010245 | Val Loss: 0.017433\n",
      "Epoch 113/400 | Train Loss: 0.009733 | Val Loss: 0.017874\n",
      "Epoch 114/400 | Train Loss: 0.013945 | Val Loss: 0.017583\n",
      "Epoch 115/400 | Train Loss: 0.010316 | Val Loss: 0.018504\n",
      "Epoch 116/400 | Train Loss: 0.009382 | Val Loss: 0.064008\n",
      "Epoch 117/400 | Train Loss: 0.019435 | Val Loss: 0.025279\n",
      "Epoch 118/400 | Train Loss: 0.010999 | Val Loss: 0.019782\n",
      "Epoch 119/400 | Train Loss: 0.008832 | Val Loss: 0.018830\n",
      "Epoch 120/400 | Train Loss: 0.010007 | Val Loss: 0.017487\n",
      "Epoch 121/400 | Train Loss: 0.009416 | Val Loss: 0.018511\n",
      "Epoch 122/400 | Train Loss: 0.012108 | Val Loss: 0.027780\n",
      "Epoch 123/400 | Train Loss: 0.011668 | Val Loss: 0.016955\n",
      "Epoch 124/400 | Train Loss: 0.008036 | Val Loss: 0.029672\n",
      "Epoch 125/400 | Train Loss: 0.011305 | Val Loss: 0.017703\n",
      "Epoch 126/400 | Train Loss: 0.008899 | Val Loss: 0.022469\n",
      "Epoch 127/400 | Train Loss: 0.009491 | Val Loss: 0.020352\n",
      "Epoch 128/400 | Train Loss: 0.008341 | Val Loss: 0.028027\n",
      "Epoch 129/400 | Train Loss: 0.017657 | Val Loss: 0.019338\n",
      "Epoch 130/400 | Train Loss: 0.008992 | Val Loss: 0.017725\n",
      "Epoch 131/400 | Train Loss: 0.006956 | Val Loss: 0.019742\n",
      "Epoch 132/400 | Train Loss: 0.008396 | Val Loss: 0.020011\n",
      "Epoch 133/400 | Train Loss: 0.008714 | Val Loss: 0.019886\n",
      "Epoch 134/400 | Train Loss: 0.012826 | Val Loss: 0.024338\n",
      "Epoch 135/400 | Train Loss: 0.011543 | Val Loss: 0.018925\n",
      "Epoch 136/400 | Train Loss: 0.007102 | Val Loss: 0.022113\n",
      "Epoch 137/400 | Train Loss: 0.008533 | Val Loss: 0.019668\n",
      "Epoch 138/400 | Train Loss: 0.007067 | Val Loss: 0.018749\n",
      "Epoch 139/400 | Train Loss: 0.008351 | Val Loss: 0.018332\n",
      "Epoch 140/400 | Train Loss: 0.008238 | Val Loss: 0.028387\n",
      "Epoch 141/400 | Train Loss: 0.008714 | Val Loss: 0.020343\n",
      "Epoch 142/400 | Train Loss: 0.007086 | Val Loss: 0.022473\n",
      "Epoch 143/400 | Train Loss: 0.007697 | Val Loss: 0.020314\n",
      "Epoch 144/400 | Train Loss: 0.007364 | Val Loss: 0.018620\n",
      "Epoch 145/400 | Train Loss: 0.007726 | Val Loss: 0.019463\n",
      "Epoch 146/400 | Train Loss: 0.006783 | Val Loss: 0.023740\n",
      "Epoch 147/400 | Train Loss: 0.009768 | Val Loss: 0.017612\n",
      "Epoch 148/400 | Train Loss: 0.006202 | Val Loss: 0.019633\n",
      "Epoch 149/400 | Train Loss: 0.008141 | Val Loss: 0.018010\n",
      "Epoch 150/400 | Train Loss: 0.007181 | Val Loss: 0.027303\n",
      "Epoch 151/400 | Train Loss: 0.008578 | Val Loss: 0.023545\n",
      "Epoch 152/400 | Train Loss: 0.008192 | Val Loss: 0.017859\n",
      "Epoch 153/400 | Train Loss: 0.005508 | Val Loss: 0.036609\n",
      "Epoch 154/400 | Train Loss: 0.008657 | Val Loss: 0.024893\n",
      "Epoch 155/400 | Train Loss: 0.006179 | Val Loss: 0.019549\n",
      "Epoch 156/400 | Train Loss: 0.005572 | Val Loss: 0.026632\n",
      "Epoch 157/400 | Train Loss: 0.008388 | Val Loss: 0.019696\n",
      "Epoch 158/400 | Train Loss: 0.006087 | Val Loss: 0.019763\n",
      "Epoch 159/400 | Train Loss: 0.006255 | Val Loss: 0.024096\n",
      "Epoch 160/400 | Train Loss: 0.006860 | Val Loss: 0.022251\n",
      "Epoch 161/400 | Train Loss: 0.005545 | Val Loss: 0.023687\n",
      "Epoch 162/400 | Train Loss: 0.011680 | Val Loss: 0.024501\n",
      "Epoch 163/400 | Train Loss: 0.008923 | Val Loss: 0.022134\n",
      "Epoch 164/400 | Train Loss: 0.010255 | Val Loss: 0.021531\n",
      "Epoch 165/400 | Train Loss: 0.006122 | Val Loss: 0.019987\n",
      "Epoch 166/400 | Train Loss: 0.005948 | Val Loss: 0.033877\n",
      "Epoch 167/400 | Train Loss: 0.008757 | Val Loss: 0.035487\n",
      "Epoch 168/400 | Train Loss: 0.011384 | Val Loss: 0.021309\n",
      "Epoch 169/400 | Train Loss: 0.006172 | Val Loss: 0.020068\n",
      "Epoch 170/400 | Train Loss: 0.005565 | Val Loss: 0.023830\n",
      "Epoch 171/400 | Train Loss: 0.005778 | Val Loss: 0.020080\n",
      "Epoch 172/400 | Train Loss: 0.008286 | Val Loss: 0.019106\n",
      "Epoch 173/400 | Train Loss: 0.006647 | Val Loss: 0.021059\n",
      "Epoch 174/400 | Train Loss: 0.008050 | Val Loss: 0.020095\n",
      "Epoch 175/400 | Train Loss: 0.005501 | Val Loss: 0.021463\n",
      "Epoch 176/400 | Train Loss: 0.005036 | Val Loss: 0.022415\n",
      "Epoch 177/400 | Train Loss: 0.007488 | Val Loss: 0.026704\n",
      "Epoch 178/400 | Train Loss: 0.009806 | Val Loss: 0.027443\n",
      "Epoch 179/400 | Train Loss: 0.008226 | Val Loss: 0.022109\n",
      "Epoch 180/400 | Train Loss: 0.006743 | Val Loss: 0.029578\n",
      "Epoch 181/400 | Train Loss: 0.005957 | Val Loss: 0.019125\n",
      "Epoch 182/400 | Train Loss: 0.005513 | Val Loss: 0.019865\n",
      "Epoch 183/400 | Train Loss: 0.005160 | Val Loss: 0.019352\n",
      "Epoch 184/400 | Train Loss: 0.005384 | Val Loss: 0.027504\n",
      "Epoch 185/400 | Train Loss: 0.012038 | Val Loss: 0.022592\n",
      "Epoch 186/400 | Train Loss: 0.005788 | Val Loss: 0.019962\n",
      "Epoch 187/400 | Train Loss: 0.005443 | Val Loss: 0.021287\n",
      "Epoch 188/400 | Train Loss: 0.004542 | Val Loss: 0.020290\n",
      "Epoch 189/400 | Train Loss: 0.005338 | Val Loss: 0.021555\n",
      "Epoch 190/400 | Train Loss: 0.006404 | Val Loss: 0.024999\n",
      "Epoch 191/400 | Train Loss: 0.008056 | Val Loss: 0.019877\n",
      "Epoch 192/400 | Train Loss: 0.004792 | Val Loss: 0.020838\n",
      "Epoch 193/400 | Train Loss: 0.007214 | Val Loss: 0.023872\n",
      "Epoch 194/400 | Train Loss: 0.004931 | Val Loss: 0.020340\n",
      "Epoch 195/400 | Train Loss: 0.006141 | Val Loss: 0.027369\n",
      "Epoch 196/400 | Train Loss: 0.009150 | Val Loss: 0.028585\n",
      "Epoch 197/400 | Train Loss: 0.006729 | Val Loss: 0.021725\n",
      "Epoch 198/400 | Train Loss: 0.004521 | Val Loss: 0.020337\n",
      "Early stopping triggered after 150 epochs without improvement.\n",
      "The bestest best validation loss: 0.015359\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Model mathematically represented, using Adam optimizer, default hyperparameters:\n",
    "$$\n",
    "\\begin{align}\n",
    "% Input\n",
    "\\mathbf{a}^{(0)} &= \\mathbf{x}, \\quad \\text{where } \\mathbf{x} \\in \\mathbb{R}^{260} \\\\\n",
    "% Layer 1\n",
    "\\mathbf{z}^{(1)} &= W^{(1)}\\mathbf{a}^{(0)} + \\mathbf{b}^{(1)}, \\quad \\text{where } W^{(1)} \\in \\mathbb{R}^{128 \\times 260}, \\mathbf{b}^{(1)} \\in \\mathbb{R}^{128} \\\\\n",
    "\\mathbf{a}^{(1)} &= \\text{ReLU}(\\mathbf{z}^{(1)}), \\quad \\mathbf{a}^{(1)} \\in \\mathbb{R}^{128} \\\\\n",
    "% Layer 2\n",
    "\\mathbf{z}^{(2)} &= W^{(2)}\\mathbf{a}^{(1)} + \\mathbf{b}^{(2)}, \\quad \\text{where } W^{(2)} \\in \\mathbb{R}^{64 \\times 128}, \\mathbf{b}^{(2)} \\in \\mathbb{R}^{64} \\\\\n",
    "\\mathbf{a}^{(2)} &= \\text{ReLU}(\\mathbf{z}^{(2)}), \\quad \\mathbf{a}^{(2)} \\in \\mathbb{R}^{64} \\\\\n",
    "% Layer 3\n",
    "\\mathbf{z}^{(3)} &= W^{(3)}\\mathbf{a}^{(2)} + \\mathbf{b}^{(3)}, \\quad \\text{where } W^{(3)} \\in \\mathbb{R}^{16 \\times 64}, \\mathbf{b}^{(3)} \\in \\mathbb{R}^{16} \\\\\n",
    "\\mathbf{a}^{(3)} &= \\text{ReLU}(\\mathbf{z}^{(3)}), \\quad \\mathbf{a}^{(3)} \\in \\mathbb{R}^{16} \\\\\n",
    "% Layer 4 (Output)\n",
    "\\mathbf{z}^{(4)} &= W^{(4)}\\mathbf{a}^{(3)} + b^{(4)}, \\quad \\text{where } W^{(4)} \\in \\mathbb{R}^{1 \\times 16}, \\mathbf{b}^{(4)} \\in \\mathbb{R} \\\\\n",
    "\\hat{y} &= \\mathbf{z}^{(4)}, \\quad \\hat{y} \\in \\mathbb{R} \\\\\n",
    "% Loss\n",
    "\\mathcal{L} &= (\\hat{y} - y_{\\text{target}})^2\n",
    "\\end{align}\n",
    "$$"
   ],
   "id": "24d8d53978c00922"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "try:\n",
    "    weights = torch.load(best_model_path, map_location=torch.device('cpu'))\n",
    "    print(\"Successfully loaded entire model.\")\n",
    "    model.path = weights\n",
    "    # You can now inspect the model structure\n",
    "    print(\"\\nModel structure:\")\n",
    "    print(model)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading .pth file: {e}\")"
   ],
   "id": "5be46c771a498f90"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
