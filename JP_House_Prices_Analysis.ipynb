{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-12T08:36:24.272282Z",
     "start_time": "2025-05-12T08:36:24.179Z"
    }
   },
   "source": [
    "from preprocess_data import DataPreprocessor\n",
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import helpers\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "data_preprocessor = DataPreprocessor()\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = data_preprocessor.preprocess_data(lot_frontage_threshold=13)\n",
    "\n",
    "#to_remove = ['ohe__MSSubClass_40', 'ohe__Utilities_NoSeWa', 'ohe__LotConfig_FR3', 'ohe__Neighborhood_Blueste', 'ohe__Condition1_RRNe', 'ohe__Condition1_RRNn', 'ohe__Condition2_Feedr', 'ohe__Condition2_PosA', 'ohe__Condition2_PosN', 'ohe__Condition2_RRAe', 'ohe__Condition2_RRAn', 'ohe__Condition2_RRNn', 'ohe__RoofStyle_Mansard', 'ohe__RoofStyle_Shed', 'ohe__RoofMatl_Membran', 'ohe__RoofMatl_Metal', 'ohe__RoofMatl_Roll', 'ohe__RoofMatl_WdShake', 'ohe__RoofMatl_WdShngl', 'ohe__Exterior1st_AsphShn', 'ohe__Exterior1st_BrkComm', 'ohe__Exterior1st_CBlock', 'ohe__Exterior1st_ImStucc', 'ohe__Exterior1st_Stone', 'ohe__Exterior2nd_AsphShn', 'ohe__Exterior2nd_CBlock', 'ohe__Exterior2nd_Other', 'ohe__Exterior2nd_Stone', 'ohe__Foundation_Stone', 'ohe__Foundation_Wood', 'ohe__Heating_OthW', 'ohe__Heating_Wall', 'ohe__Electrical_FuseP', 'ohe__Electrical_Mix', 'ohe__MiscFeature_Othr', 'ohe__MiscFeature_TenC', 'ohe__SaleType_CWD', 'ohe__SaleType_Con', 'ohe__SaleType_ConLI', 'ohe__SaleType_ConLw', 'ohe__SaleType_Oth', 'ohe__SaleCondition_AdjLand']\n",
    "\n",
    "#X_train = X_train.drop(to_remove, axis=1)\n",
    "#X_test = X_test.drop(to_remove, axis=1)\n",
    "\n",
    "# Test data to be processed separately, here.\n",
    "#test_df['Electrical'] = test_df['Electrical'].fillna(mode_electrical)\n",
    "#test_df = helpers.init_fill_na(test_df)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated Global Median Ratio: 0.7235 (from 951 samples)\n",
      "Calculating for group level: 3way (['MSZoning', 'BldgType', 'LotShape'])\n",
      " -> Found 39 groups for 3way\n",
      "Calculating for group level: 2way_ZS (['MSZoning', 'LotShape'])\n",
      " -> Found 16 groups for 2way_ZS\n",
      "Calculating for group level: 2way_ZB (['MSZoning', 'BldgType'])\n",
      " -> Found 19 groups for 2way_ZB\n",
      "Calculating for group level: 2way_BS (['BldgType', 'LotShape'])\n",
      " -> Found 14 groups for 2way_BS\n",
      "Calculating for group level: 1way_Z (['MSZoning'])\n",
      " -> Found 5 groups for 1way_Z\n",
      "Calculating for group level: 1way_B (['BldgType'])\n",
      " -> Found 5 groups for 1way_B\n",
      "Calculating for group level: 1way_S (['LotShape'])\n",
      " -> Found 4 groups for 1way_S\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T08:36:24.275829Z",
     "start_time": "2025-05-12T08:36:24.273282Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Update the pipeline\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', helpers.generate_preprocessor(\n",
    "        ordinal_cats_ordered=helpers.get_ordinal_cats_ordered(),\n",
    "        categorical_cols_ordinal=helpers.get_categorical_cols_ordinal(),\n",
    "        numerical_cols=helpers.get_numeric_cols()))\n",
    "])"
   ],
   "id": "56c9bf0e55d18057",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T08:36:38.409381Z",
     "start_time": "2025-05-12T08:36:24.276830Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from scipy.sparse import issparse\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "# Deep learning model:\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "X_train = model_pipeline.fit_transform(X_train)\n",
    "X_test = model_pipeline.transform(X_test)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# float64 acceptable for EDA, float32 preferred for training.\n",
    "X_train = torch.tensor(X_train, device=device, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, device=device, dtype=torch.float32)\n",
    "Y_train = torch.tensor(Y_train.values, device=device, dtype=torch.float32)\n",
    "Y_test = torch.tensor(Y_test.values, device=device, dtype=torch.float32)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(X_train.shape[1], 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 16),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16, 1)\n",
    ")\n",
    "\n",
    "try:\n",
    "    train_dataset = TensorDataset(X_train, Y_train)\n",
    "    val_dataset = TensorDataset(X_test, Y_test)\n",
    "    print(f\"Train dataset length: {len(train_dataset)}\")\n",
    "    print(f\"Validation dataset length: {len(val_dataset)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating TensorDataset: {e}\")\n",
    "    # Likely length mismatch between X and y tensors if error here\n",
    "\n",
    "batch_size = 64 # Hyperparameter: How many samples per batch\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) # Shuffle training data\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False) # No need to shuffle validation\n",
    "\n",
    "loss_func = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 500 # Hyperparameter: How many times to iterate over the dataset\n",
    "\n",
    "print(\"\\nStarting Training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    # --- Training Phase ---\n",
    "    model.train() # Set model to training mode (enables dropout, batchnorm updates)\n",
    "    running_train_loss = 0.0\n",
    "    for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "        # Move batch data to the target device (GPU or CPU)\n",
    "        features, targets = features.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features)\n",
    "        # Reshape the targets tensor to match the outputs shape ([batch_size, 1])\n",
    "        targets_reshaped = targets.unsqueeze(1)\n",
    "        loss = loss_func(outputs, targets_reshaped)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_train_loss += loss.item() * features.size(0)\n",
    "\n",
    "    epoch_train_loss = running_train_loss / len(train_loader.dataset)\n",
    "\n",
    "    # --- Validation Phase ---\n",
    "    model.eval() # Set model to evaluation mode (disables dropout, batchnorm updates)\n",
    "    running_val_loss = 0.0\n",
    "    with torch.no_grad(): # No need to calculate gradients during validation\n",
    "        for features, targets in val_loader:\n",
    "            features, targets = features.to(device), targets.to(device)\n",
    "            outputs = model(features)\n",
    "            targets_reshaped = targets.unsqueeze(1)\n",
    "            loss = loss_func(outputs, targets_reshaped)\n",
    "            running_val_loss += loss.item() * features.size(0)\n",
    "\n",
    "    epoch_val_loss = running_val_loss / len(val_loader.dataset)\n",
    "\n",
    "    # Print progress (e.g., every epoch or every few epochs)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs} | Train Loss: {epoch_train_loss:.6f} | Val Loss: {epoch_val_loss:.6f}')"
   ],
   "id": "8a41a83688b07ce0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset length: 1168\n",
      "Validation dataset length: 292\n",
      "\n",
      "Starting Training...\n",
      "Epoch 1/500 | Train Loss: 38885370922.082191 | Val Loss: 39653252951.671234\n",
      "Epoch 2/500 | Train Loss: 38882968898.630135 | Val Loss: 39647411775.123291\n",
      "Epoch 3/500 | Train Loss: 38869442616.109589 | Val Loss: 39619934404.383560\n",
      "Epoch 4/500 | Train Loss: 38817453434.739723 | Val Loss: 39530671454.684929\n",
      "Epoch 5/500 | Train Loss: 38672402572.273972 | Val Loss: 39302922520.547943\n",
      "Epoch 6/500 | Train Loss: 38331230797.150688 | Val Loss: 38816165270.794518\n",
      "Epoch 7/500 | Train Loss: 37657501976.547943 | Val Loss: 37899009318.575340\n",
      "Epoch 8/500 | Train Loss: 36456105282.630135 | Val Loss: 36357935104.000000\n",
      "Epoch 9/500 | Train Loss: 34500718143.123291 | Val Loss: 33958050381.150684\n",
      "Epoch 10/500 | Train Loss: 31601833703.452053 | Val Loss: 30546922650.301369\n",
      "Epoch 11/500 | Train Loss: 27649055309.150684 | Val Loss: 26122463680.876713\n",
      "Epoch 12/500 | Train Loss: 22735532424.767124 | Val Loss: 20837007219.726028\n",
      "Epoch 13/500 | Train Loss: 17176146368.876713 | Val Loss: 15227098224.219177\n",
      "Epoch 14/500 | Train Loss: 11691685831.890411 | Val Loss: 10044310990.904110\n",
      "Epoch 15/500 | Train Loss: 7104394608.219178 | Val Loss: 6141849214.246575\n",
      "Epoch 16/500 | Train Loss: 3986734090.520548 | Val Loss: 3994859036.054794\n",
      "Epoch 17/500 | Train Loss: 2570210807.232877 | Val Loss: 3038618532.821918\n",
      "Epoch 18/500 | Train Loss: 2094111337.205479 | Val Loss: 2720021041.095891\n",
      "Epoch 19/500 | Train Loss: 1949873867.397260 | Val Loss: 2591904231.452055\n",
      "Epoch 20/500 | Train Loss: 1881168028.054795 | Val Loss: 2497901739.835617\n",
      "Epoch 21/500 | Train Loss: 1826325351.452055 | Val Loss: 2414663799.232877\n",
      "Epoch 22/500 | Train Loss: 1774825142.356164 | Val Loss: 2343164170.520548\n",
      "Epoch 23/500 | Train Loss: 1729582402.630137 | Val Loss: 2276633252.821918\n",
      "Epoch 24/500 | Train Loss: 1687724869.260274 | Val Loss: 2213954658.191781\n",
      "Epoch 25/500 | Train Loss: 1652964623.780822 | Val Loss: 2150632917.917808\n",
      "Epoch 26/500 | Train Loss: 1616901109.479452 | Val Loss: 2104777985.753425\n",
      "Epoch 27/500 | Train Loss: 1586948983.232877 | Val Loss: 2060423020.712329\n",
      "Epoch 28/500 | Train Loss: 1562028270.465753 | Val Loss: 2048412405.479452\n",
      "Epoch 29/500 | Train Loss: 1541224724.164384 | Val Loss: 1996279357.369863\n",
      "Epoch 30/500 | Train Loss: 1518129623.671233 | Val Loss: 1963816598.794521\n",
      "Epoch 31/500 | Train Loss: 1494464702.684932 | Val Loss: 1924932869.260274\n",
      "Epoch 32/500 | Train Loss: 1477845041.972603 | Val Loss: 1889376715.397260\n",
      "Epoch 33/500 | Train Loss: 1455266357.479452 | Val Loss: 1873422586.739726\n",
      "Epoch 34/500 | Train Loss: 1440083112.328767 | Val Loss: 1847410070.794521\n",
      "Epoch 35/500 | Train Loss: 1424779786.520548 | Val Loss: 1810397504.876712\n",
      "Epoch 36/500 | Train Loss: 1406864701.369863 | Val Loss: 1787325459.287671\n",
      "Epoch 37/500 | Train Loss: 1392549474.191781 | Val Loss: 1763297797.260274\n",
      "Epoch 38/500 | Train Loss: 1379675870.684932 | Val Loss: 1737267007.123288\n",
      "Epoch 39/500 | Train Loss: 1367385464.986301 | Val Loss: 1711244438.794521\n",
      "Epoch 40/500 | Train Loss: 1352765725.808219 | Val Loss: 1711601678.027397\n",
      "Epoch 41/500 | Train Loss: 1342462857.643836 | Val Loss: 1697684209.972603\n",
      "Epoch 42/500 | Train Loss: 1332601689.424658 | Val Loss: 1666281735.013699\n",
      "Epoch 43/500 | Train Loss: 1321158329.863014 | Val Loss: 1657052596.602740\n",
      "Epoch 44/500 | Train Loss: 1309837210.301370 | Val Loss: 1637393769.205479\n",
      "Epoch 45/500 | Train Loss: 1299051919.780822 | Val Loss: 1624249585.972603\n",
      "Epoch 46/500 | Train Loss: 1290197511.452055 | Val Loss: 1599887659.835616\n",
      "Epoch 47/500 | Train Loss: 1284665877.041096 | Val Loss: 1603017706.958904\n",
      "Epoch 48/500 | Train Loss: 1273742118.575342 | Val Loss: 1574548853.479452\n",
      "Epoch 49/500 | Train Loss: 1268212439.671233 | Val Loss: 1552907623.452055\n",
      "Epoch 50/500 | Train Loss: 1255007186.410959 | Val Loss: 1565645012.164384\n",
      "Epoch 51/500 | Train Loss: 1247227372.712329 | Val Loss: 1543130744.986301\n",
      "Epoch 52/500 | Train Loss: 1241374544.657534 | Val Loss: 1523668793.863014\n",
      "Epoch 53/500 | Train Loss: 1231998079.123288 | Val Loss: 1526315786.520548\n",
      "Epoch 54/500 | Train Loss: 1222709623.232877 | Val Loss: 1512725802.082192\n",
      "Epoch 55/500 | Train Loss: 1217017427.287671 | Val Loss: 1489155980.273973\n",
      "Epoch 56/500 | Train Loss: 1209945760.876712 | Val Loss: 1492409249.315068\n",
      "Epoch 57/500 | Train Loss: 1202401056.438356 | Val Loss: 1490328628.602740\n",
      "Epoch 58/500 | Train Loss: 1197151502.027397 | Val Loss: 1483163968.876712\n",
      "Epoch 59/500 | Train Loss: 1192593433.424658 | Val Loss: 1453084636.931507\n",
      "Epoch 60/500 | Train Loss: 1184317799.452055 | Val Loss: 1465739726.904109\n",
      "Epoch 61/500 | Train Loss: 1176478722.630137 | Val Loss: 1449450883.506849\n",
      "Epoch 62/500 | Train Loss: 1169973048.109589 | Val Loss: 1445536427.835616\n",
      "Epoch 63/500 | Train Loss: 1164793223.013699 | Val Loss: 1438571875.945205\n",
      "Epoch 64/500 | Train Loss: 1158869304.986301 | Val Loss: 1431574564.821918\n",
      "Epoch 65/500 | Train Loss: 1155902388.602740 | Val Loss: 1441663798.356164\n",
      "Epoch 66/500 | Train Loss: 1149989433.863014 | Val Loss: 1422294598.136986\n",
      "Epoch 67/500 | Train Loss: 1143426367.561644 | Val Loss: 1411502937.424658\n",
      "Epoch 68/500 | Train Loss: 1140949831.890411 | Val Loss: 1399016262.136986\n",
      "Epoch 69/500 | Train Loss: 1134117466.301370 | Val Loss: 1414844138.958904\n",
      "Epoch 70/500 | Train Loss: 1128249723.178082 | Val Loss: 1392300901.698630\n",
      "Epoch 71/500 | Train Loss: 1123044260.821918 | Val Loss: 1388750716.493151\n",
      "Epoch 72/500 | Train Loss: 1117377081.863014 | Val Loss: 1391518279.890411\n",
      "Epoch 73/500 | Train Loss: 1112918817.315068 | Val Loss: 1367254531.506849\n",
      "Epoch 74/500 | Train Loss: 1108155523.506849 | Val Loss: 1373741568.000000\n",
      "Epoch 75/500 | Train Loss: 1103299326.246575 | Val Loss: 1363782070.356164\n",
      "Epoch 76/500 | Train Loss: 1101247842.630137 | Val Loss: 1358443835.616438\n",
      "Epoch 77/500 | Train Loss: 1093831474.849315 | Val Loss: 1360804572.931507\n",
      "Epoch 78/500 | Train Loss: 1095091745.315068 | Val Loss: 1364406897.972603\n",
      "Epoch 79/500 | Train Loss: 1084623468.273973 | Val Loss: 1341061111.232877\n",
      "Epoch 80/500 | Train Loss: 1085070094.904109 | Val Loss: 1345175811.506849\n",
      "Epoch 81/500 | Train Loss: 1083237161.205479 | Val Loss: 1328011232.438356\n",
      "Epoch 82/500 | Train Loss: 1071285309.369863 | Val Loss: 1344505976.986301\n",
      "Epoch 83/500 | Train Loss: 1071904461.150685 | Val Loss: 1334456002.630137\n",
      "Epoch 84/500 | Train Loss: 1061861712.657534 | Val Loss: 1310208187.616438\n",
      "Epoch 85/500 | Train Loss: 1072426488.986301 | Val Loss: 1300408912.657534\n",
      "Epoch 86/500 | Train Loss: 1088576364.712329 | Val Loss: 1387226448.657534\n",
      "Epoch 87/500 | Train Loss: 1057769254.575342 | Val Loss: 1312594824.767123\n",
      "Epoch 88/500 | Train Loss: 1050875151.780822 | Val Loss: 1314992040.328767\n",
      "Epoch 89/500 | Train Loss: 1047368505.863014 | Val Loss: 1315101373.369863\n",
      "Epoch 90/500 | Train Loss: 1045813924.821918 | Val Loss: 1304147050.958904\n",
      "Epoch 91/500 | Train Loss: 1040441043.726027 | Val Loss: 1299524048.657534\n",
      "Epoch 92/500 | Train Loss: 1036277333.917808 | Val Loss: 1288319936.876712\n",
      "Epoch 93/500 | Train Loss: 1033116309.917808 | Val Loss: 1288924821.041096\n",
      "Epoch 94/500 | Train Loss: 1029823304.328767 | Val Loss: 1281468302.027397\n",
      "Epoch 95/500 | Train Loss: 1027763097.863014 | Val Loss: 1282697077.479452\n",
      "Epoch 96/500 | Train Loss: 1023682442.520548 | Val Loss: 1266994672.219178\n",
      "Epoch 97/500 | Train Loss: 1019698102.356164 | Val Loss: 1258791362.630137\n",
      "Epoch 98/500 | Train Loss: 1016984930.191781 | Val Loss: 1270449474.630137\n",
      "Epoch 99/500 | Train Loss: 1016391737.863014 | Val Loss: 1246878919.890411\n",
      "Epoch 100/500 | Train Loss: 1007557919.561644 | Val Loss: 1266768168.328767\n",
      "Epoch 101/500 | Train Loss: 1007855162.301370 | Val Loss: 1256361982.246575\n",
      "Epoch 102/500 | Train Loss: 1005425519.561644 | Val Loss: 1245649735.890411\n",
      "Epoch 103/500 | Train Loss: 1001857145.863014 | Val Loss: 1237725930.958904\n",
      "Epoch 104/500 | Train Loss: 998943152.219178 | Val Loss: 1247221882.739726\n",
      "Epoch 105/500 | Train Loss: 994491241.205480 | Val Loss: 1240736631.232877\n",
      "Epoch 106/500 | Train Loss: 1004712888.986301 | Val Loss: 1270474467.945205\n",
      "Epoch 107/500 | Train Loss: 991773127.890411 | Val Loss: 1240939390.246575\n",
      "Epoch 108/500 | Train Loss: 986944630.356164 | Val Loss: 1242048064.876712\n",
      "Epoch 109/500 | Train Loss: 984448309.917808 | Val Loss: 1240528212.164384\n",
      "Epoch 110/500 | Train Loss: 981914245.041096 | Val Loss: 1228137331.726027\n",
      "Epoch 111/500 | Train Loss: 978577120.438356 | Val Loss: 1223799932.493151\n",
      "Epoch 112/500 | Train Loss: 977232256.000000 | Val Loss: 1214434354.849315\n",
      "Epoch 113/500 | Train Loss: 981077609.643836 | Val Loss: 1228588675.506849\n",
      "Epoch 114/500 | Train Loss: 982683662.465753 | Val Loss: 1207500896.438356\n",
      "Epoch 115/500 | Train Loss: 968351532.712329 | Val Loss: 1219170712.547945\n",
      "Epoch 116/500 | Train Loss: 969557684.602740 | Val Loss: 1206844787.726027\n",
      "Epoch 117/500 | Train Loss: 963080600.328767 | Val Loss: 1195417010.849315\n",
      "Epoch 118/500 | Train Loss: 960067129.863014 | Val Loss: 1198829105.095891\n",
      "Epoch 119/500 | Train Loss: 959469603.945205 | Val Loss: 1195979255.232877\n",
      "Epoch 120/500 | Train Loss: 955933287.452055 | Val Loss: 1189904038.575342\n",
      "Epoch 121/500 | Train Loss: 958260782.465753 | Val Loss: 1185620760.547945\n",
      "Epoch 122/500 | Train Loss: 951205165.150685 | Val Loss: 1197179625.205479\n",
      "Epoch 123/500 | Train Loss: 951011750.575342 | Val Loss: 1183222768.219178\n",
      "Epoch 124/500 | Train Loss: 946571409.534247 | Val Loss: 1177484738.630137\n",
      "Epoch 125/500 | Train Loss: 943587753.205480 | Val Loss: 1176202329.424658\n",
      "Epoch 126/500 | Train Loss: 943544267.397260 | Val Loss: 1174989750.356164\n",
      "Epoch 127/500 | Train Loss: 940068900.821918 | Val Loss: 1161033976.986301\n",
      "Epoch 128/500 | Train Loss: 938978266.739726 | Val Loss: 1164164583.452055\n",
      "Epoch 129/500 | Train Loss: 937391132.054795 | Val Loss: 1178759555.506849\n",
      "Epoch 130/500 | Train Loss: 932384946.849315 | Val Loss: 1154698073.424658\n",
      "Epoch 131/500 | Train Loss: 932671667.287671 | Val Loss: 1162440535.671233\n",
      "Epoch 132/500 | Train Loss: 929918213.698630 | Val Loss: 1163778535.452055\n",
      "Epoch 133/500 | Train Loss: 928501462.794520 | Val Loss: 1155448199.013699\n",
      "Epoch 134/500 | Train Loss: 926821971.287671 | Val Loss: 1159926045.808219\n",
      "Epoch 135/500 | Train Loss: 924447907.068493 | Val Loss: 1144309593.424658\n",
      "Epoch 136/500 | Train Loss: 922149736.328767 | Val Loss: 1157671199.561644\n",
      "Epoch 137/500 | Train Loss: 922827521.315068 | Val Loss: 1144304366.465753\n",
      "Epoch 138/500 | Train Loss: 921195605.041096 | Val Loss: 1154608683.835616\n",
      "Epoch 139/500 | Train Loss: 918225189.260274 | Val Loss: 1138346262.794521\n",
      "Epoch 140/500 | Train Loss: 917573822.684932 | Val Loss: 1134950384.219178\n",
      "Epoch 141/500 | Train Loss: 912485833.643836 | Val Loss: 1154716698.301370\n",
      "Epoch 142/500 | Train Loss: 910063642.301370 | Val Loss: 1136480119.232877\n",
      "Epoch 143/500 | Train Loss: 914271168.000000 | Val Loss: 1125455021.589041\n",
      "Epoch 144/500 | Train Loss: 906388266.739726 | Val Loss: 1135382098.410959\n",
      "Epoch 145/500 | Train Loss: 904708057.424658 | Val Loss: 1129503489.753425\n",
      "Epoch 146/500 | Train Loss: 904565235.068493 | Val Loss: 1127214464.000000\n",
      "Epoch 147/500 | Train Loss: 902201354.958904 | Val Loss: 1128403613.808219\n",
      "Epoch 148/500 | Train Loss: 900160898.630137 | Val Loss: 1124672345.424658\n",
      "Epoch 149/500 | Train Loss: 898722069.917808 | Val Loss: 1127564424.767123\n",
      "Epoch 150/500 | Train Loss: 895570304.000000 | Val Loss: 1120267539.287671\n",
      "Epoch 151/500 | Train Loss: 894977197.150685 | Val Loss: 1127104322.630137\n",
      "Epoch 152/500 | Train Loss: 896807944.767123 | Val Loss: 1112908412.493151\n",
      "Epoch 153/500 | Train Loss: 897011960.109589 | Val Loss: 1116111419.616438\n",
      "Epoch 154/500 | Train Loss: 894911569.095890 | Val Loss: 1115450429.369863\n",
      "Epoch 155/500 | Train Loss: 889118998.794520 | Val Loss: 1110053144.547945\n",
      "Epoch 156/500 | Train Loss: 887762232.109589 | Val Loss: 1112115310.465753\n",
      "Epoch 157/500 | Train Loss: 887923717.260274 | Val Loss: 1101117680.219178\n",
      "Epoch 158/500 | Train Loss: 886183594.520548 | Val Loss: 1124375644.931507\n",
      "Epoch 159/500 | Train Loss: 887579118.465753 | Val Loss: 1094216018.410959\n",
      "Epoch 160/500 | Train Loss: 883401982.246575 | Val Loss: 1112143049.643836\n",
      "Epoch 161/500 | Train Loss: 890688113.534247 | Val Loss: 1138291007.123288\n",
      "Epoch 162/500 | Train Loss: 892739273.643836 | Val Loss: 1110440739.068493\n",
      "Epoch 163/500 | Train Loss: 878004220.493151 | Val Loss: 1124331195.616438\n",
      "Epoch 164/500 | Train Loss: 880003368.767123 | Val Loss: 1101238321.095891\n",
      "Epoch 165/500 | Train Loss: 881680144.657534 | Val Loss: 1114056509.369863\n",
      "Epoch 166/500 | Train Loss: 872554772.164384 | Val Loss: 1102497930.520548\n",
      "Epoch 167/500 | Train Loss: 874553422.904110 | Val Loss: 1093315072.000000\n",
      "Epoch 168/500 | Train Loss: 867928956.054795 | Val Loss: 1106482062.027397\n",
      "Epoch 169/500 | Train Loss: 871226886.136986 | Val Loss: 1094269503.123288\n",
      "Epoch 170/500 | Train Loss: 869215689.205480 | Val Loss: 1093534292.164384\n",
      "Epoch 171/500 | Train Loss: 863987750.575342 | Val Loss: 1097585462.356164\n",
      "Epoch 172/500 | Train Loss: 867097413.260274 | Val Loss: 1095692987.616438\n",
      "Epoch 173/500 | Train Loss: 861942389.917808 | Val Loss: 1092041189.698630\n",
      "Epoch 174/500 | Train Loss: 862380572.931507 | Val Loss: 1087712650.520548\n",
      "Epoch 175/500 | Train Loss: 861678560.438356 | Val Loss: 1084192757.479452\n",
      "Epoch 176/500 | Train Loss: 867032151.452055 | Val Loss: 1122008339.287671\n",
      "Epoch 177/500 | Train Loss: 860295008.000000 | Val Loss: 1095323304.328767\n",
      "Epoch 178/500 | Train Loss: 860718606.465753 | Val Loss: 1093813761.753425\n",
      "Epoch 179/500 | Train Loss: 856950477.150685 | Val Loss: 1095960463.780822\n",
      "Epoch 180/500 | Train Loss: 854619279.780822 | Val Loss: 1092302442.958904\n",
      "Epoch 181/500 | Train Loss: 853361322.082192 | Val Loss: 1081769550.904109\n",
      "Epoch 182/500 | Train Loss: 851160982.356164 | Val Loss: 1085322585.424658\n",
      "Epoch 183/500 | Train Loss: 854315768.109589 | Val Loss: 1084236338.849315\n",
      "Epoch 184/500 | Train Loss: 851799900.931507 | Val Loss: 1078814360.547945\n",
      "Epoch 185/500 | Train Loss: 847379034.301370 | Val Loss: 1081210352.219178\n",
      "Epoch 186/500 | Train Loss: 847172727.890411 | Val Loss: 1074597433.863014\n",
      "Epoch 187/500 | Train Loss: 846529174.794520 | Val Loss: 1076124599.232877\n",
      "Epoch 188/500 | Train Loss: 848708471.232877 | Val Loss: 1073416037.698630\n",
      "Epoch 189/500 | Train Loss: 844832572.493151 | Val Loss: 1076225208.986301\n",
      "Epoch 190/500 | Train Loss: 842300274.410959 | Val Loss: 1075370375.890411\n",
      "Epoch 191/500 | Train Loss: 842751623.013699 | Val Loss: 1067048908.273973\n",
      "Epoch 192/500 | Train Loss: 845974610.410959 | Val Loss: 1065464734.684932\n",
      "Epoch 193/500 | Train Loss: 839462904.986301 | Val Loss: 1065021192.767123\n",
      "Epoch 194/500 | Train Loss: 838611092.164384 | Val Loss: 1056870025.643836\n",
      "Epoch 195/500 | Train Loss: 838861409.315068 | Val Loss: 1059965124.383562\n",
      "Epoch 196/500 | Train Loss: 837253619.287671 | Val Loss: 1070960988.931507\n",
      "Epoch 197/500 | Train Loss: 836658472.328767 | Val Loss: 1063117084.054795\n",
      "Epoch 198/500 | Train Loss: 835585288.767123 | Val Loss: 1059747800.547945\n",
      "Epoch 199/500 | Train Loss: 837145150.246575 | Val Loss: 1064700619.397260\n",
      "Epoch 200/500 | Train Loss: 837984496.219178 | Val Loss: 1057591760.657534\n",
      "Epoch 201/500 | Train Loss: 840907395.945205 | Val Loss: 1067905184.438356\n",
      "Epoch 202/500 | Train Loss: 827469227.397260 | Val Loss: 1049380188.931507\n",
      "Epoch 203/500 | Train Loss: 830836175.780822 | Val Loss: 1062523666.410959\n",
      "Epoch 204/500 | Train Loss: 829346813.369863 | Val Loss: 1058418025.205480\n",
      "Epoch 205/500 | Train Loss: 828482087.013699 | Val Loss: 1053139882.958904\n",
      "Epoch 206/500 | Train Loss: 831474875.616438 | Val Loss: 1055269036.712329\n",
      "Epoch 207/500 | Train Loss: 826960938.082192 | Val Loss: 1048571712.000000\n",
      "Epoch 208/500 | Train Loss: 824893944.109589 | Val Loss: 1050412352.876712\n",
      "Epoch 209/500 | Train Loss: 824457256.328767 | Val Loss: 1049555306.082192\n",
      "Epoch 210/500 | Train Loss: 823386183.013699 | Val Loss: 1047746389.041096\n",
      "Epoch 211/500 | Train Loss: 823505215.123288 | Val Loss: 1046205258.520548\n",
      "Epoch 212/500 | Train Loss: 823352597.479452 | Val Loss: 1045251399.890411\n",
      "Epoch 213/500 | Train Loss: 823085732.821918 | Val Loss: 1041356458.958904\n",
      "Epoch 214/500 | Train Loss: 820965815.671233 | Val Loss: 1042377686.794520\n",
      "Epoch 215/500 | Train Loss: 819561673.643836 | Val Loss: 1045157059.506849\n",
      "Epoch 216/500 | Train Loss: 818661826.630137 | Val Loss: 1038484362.520548\n",
      "Epoch 217/500 | Train Loss: 818899343.342466 | Val Loss: 1039074371.506849\n",
      "Epoch 218/500 | Train Loss: 817053134.904110 | Val Loss: 1050817091.506849\n",
      "Epoch 219/500 | Train Loss: 815755233.315068 | Val Loss: 1038077401.424658\n",
      "Epoch 220/500 | Train Loss: 817333434.301370 | Val Loss: 1039926191.342466\n",
      "Epoch 221/500 | Train Loss: 813929086.465753 | Val Loss: 1039105732.383562\n",
      "Epoch 222/500 | Train Loss: 813053365.479452 | Val Loss: 1038296201.643836\n",
      "Epoch 223/500 | Train Loss: 811389361.972603 | Val Loss: 1042126986.520548\n",
      "Epoch 224/500 | Train Loss: 811198957.150685 | Val Loss: 1038688937.205480\n",
      "Epoch 225/500 | Train Loss: 810536824.986301 | Val Loss: 1032409357.150685\n",
      "Epoch 226/500 | Train Loss: 815923636.602740 | Val Loss: 1039612764.931507\n",
      "Epoch 227/500 | Train Loss: 807693080.547945 | Val Loss: 1032748156.493151\n",
      "Epoch 228/500 | Train Loss: 808160384.000000 | Val Loss: 1034572433.534247\n",
      "Epoch 229/500 | Train Loss: 814094664.767123 | Val Loss: 1030341127.890411\n",
      "Epoch 230/500 | Train Loss: 809678389.479452 | Val Loss: 1034964982.356164\n",
      "Epoch 231/500 | Train Loss: 805062625.095890 | Val Loss: 1027931448.109589\n",
      "Epoch 232/500 | Train Loss: 806532029.150685 | Val Loss: 1034875221.917808\n",
      "Epoch 233/500 | Train Loss: 806789851.178082 | Val Loss: 1029558512.219178\n",
      "Epoch 234/500 | Train Loss: 802008337.534247 | Val Loss: 1031711072.438356\n",
      "Epoch 235/500 | Train Loss: 801597080.547945 | Val Loss: 1032137226.520548\n",
      "Epoch 236/500 | Train Loss: 806356092.493151 | Val Loss: 1024069404.931507\n",
      "Epoch 237/500 | Train Loss: 800985879.452055 | Val Loss: 1032905605.260274\n",
      "Epoch 238/500 | Train Loss: 800897679.780822 | Val Loss: 1022515507.726027\n",
      "Epoch 239/500 | Train Loss: 798847144.328767 | Val Loss: 1025746076.931507\n",
      "Epoch 240/500 | Train Loss: 800050572.931507 | Val Loss: 1032352063.123288\n",
      "Epoch 241/500 | Train Loss: 799202211.945205 | Val Loss: 1019760960.876712\n",
      "Epoch 242/500 | Train Loss: 795743676.493151 | Val Loss: 1030920540.054795\n",
      "Epoch 243/500 | Train Loss: 796057580.712329 | Val Loss: 1022743818.520548\n",
      "Epoch 244/500 | Train Loss: 799986227.726027 | Val Loss: 1011843233.315068\n",
      "Epoch 245/500 | Train Loss: 793115954.849315 | Val Loss: 1017684994.630137\n",
      "Epoch 246/500 | Train Loss: 795487068.931507 | Val Loss: 1010716753.534247\n",
      "Epoch 247/500 | Train Loss: 802527127.232877 | Val Loss: 1028089700.821918\n",
      "Epoch 248/500 | Train Loss: 796167712.438356 | Val Loss: 1021258948.383562\n",
      "Epoch 249/500 | Train Loss: 794136239.780822 | Val Loss: 1027845825.753425\n",
      "Epoch 250/500 | Train Loss: 789220327.452055 | Val Loss: 1017237362.849315\n",
      "Epoch 251/500 | Train Loss: 787595285.041096 | Val Loss: 1023364690.410959\n",
      "Epoch 252/500 | Train Loss: 789312368.657534 | Val Loss: 1014386623.123288\n",
      "Epoch 253/500 | Train Loss: 789369745.095890 | Val Loss: 1013916963.068493\n",
      "Epoch 254/500 | Train Loss: 787074178.191781 | Val Loss: 1015902371.068493\n",
      "Epoch 255/500 | Train Loss: 787171936.438356 | Val Loss: 1015893682.849315\n",
      "Epoch 256/500 | Train Loss: 784323129.863014 | Val Loss: 1009387308.712329\n",
      "Epoch 257/500 | Train Loss: 785876436.164384 | Val Loss: 1005713388.712329\n",
      "Epoch 258/500 | Train Loss: 783232484.821918 | Val Loss: 1012801903.342466\n",
      "Epoch 259/500 | Train Loss: 780884471.232877 | Val Loss: 1003450312.767123\n",
      "Epoch 260/500 | Train Loss: 785203890.849315 | Val Loss: 1005016554.958904\n",
      "Epoch 261/500 | Train Loss: 785090398.684932 | Val Loss: 998525071.780822\n",
      "Epoch 262/500 | Train Loss: 788963048.767123 | Val Loss: 994334336.876712\n",
      "Epoch 263/500 | Train Loss: 787172442.301370 | Val Loss: 1001652907.835616\n",
      "Epoch 264/500 | Train Loss: 782181150.684932 | Val Loss: 994743388.054795\n",
      "Epoch 265/500 | Train Loss: 777011319.232877 | Val Loss: 1004616917.917808\n",
      "Epoch 266/500 | Train Loss: 778226467.506849 | Val Loss: 1016073763.068493\n",
      "Epoch 267/500 | Train Loss: 775554163.726027 | Val Loss: 1004697822.684932\n",
      "Epoch 268/500 | Train Loss: 774818910.684932 | Val Loss: 1005576744.328767\n",
      "Epoch 269/500 | Train Loss: 776070611.068493 | Val Loss: 1000366897.972603\n",
      "Epoch 270/500 | Train Loss: 773933636.383562 | Val Loss: 1003665858.630137\n",
      "Epoch 271/500 | Train Loss: 772995594.739726 | Val Loss: 1003670286.904110\n",
      "Epoch 272/500 | Train Loss: 776174122.958904 | Val Loss: 1000756238.904110\n",
      "Epoch 273/500 | Train Loss: 771035987.287671 | Val Loss: 1004541168.219178\n",
      "Epoch 274/500 | Train Loss: 771995907.726027 | Val Loss: 1005645714.410959\n",
      "Epoch 275/500 | Train Loss: 770574723.945205 | Val Loss: 996427831.232877\n",
      "Epoch 276/500 | Train Loss: 774994247.890411 | Val Loss: 997553037.150685\n",
      "Epoch 277/500 | Train Loss: 769205279.561644 | Val Loss: 1001162653.808219\n",
      "Epoch 278/500 | Train Loss: 768947697.095890 | Val Loss: 1001935110.136986\n",
      "Epoch 279/500 | Train Loss: 768300487.890411 | Val Loss: 997837396.164384\n",
      "Epoch 280/500 | Train Loss: 768310619.616438 | Val Loss: 1002081863.013699\n",
      "Epoch 281/500 | Train Loss: 771446044.931507 | Val Loss: 998661483.835616\n",
      "Epoch 282/500 | Train Loss: 770198362.301370 | Val Loss: 990919443.287671\n",
      "Epoch 283/500 | Train Loss: 771197373.369863 | Val Loss: 984915761.972603\n",
      "Epoch 284/500 | Train Loss: 774823340.273973 | Val Loss: 1001227095.671233\n",
      "Epoch 285/500 | Train Loss: 770805991.452055 | Val Loss: 990139388.493151\n",
      "Epoch 286/500 | Train Loss: 792935083.616438 | Val Loss: 1027681287.013699\n",
      "Epoch 287/500 | Train Loss: 764745357.150685 | Val Loss: 1000257802.520548\n",
      "Epoch 288/500 | Train Loss: 767314231.232877 | Val Loss: 1014908128.438356\n",
      "Epoch 289/500 | Train Loss: 760223704.328767 | Val Loss: 999773177.863014\n",
      "Epoch 290/500 | Train Loss: 767722052.821918 | Val Loss: 997867712.876712\n",
      "Epoch 291/500 | Train Loss: 763260472.986301 | Val Loss: 1006732016.219178\n",
      "Epoch 292/500 | Train Loss: 758711964.931507 | Val Loss: 994468814.904110\n",
      "Epoch 293/500 | Train Loss: 764388907.397260 | Val Loss: 995965659.178082\n",
      "Epoch 294/500 | Train Loss: 760228152.986301 | Val Loss: 1001420986.739726\n",
      "Epoch 295/500 | Train Loss: 758333570.191781 | Val Loss: 993823500.273973\n",
      "Epoch 296/500 | Train Loss: 757385422.465753 | Val Loss: 988211496.328767\n",
      "Epoch 297/500 | Train Loss: 757752628.164384 | Val Loss: 991493634.630137\n",
      "Epoch 298/500 | Train Loss: 757352918.794520 | Val Loss: 994163610.301370\n",
      "Epoch 299/500 | Train Loss: 757636980.164384 | Val Loss: 993316062.684932\n",
      "Epoch 300/500 | Train Loss: 756518170.958904 | Val Loss: 995810329.424658\n",
      "Epoch 301/500 | Train Loss: 756095913.205480 | Val Loss: 987628319.561644\n",
      "Epoch 302/500 | Train Loss: 757348763.616438 | Val Loss: 997799877.260274\n",
      "Epoch 303/500 | Train Loss: 756977454.465753 | Val Loss: 986419764.602740\n",
      "Epoch 304/500 | Train Loss: 757284512.219178 | Val Loss: 991548519.452055\n",
      "Epoch 305/500 | Train Loss: 753239282.410959 | Val Loss: 988554293.479452\n",
      "Epoch 306/500 | Train Loss: 753710647.232877 | Val Loss: 986247293.369863\n",
      "Epoch 307/500 | Train Loss: 752472712.328767 | Val Loss: 989440727.671233\n",
      "Epoch 308/500 | Train Loss: 755830214.136986 | Val Loss: 989860174.904110\n",
      "Epoch 309/500 | Train Loss: 753362507.178082 | Val Loss: 988854493.808219\n",
      "Epoch 310/500 | Train Loss: 756018403.068493 | Val Loss: 986531039.561644\n",
      "Epoch 311/500 | Train Loss: 752352043.397260 | Val Loss: 984449151.123288\n",
      "Epoch 312/500 | Train Loss: 756278317.150685 | Val Loss: 982345536.876712\n",
      "Epoch 313/500 | Train Loss: 751905030.575342 | Val Loss: 986740537.863014\n",
      "Epoch 314/500 | Train Loss: 752901649.534247 | Val Loss: 985233220.383562\n",
      "Epoch 315/500 | Train Loss: 751899325.369863 | Val Loss: 982503475.726027\n",
      "Epoch 316/500 | Train Loss: 749499876.383562 | Val Loss: 981132700.054795\n",
      "Epoch 317/500 | Train Loss: 749499370.082192 | Val Loss: 989256218.301370\n",
      "Epoch 318/500 | Train Loss: 755271032.986301 | Val Loss: 998972081.972603\n",
      "Epoch 319/500 | Train Loss: 746344740.821918 | Val Loss: 994951514.301370\n",
      "Epoch 320/500 | Train Loss: 751509494.794520 | Val Loss: 997519795.726027\n",
      "Epoch 321/500 | Train Loss: 743929504.438356 | Val Loss: 988449516.712329\n",
      "Epoch 322/500 | Train Loss: 745512156.931507 | Val Loss: 988036390.575342\n",
      "Epoch 323/500 | Train Loss: 744180639.561644 | Val Loss: 986363901.369863\n",
      "Epoch 324/500 | Train Loss: 743776644.821918 | Val Loss: 986940951.671233\n",
      "Epoch 325/500 | Train Loss: 742963489.315068 | Val Loss: 985448183.232877\n",
      "Epoch 326/500 | Train Loss: 741288871.452055 | Val Loss: 988287296.000000\n",
      "Epoch 327/500 | Train Loss: 741658358.356164 | Val Loss: 985280053.479452\n",
      "Epoch 328/500 | Train Loss: 744733410.191781 | Val Loss: 985105517.589041\n",
      "Epoch 329/500 | Train Loss: 742133743.780822 | Val Loss: 979939496.328767\n",
      "Epoch 330/500 | Train Loss: 739810834.410959 | Val Loss: 990706903.671233\n",
      "Epoch 331/500 | Train Loss: 740682802.849315 | Val Loss: 985597984.438356\n",
      "Epoch 332/500 | Train Loss: 739581331.068493 | Val Loss: 983056708.383562\n",
      "Epoch 333/500 | Train Loss: 741256408.986301 | Val Loss: 977427850.520548\n",
      "Epoch 334/500 | Train Loss: 739349968.219178 | Val Loss: 982961028.383562\n",
      "Epoch 335/500 | Train Loss: 739780385.315068 | Val Loss: 984172093.369863\n",
      "Epoch 336/500 | Train Loss: 736190835.726027 | Val Loss: 978028494.904110\n",
      "Epoch 337/500 | Train Loss: 736104635.616438 | Val Loss: 978223656.328767\n",
      "Epoch 338/500 | Train Loss: 737087587.068493 | Val Loss: 991521031.890411\n",
      "Epoch 339/500 | Train Loss: 740790253.589041 | Val Loss: 981741794.191781\n",
      "Epoch 340/500 | Train Loss: 736064505.424658 | Val Loss: 989457594.739726\n",
      "Epoch 341/500 | Train Loss: 738406843.616438 | Val Loss: 973921515.835616\n",
      "Epoch 342/500 | Train Loss: 736634556.054795 | Val Loss: 975122483.726027\n",
      "Epoch 343/500 | Train Loss: 735026818.630137 | Val Loss: 983247342.465753\n",
      "Epoch 344/500 | Train Loss: 735747964.493151 | Val Loss: 972848351.561644\n",
      "Epoch 345/500 | Train Loss: 732160910.904110 | Val Loss: 979195816.328767\n",
      "Epoch 346/500 | Train Loss: 735923441.095890 | Val Loss: 979697907.726027\n",
      "Epoch 347/500 | Train Loss: 733367075.945205 | Val Loss: 975368682.082192\n",
      "Epoch 348/500 | Train Loss: 731431003.945205 | Val Loss: 980561666.630137\n",
      "Epoch 349/500 | Train Loss: 733184680.328767 | Val Loss: 974991797.479452\n",
      "Epoch 350/500 | Train Loss: 730473707.835616 | Val Loss: 975879186.410959\n",
      "Epoch 351/500 | Train Loss: 730659955.287671 | Val Loss: 978183135.561644\n",
      "Epoch 352/500 | Train Loss: 729837311.123288 | Val Loss: 975565036.712329\n",
      "Epoch 353/500 | Train Loss: 730540528.219178 | Val Loss: 974401838.465753\n",
      "Epoch 354/500 | Train Loss: 733277895.890411 | Val Loss: 968254832.219178\n",
      "Epoch 355/500 | Train Loss: 737407236.383562 | Val Loss: 971077509.260274\n",
      "Epoch 356/500 | Train Loss: 732849832.328767 | Val Loss: 969072302.465753\n",
      "Epoch 357/500 | Train Loss: 729889325.589041 | Val Loss: 977838428.931507\n",
      "Epoch 358/500 | Train Loss: 727066826.082192 | Val Loss: 977410398.684932\n",
      "Epoch 359/500 | Train Loss: 729561394.849315 | Val Loss: 966903680.876712\n",
      "Epoch 360/500 | Train Loss: 727942905.424658 | Val Loss: 982795637.479452\n",
      "Epoch 361/500 | Train Loss: 725790906.520548 | Val Loss: 969436882.410959\n",
      "Epoch 362/500 | Train Loss: 725066030.246575 | Val Loss: 976068077.589041\n",
      "Epoch 363/500 | Train Loss: 722972604.054795 | Val Loss: 970732774.575342\n",
      "Epoch 364/500 | Train Loss: 724280711.452055 | Val Loss: 967287219.726027\n",
      "Epoch 365/500 | Train Loss: 722137849.863014 | Val Loss: 970066324.164384\n",
      "Epoch 366/500 | Train Loss: 724665278.684932 | Val Loss: 964726883.945205\n",
      "Epoch 367/500 | Train Loss: 722413632.876712 | Val Loss: 968981688.986301\n",
      "Epoch 368/500 | Train Loss: 724287835.178082 | Val Loss: 968423853.589041\n",
      "Epoch 369/500 | Train Loss: 722376433.534247 | Val Loss: 968421122.630137\n",
      "Epoch 370/500 | Train Loss: 720102807.671233 | Val Loss: 967277642.520548\n",
      "Epoch 371/500 | Train Loss: 720269078.794520 | Val Loss: 970400324.383562\n",
      "Epoch 372/500 | Train Loss: 720889079.232877 | Val Loss: 969658298.739726\n",
      "Epoch 373/500 | Train Loss: 720682392.547945 | Val Loss: 969964053.041096\n",
      "Epoch 374/500 | Train Loss: 718088043.397260 | Val Loss: 963077057.753425\n",
      "Epoch 375/500 | Train Loss: 719161035.397260 | Val Loss: 965376421.698630\n",
      "Epoch 376/500 | Train Loss: 717544628.602740 | Val Loss: 964867881.205480\n",
      "Epoch 377/500 | Train Loss: 717328836.383562 | Val Loss: 965398180.821918\n",
      "Epoch 378/500 | Train Loss: 719900901.698630 | Val Loss: 965091898.739726\n",
      "Epoch 379/500 | Train Loss: 716319599.342466 | Val Loss: 959625260.712329\n",
      "Epoch 380/500 | Train Loss: 718679147.835616 | Val Loss: 958220940.273973\n",
      "Epoch 381/500 | Train Loss: 718763843.945205 | Val Loss: 955574956.712329\n",
      "Epoch 382/500 | Train Loss: 714502071.890411 | Val Loss: 966017774.465753\n",
      "Epoch 383/500 | Train Loss: 714123225.424658 | Val Loss: 958834256.657534\n",
      "Epoch 384/500 | Train Loss: 713145224.328767 | Val Loss: 959440333.150685\n",
      "Epoch 385/500 | Train Loss: 715615904.000000 | Val Loss: 962540849.972603\n",
      "Epoch 386/500 | Train Loss: 715785771.835616 | Val Loss: 960390227.287671\n",
      "Epoch 387/500 | Train Loss: 712110161.534247 | Val Loss: 964160491.835616\n",
      "Epoch 388/500 | Train Loss: 712881155.726027 | Val Loss: 957538797.589041\n",
      "Epoch 389/500 | Train Loss: 711773121.753425 | Val Loss: 961638258.849315\n",
      "Epoch 390/500 | Train Loss: 711908482.630137 | Val Loss: 955271557.260274\n",
      "Epoch 391/500 | Train Loss: 709813379.287671 | Val Loss: 957790535.013699\n",
      "Epoch 392/500 | Train Loss: 710340238.465753 | Val Loss: 958195598.904110\n",
      "Epoch 393/500 | Train Loss: 710401516.712329 | Val Loss: 953638736.657534\n",
      "Epoch 394/500 | Train Loss: 712685122.630137 | Val Loss: 953254199.232877\n",
      "Epoch 395/500 | Train Loss: 709373939.726027 | Val Loss: 956733231.342466\n",
      "Epoch 396/500 | Train Loss: 709322618.739726 | Val Loss: 959337085.369863\n",
      "Epoch 397/500 | Train Loss: 709980558.027397 | Val Loss: 960511672.986301\n",
      "Epoch 398/500 | Train Loss: 706557090.191781 | Val Loss: 957431530.958904\n",
      "Epoch 399/500 | Train Loss: 707304882.849315 | Val Loss: 956344285.808219\n",
      "Epoch 400/500 | Train Loss: 712576950.356164 | Val Loss: 972489618.410959\n",
      "Epoch 401/500 | Train Loss: 705716737.315068 | Val Loss: 966218911.561644\n",
      "Epoch 402/500 | Train Loss: 706420514.849315 | Val Loss: 969452450.191781\n",
      "Epoch 403/500 | Train Loss: 713086628.821918 | Val Loss: 960425370.301370\n",
      "Epoch 404/500 | Train Loss: 720332941.150685 | Val Loss: 983311656.328767\n",
      "Epoch 405/500 | Train Loss: 704437361.095890 | Val Loss: 975665406.246575\n",
      "Epoch 406/500 | Train Loss: 703581319.890411 | Val Loss: 967692159.123288\n",
      "Epoch 407/500 | Train Loss: 702318534.136986 | Val Loss: 966866751.123288\n",
      "Epoch 408/500 | Train Loss: 702838329.863014 | Val Loss: 961916371.287671\n",
      "Epoch 409/500 | Train Loss: 702809286.136986 | Val Loss: 964592525.150685\n",
      "Epoch 410/500 | Train Loss: 702060671.123288 | Val Loss: 957808028.054795\n",
      "Epoch 411/500 | Train Loss: 708540851.726027 | Val Loss: 976770453.917808\n",
      "Epoch 412/500 | Train Loss: 703167634.849315 | Val Loss: 963738090.958904\n",
      "Epoch 413/500 | Train Loss: 700466275.726027 | Val Loss: 973951786.958904\n",
      "Epoch 414/500 | Train Loss: 699385248.876712 | Val Loss: 963862577.972603\n",
      "Epoch 415/500 | Train Loss: 702366402.630137 | Val Loss: 960912340.164384\n",
      "Epoch 416/500 | Train Loss: 700332988.493151 | Val Loss: 965051211.397260\n",
      "Epoch 417/500 | Train Loss: 697647686.794520 | Val Loss: 960168409.424658\n",
      "Epoch 418/500 | Train Loss: 698283996.054795 | Val Loss: 959648692.602740\n",
      "Epoch 419/500 | Train Loss: 699504129.753425 | Val Loss: 959520655.780822\n",
      "Epoch 420/500 | Train Loss: 697802146.191781 | Val Loss: 960353894.575342\n",
      "Epoch 421/500 | Train Loss: 695565769.205480 | Val Loss: 958412804.383562\n",
      "Epoch 422/500 | Train Loss: 695064882.849315 | Val Loss: 955090993.095890\n",
      "Epoch 423/500 | Train Loss: 697943421.369863 | Val Loss: 964040603.178082\n",
      "Epoch 424/500 | Train Loss: 694466983.890411 | Val Loss: 952219794.410959\n",
      "Epoch 425/500 | Train Loss: 694545529.424658 | Val Loss: 956915866.301370\n",
      "Epoch 426/500 | Train Loss: 698980654.465753 | Val Loss: 950062464.876712\n",
      "Epoch 427/500 | Train Loss: 699530902.794520 | Val Loss: 963107624.328767\n",
      "Epoch 428/500 | Train Loss: 694061686.356164 | Val Loss: 958211211.397260\n",
      "Epoch 429/500 | Train Loss: 693110122.082192 | Val Loss: 957740226.630137\n",
      "Epoch 430/500 | Train Loss: 691544926.684932 | Val Loss: 960296807.452055\n",
      "Epoch 431/500 | Train Loss: 693704960.876712 | Val Loss: 952640483.068493\n",
      "Epoch 432/500 | Train Loss: 690215858.849315 | Val Loss: 959346380.273973\n",
      "Epoch 433/500 | Train Loss: 690103245.589041 | Val Loss: 953596337.972603\n",
      "Epoch 434/500 | Train Loss: 692925578.082192 | Val Loss: 958462515.726027\n",
      "Epoch 435/500 | Train Loss: 690586211.506849 | Val Loss: 954033123.068493\n",
      "Epoch 436/500 | Train Loss: 691356161.753425 | Val Loss: 951874801.095890\n",
      "Epoch 437/500 | Train Loss: 691678221.369863 | Val Loss: 953388880.657534\n",
      "Epoch 438/500 | Train Loss: 688525940.602740 | Val Loss: 951611120.219178\n",
      "Epoch 439/500 | Train Loss: 687098878.684932 | Val Loss: 948839299.506849\n",
      "Epoch 440/500 | Train Loss: 687472127.123288 | Val Loss: 952667022.904110\n",
      "Epoch 441/500 | Train Loss: 686600265.205480 | Val Loss: 950817954.191781\n",
      "Epoch 442/500 | Train Loss: 686583875.506849 | Val Loss: 952878066.849315\n",
      "Epoch 443/500 | Train Loss: 689863421.808219 | Val Loss: 948174315.835616\n",
      "Epoch 444/500 | Train Loss: 685158219.397260 | Val Loss: 945398509.589041\n",
      "Epoch 445/500 | Train Loss: 686839608.986301 | Val Loss: 948834517.917808\n",
      "Epoch 446/500 | Train Loss: 688909721.424658 | Val Loss: 952495629.150685\n",
      "Epoch 447/500 | Train Loss: 685224108.712329 | Val Loss: 943941860.821918\n",
      "Epoch 448/500 | Train Loss: 684369248.438356 | Val Loss: 946386638.904110\n",
      "Epoch 449/500 | Train Loss: 682231609.205480 | Val Loss: 944685963.397260\n",
      "Epoch 450/500 | Train Loss: 682270523.616438 | Val Loss: 941642956.273973\n",
      "Epoch 451/500 | Train Loss: 681624255.123288 | Val Loss: 942901739.835616\n",
      "Epoch 452/500 | Train Loss: 682928745.205480 | Val Loss: 948223295.123288\n",
      "Epoch 453/500 | Train Loss: 683717885.369863 | Val Loss: 963142699.835616\n",
      "Epoch 454/500 | Train Loss: 684494957.260274 | Val Loss: 948603660.273973\n",
      "Epoch 455/500 | Train Loss: 679808317.369863 | Val Loss: 953741717.917808\n",
      "Epoch 456/500 | Train Loss: 683749093.698630 | Val Loss: 954500970.082192\n",
      "Epoch 457/500 | Train Loss: 683276414.027397 | Val Loss: 946203546.301370\n",
      "Epoch 458/500 | Train Loss: 680949757.369863 | Val Loss: 950352242.849315\n",
      "Epoch 459/500 | Train Loss: 679937976.547945 | Val Loss: 952822460.493151\n",
      "Epoch 460/500 | Train Loss: 680566410.520548 | Val Loss: 940970923.835616\n",
      "Epoch 461/500 | Train Loss: 677662013.369863 | Val Loss: 947573656.547945\n",
      "Epoch 462/500 | Train Loss: 680081961.205480 | Val Loss: 940298368.000000\n",
      "Epoch 463/500 | Train Loss: 681702784.876712 | Val Loss: 946673508.821918\n",
      "Epoch 464/500 | Train Loss: 680038424.547945 | Val Loss: 945236373.917808\n",
      "Epoch 465/500 | Train Loss: 674610805.041096 | Val Loss: 940773092.821918\n",
      "Epoch 466/500 | Train Loss: 675960386.630137 | Val Loss: 936035192.109589\n",
      "Epoch 467/500 | Train Loss: 680171382.356164 | Val Loss: 938834221.589041\n",
      "Epoch 468/500 | Train Loss: 675114241.753425 | Val Loss: 937018157.589041\n",
      "Epoch 469/500 | Train Loss: 674407287.671233 | Val Loss: 935969377.315068\n",
      "Epoch 470/500 | Train Loss: 671453979.616438 | Val Loss: 936976466.410959\n",
      "Epoch 471/500 | Train Loss: 671920322.191781 | Val Loss: 937628593.972603\n",
      "Epoch 472/500 | Train Loss: 671757418.520548 | Val Loss: 938807121.534247\n",
      "Epoch 473/500 | Train Loss: 672984903.890411 | Val Loss: 938735852.712329\n",
      "Epoch 474/500 | Train Loss: 673545434.301370 | Val Loss: 932159569.534247\n",
      "Epoch 475/500 | Train Loss: 671667650.191781 | Val Loss: 940658155.835616\n",
      "Epoch 476/500 | Train Loss: 673206881.315068 | Val Loss: 931866456.547945\n",
      "Epoch 477/500 | Train Loss: 671006123.945205 | Val Loss: 935979292.054795\n",
      "Epoch 478/500 | Train Loss: 670215690.520548 | Val Loss: 941951592.328767\n",
      "Epoch 479/500 | Train Loss: 670604924.493151 | Val Loss: 933108422.136986\n",
      "Epoch 480/500 | Train Loss: 669525107.726027 | Val Loss: 934574873.424658\n",
      "Epoch 481/500 | Train Loss: 667543034.739726 | Val Loss: 935136128.000000\n",
      "Epoch 482/500 | Train Loss: 667953738.520548 | Val Loss: 932808623.342466\n",
      "Epoch 483/500 | Train Loss: 672766653.369863 | Val Loss: 923091990.794520\n",
      "Epoch 484/500 | Train Loss: 671574632.109589 | Val Loss: 928434417.972603\n",
      "Epoch 485/500 | Train Loss: 667394241.753425 | Val Loss: 929206430.684932\n",
      "Epoch 486/500 | Train Loss: 668893126.575342 | Val Loss: 929172871.013699\n",
      "Epoch 487/500 | Train Loss: 665014300.931507 | Val Loss: 931886411.397260\n",
      "Epoch 488/500 | Train Loss: 664840508.054795 | Val Loss: 927271937.753425\n",
      "Epoch 489/500 | Train Loss: 665101973.917808 | Val Loss: 931510300.054795\n",
      "Epoch 490/500 | Train Loss: 665634071.232877 | Val Loss: 927635498.958904\n",
      "Epoch 491/500 | Train Loss: 669484134.575342 | Val Loss: 931090915.068493\n",
      "Epoch 492/500 | Train Loss: 663854066.849315 | Val Loss: 934837608.328767\n",
      "Epoch 493/500 | Train Loss: 662774181.698630 | Val Loss: 927752207.780822\n",
      "Epoch 494/500 | Train Loss: 663267382.575342 | Val Loss: 930104142.904110\n",
      "Epoch 495/500 | Train Loss: 661838014.684932 | Val Loss: 930095185.534247\n",
      "Epoch 496/500 | Train Loss: 662024454.136986 | Val Loss: 928499171.945205\n",
      "Epoch 497/500 | Train Loss: 664223196.931507 | Val Loss: 928613512.767123\n",
      "Epoch 498/500 | Train Loss: 661468530.849315 | Val Loss: 924297597.369863\n",
      "Epoch 499/500 | Train Loss: 659864316.054795 | Val Loss: 931430471.890411\n",
      "Epoch 500/500 | Train Loss: 659767977.643836 | Val Loss: 927079886.904110\n"
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
