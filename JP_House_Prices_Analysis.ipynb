{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-06T07:29:36.673339Z",
     "start_time": "2025-05-06T07:29:36.643960Z"
    }
   },
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import helpers\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "train_df = pd.read_csv('data/train.csv')\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "\n",
    "target_col = \"SalePrice\"\n",
    "X_train = train_df.drop(columns=[target_col])\n",
    "Y_train = train_df[target_col]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_train, Y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# I cross-referenced missing values with expected missing values from the description, electrical has one unexpected\n",
    "# missing value checks done in EDA_Analysis\n",
    "\n",
    "mode_electrical = X_train['Electrical'].mode()\n",
    "X_train['Electrical'] = X_train['Electrical'].fillna(mode_electrical)\n",
    "X_test['Electrical'] = X_test['Electrical'].fillna(mode_electrical)\n",
    "test_df['Electrical'] = test_df['Electrical'].fillna(mode_electrical)\n",
    "\n",
    "# helpers.py manages imputation for missing data.\n",
    "\n",
    "X_train = helpers.init_fill_na(X_train)\n",
    "X_test = helpers.init_fill_na(X_test)\n",
    "test_df = helpers.init_fill_na(test_df)"
   ],
   "outputs": [],
   "execution_count": 89
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T07:29:36.734899Z",
     "start_time": "2025-05-06T07:29:36.674337Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Feature engineering:\n",
    "\n",
    "X_train[\"SqrtLotArea\"] = np.sqrt(X_train[\"LotArea\"])\n",
    "X_test[\"SqrtLotArea\"] = np.sqrt(X_test[\"LotArea\"])\n",
    "test_df[\"SqrtLotArea\"] = np.sqrt(test_df[\"LotArea\"])\n",
    "# Made redundant by SqrtLotArea.\n",
    "del train_df[\"LotArea\"]\n",
    "del test_df[\"LotArea\"]\n",
    "\n",
    "try:\n",
    "    # Only learning medians based on X_train as to not leak data to x_test.\n",
    "    indexed_scales_dict, global_val = helpers.learn_scaling_factors(X_train)\n",
    "    # Now pass indexed_scales_dict and global_val to your apply function\n",
    "    X_train[\"LotFrontage\"] = helpers.fill_na_lotfrontage(X_train, indexed_scales_dict, 13, global_val)\n",
    "    X_test[\"LotFrontage\"] = helpers.fill_na_lotfrontage(X_test, indexed_scales_dict, 13, global_val)\n",
    "    test_df[\"LotFrontage\"] = helpers.fill_na_lotfrontage(test_df, indexed_scales_dict, 13, global_val)\n",
    "except ValueError as e:\n",
    "    print(f\"Error learning rules: {e}\")\n",
    "\n",
    "X_train[\"ScaleFactor\"] = X_train[\"LotFrontage\"] / X_train[\"SqrtLotArea\"]\n",
    "X_test[\"ScaleFactor\"] = X_test[\"LotFrontage\"] / X_test[\"SqrtLotArea\"]\n",
    "test_df[\"ScaleFactor\"] = test_df[\"LotFrontage\"] / test_df[\"SqrtLotArea\"]"
   ],
   "id": "b17b63919a68236d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated Global Median Ratio: 0.7235 (from 951 samples)\n",
      "Calculating for group level: 3way (['MSZoning', 'BldgType', 'LotShape'])\n",
      " -> Found 39 groups for 3way\n",
      "Calculating for group level: 2way_ZS (['MSZoning', 'LotShape'])\n",
      " -> Found 16 groups for 2way_ZS\n",
      "Calculating for group level: 2way_ZB (['MSZoning', 'BldgType'])\n",
      " -> Found 19 groups for 2way_ZB\n",
      "Calculating for group level: 2way_BS (['BldgType', 'LotShape'])\n",
      " -> Found 14 groups for 2way_BS\n",
      "Calculating for group level: 1way_Z (['MSZoning'])\n",
      " -> Found 5 groups for 1way_Z\n",
      "Calculating for group level: 1way_B (['BldgType'])\n",
      " -> Found 5 groups for 1way_B\n",
      "Calculating for group level: 1way_S (['LotShape'])\n",
      " -> Found 4 groups for 1way_S\n",
      "Finished learning rules.\n"
     ]
    }
   ],
   "execution_count": 90
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T07:29:36.738431Z",
     "start_time": "2025-05-06T07:29:36.735397Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "num_cols = helpers.get_numeric_cols()\n",
    "num_cols = num_cols + [\"SqrtLotArea\", \"ScaleFactor\"]\n",
    "num_cols.remove(\"LotArea\")\n",
    "\n",
    "# Update the pipeline\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', helpers.generate_preprocessor(\n",
    "        drop='first',\n",
    "        sparse_output=True,\n",
    "        ordinal_cats_ordered=helpers.get_ordinal_cats_ordered(),\n",
    "        categorical_cols_ordinal=helpers.get_categorical_cols_ordinal(),\n",
    "        numerical_cols=num_cols,\n",
    "        categorical_cols_nominal=helpers.get_categorical_cols_nominal()\n",
    "    ))\n",
    "])"
   ],
   "id": "56c9bf0e55d18057",
   "outputs": [],
   "execution_count": 91
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-06T07:29:50.524277Z",
     "start_time": "2025-05-06T07:29:36.738934Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from scipy.sparse import issparse\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "# Deep learning model:\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "X_train = model_pipeline.fit_transform(X_train)\n",
    "X_test = model_pipeline.transform(X_test)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# float64 acceptable for EDA, float32 preferred for training.\n",
    "X_train = torch.tensor(X_train, device=device, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, device=device, dtype=torch.float32)\n",
    "Y_train = torch.tensor(Y_train.values, device=device, dtype=torch.float32)\n",
    "Y_test = torch.tensor(Y_test.values, device=device, dtype=torch.float32)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(X_train.shape[1], 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 16),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16, 1)\n",
    ")\n",
    "\n",
    "try:\n",
    "    train_dataset = TensorDataset(X_train, Y_train)\n",
    "    val_dataset = TensorDataset(X_test, Y_test)\n",
    "    print(f\"Train dataset length: {len(train_dataset)}\")\n",
    "    print(f\"Validation dataset length: {len(val_dataset)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating TensorDataset: {e}\")\n",
    "    # Likely length mismatch between X and y tensors if error here\n",
    "\n",
    "batch_size = 64 # Hyperparameter: How many samples per batch\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) # Shuffle training data\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False) # No need to shuffle validation\n",
    "\n",
    "loss_func = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 500 # Hyperparameter: How many times to iterate over the dataset\n",
    "\n",
    "print(\"\\nStarting Training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    # --- Training Phase ---\n",
    "    model.train() # Set model to training mode (enables dropout, batchnorm updates)\n",
    "    running_train_loss = 0.0\n",
    "    for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "        # Move batch data to the target device (GPU or CPU)\n",
    "        features, targets = features.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features)\n",
    "        # Reshape the targets tensor to match the outputs shape ([batch_size, 1])\n",
    "        targets_reshaped = targets.unsqueeze(1)\n",
    "        loss = loss_func(outputs, targets_reshaped)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_train_loss += loss.item() * features.size(0)\n",
    "\n",
    "    epoch_train_loss = running_train_loss / len(train_loader.dataset)\n",
    "\n",
    "    # --- Validation Phase ---\n",
    "    model.eval() # Set model to evaluation mode (disables dropout, batchnorm updates)\n",
    "    running_val_loss = 0.0\n",
    "    with torch.no_grad(): # No need to calculate gradients during validation\n",
    "        for features, targets in val_loader:\n",
    "            features, targets = features.to(device), targets.to(device)\n",
    "            outputs = model(features)\n",
    "            targets_reshaped = targets.unsqueeze(1)\n",
    "            loss = loss_func(outputs, targets_reshaped)\n",
    "            running_val_loss += loss.item() * features.size(0)\n",
    "\n",
    "    epoch_val_loss = running_val_loss / len(val_loader.dataset)\n",
    "\n",
    "    # Print progress (e.g., every epoch or every few epochs)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs} | Train Loss: {epoch_train_loss:.6f} | Val Loss: {epoch_val_loss:.6f}')"
   ],
   "id": "8a41a83688b07ce0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset length: 1168\n",
      "Validation dataset length: 292\n",
      "\n",
      "Starting Training...\n",
      "Epoch 1/500 | Train Loss: 38774767167.123291 | Val Loss: 39358063882.520546\n",
      "Epoch 2/500 | Train Loss: 38044874976.438354 | Val Loss: 37925816965.260277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jakep\\miniconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:246: UserWarning: Found unknown categories in columns [13, 23] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/500 | Train Loss: 34874688890.739723 | Val Loss: 32776287666.849316\n",
      "Epoch 4/500 | Train Loss: 26920410911.561646 | Val Loss: 21604932088.986301\n",
      "Epoch 5/500 | Train Loss: 19673199349.479450 | Val Loss: 14392968781.150684\n",
      "Epoch 6/500 | Train Loss: 18398497749.917809 | Val Loss: 13103486821.698629\n",
      "Epoch 7/500 | Train Loss: 17156744696.986301 | Val Loss: 15499385491.287672\n",
      "Epoch 8/500 | Train Loss: 17304412174.027397 | Val Loss: 14522042830.904110\n",
      "Epoch 9/500 | Train Loss: 16806715588.383562 | Val Loss: 14001244917.479452\n",
      "Epoch 10/500 | Train Loss: 16578250611.726027 | Val Loss: 13593247758.027397\n",
      "Epoch 11/500 | Train Loss: 16397799592.328768 | Val Loss: 13172866833.534246\n",
      "Epoch 12/500 | Train Loss: 16431502434.191780 | Val Loss: 12390083836.493151\n",
      "Epoch 13/500 | Train Loss: 16034704888.986301 | Val Loss: 12704850032.219177\n",
      "Epoch 14/500 | Train Loss: 15867553427.287672 | Val Loss: 12943505253.698629\n",
      "Epoch 15/500 | Train Loss: 15733159487.123287 | Val Loss: 12883215710.684931\n",
      "Epoch 16/500 | Train Loss: 15737210851.945206 | Val Loss: 12461183186.410959\n",
      "Epoch 17/500 | Train Loss: 15347900629.917809 | Val Loss: 12448369341.369864\n",
      "Epoch 18/500 | Train Loss: 15380497969.095890 | Val Loss: 12513892604.493151\n",
      "Epoch 19/500 | Train Loss: 15178991594.958904 | Val Loss: 12042019068.493151\n",
      "Epoch 20/500 | Train Loss: 14983477472.438356 | Val Loss: 12378857387.835617\n",
      "Epoch 21/500 | Train Loss: 14767941898.520548 | Val Loss: 11766636922.739725\n",
      "Epoch 22/500 | Train Loss: 14581632855.671232 | Val Loss: 11861030056.328768\n",
      "Epoch 23/500 | Train Loss: 14378246845.369864 | Val Loss: 11512256105.205479\n",
      "Epoch 24/500 | Train Loss: 14187272753.095890 | Val Loss: 11439640267.397261\n",
      "Epoch 25/500 | Train Loss: 14064911247.780823 | Val Loss: 11579789269.917809\n",
      "Epoch 26/500 | Train Loss: 13953640889.863014 | Val Loss: 10939874191.780823\n",
      "Epoch 27/500 | Train Loss: 13669967416.109589 | Val Loss: 11129843291.178082\n",
      "Epoch 28/500 | Train Loss: 13498498314.520548 | Val Loss: 11012328237.589041\n",
      "Epoch 29/500 | Train Loss: 13330873245.808220 | Val Loss: 10872623468.712328\n",
      "Epoch 30/500 | Train Loss: 13119093963.397261 | Val Loss: 11048586268.054794\n",
      "Epoch 31/500 | Train Loss: 12997205931.835617 | Val Loss: 10840512764.493151\n",
      "Epoch 32/500 | Train Loss: 12881308174.027397 | Val Loss: 10566283095.671232\n",
      "Epoch 33/500 | Train Loss: 12650560582.136986 | Val Loss: 10595697187.068493\n",
      "Epoch 34/500 | Train Loss: 12612440765.369864 | Val Loss: 11054737520.219177\n",
      "Epoch 35/500 | Train Loss: 12298590642.849316 | Val Loss: 10660102663.013699\n",
      "Epoch 36/500 | Train Loss: 12291079967.561644 | Val Loss: 10276383449.424658\n",
      "Epoch 37/500 | Train Loss: 11933517711.780823 | Val Loss: 10481190000.219177\n",
      "Epoch 38/500 | Train Loss: 11888616476.054794 | Val Loss: 10373370360.986301\n",
      "Epoch 39/500 | Train Loss: 11835679729.972603 | Val Loss: 9978923232.438356\n",
      "Epoch 40/500 | Train Loss: 11549886926.904110 | Val Loss: 10050416008.767124\n",
      "Epoch 41/500 | Train Loss: 11382820302.904110 | Val Loss: 10390620875.397261\n",
      "Epoch 42/500 | Train Loss: 11292685466.301371 | Val Loss: 10327608530.410959\n",
      "Epoch 43/500 | Train Loss: 11077451719.890411 | Val Loss: 9941829337.424658\n",
      "Epoch 44/500 | Train Loss: 11002768398.027397 | Val Loss: 10028790671.780823\n",
      "Epoch 45/500 | Train Loss: 10843266405.698629 | Val Loss: 9952493778.410959\n",
      "Epoch 46/500 | Train Loss: 10829831438.027397 | Val Loss: 9893594294.356165\n",
      "Epoch 47/500 | Train Loss: 10659539533.150684 | Val Loss: 10030657297.534246\n",
      "Epoch 48/500 | Train Loss: 10578631820.273973 | Val Loss: 10051642536.328768\n",
      "Epoch 49/500 | Train Loss: 10517535316.164383 | Val Loss: 9886499503.342466\n",
      "Epoch 50/500 | Train Loss: 10423679915.835617 | Val Loss: 10129026202.301371\n",
      "Epoch 51/500 | Train Loss: 10315516086.356165 | Val Loss: 10077365430.356165\n",
      "Epoch 52/500 | Train Loss: 10317439873.753426 | Val Loss: 9974313170.410959\n",
      "Epoch 53/500 | Train Loss: 10235194606.465754 | Val Loss: 9880229088.438356\n",
      "Epoch 54/500 | Train Loss: 10223455772.054794 | Val Loss: 10066424691.726027\n",
      "Epoch 55/500 | Train Loss: 10088451822.465754 | Val Loss: 10133235263.123287\n",
      "Epoch 56/500 | Train Loss: 10063388216.109589 | Val Loss: 10082689655.232876\n",
      "Epoch 57/500 | Train Loss: 10151938665.205479 | Val Loss: 10251328624.219177\n",
      "Epoch 58/500 | Train Loss: 9946216097.315069 | Val Loss: 10036657236.164383\n",
      "Epoch 59/500 | Train Loss: 9979303900.931507 | Val Loss: 10033011683.945206\n",
      "Epoch 60/500 | Train Loss: 9988712013.150684 | Val Loss: 10086759578.301371\n",
      "Epoch 61/500 | Train Loss: 9958212229.260275 | Val Loss: 10164046104.547945\n",
      "Epoch 62/500 | Train Loss: 9895972674.630136 | Val Loss: 10029189498.739725\n",
      "Epoch 63/500 | Train Loss: 9880897451.835617 | Val Loss: 10051734093.150684\n",
      "Epoch 64/500 | Train Loss: 9897751818.520548 | Val Loss: 10222615650.191780\n",
      "Epoch 65/500 | Train Loss: 9824022864.657534 | Val Loss: 10099552606.684931\n",
      "Epoch 66/500 | Train Loss: 9861828818.410959 | Val Loss: 10029868957.808220\n",
      "Epoch 67/500 | Train Loss: 9867179022.027397 | Val Loss: 10514703794.849316\n",
      "Epoch 68/500 | Train Loss: 9840441007.342466 | Val Loss: 10477570188.273973\n",
      "Epoch 69/500 | Train Loss: 9767410744.109589 | Val Loss: 10271926426.301371\n",
      "Epoch 70/500 | Train Loss: 9737632305.095890 | Val Loss: 10108732640.438356\n",
      "Epoch 71/500 | Train Loss: 9714179156.164383 | Val Loss: 10138647467.835617\n",
      "Epoch 72/500 | Train Loss: 9705949184.000000 | Val Loss: 10124849783.232876\n",
      "Epoch 73/500 | Train Loss: 9687284371.287672 | Val Loss: 10093675744.438356\n",
      "Epoch 74/500 | Train Loss: 9659198583.232876 | Val Loss: 10090712989.808220\n",
      "Epoch 75/500 | Train Loss: 9677216150.794521 | Val Loss: 10158514105.863014\n",
      "Epoch 76/500 | Train Loss: 9684448073.643835 | Val Loss: 9951278388.602739\n",
      "Epoch 77/500 | Train Loss: 9603360291.068493 | Val Loss: 10105710409.643835\n",
      "Epoch 78/500 | Train Loss: 9617262970.739725 | Val Loss: 10102839955.287672\n",
      "Epoch 79/500 | Train Loss: 9642732473.863014 | Val Loss: 10071460765.808220\n",
      "Epoch 80/500 | Train Loss: 9604510306.191780 | Val Loss: 9846723359.561644\n",
      "Epoch 81/500 | Train Loss: 9576419328.000000 | Val Loss: 10001524231.013699\n",
      "Epoch 82/500 | Train Loss: 9519362665.205479 | Val Loss: 9943053438.246574\n",
      "Epoch 83/500 | Train Loss: 9564295644.931507 | Val Loss: 9909910177.315069\n",
      "Epoch 84/500 | Train Loss: 9496646599.890411 | Val Loss: 9994870882.191780\n",
      "Epoch 85/500 | Train Loss: 9491613387.397261 | Val Loss: 10045276117.917809\n",
      "Epoch 86/500 | Train Loss: 9515102755.068493 | Val Loss: 9924125289.205479\n",
      "Epoch 87/500 | Train Loss: 9443601513.205479 | Val Loss: 9851665941.041096\n",
      "Epoch 88/500 | Train Loss: 9436580008.328768 | Val Loss: 9832983243.397261\n",
      "Epoch 89/500 | Train Loss: 9433791971.945206 | Val Loss: 9948917507.506849\n",
      "Epoch 90/500 | Train Loss: 9374529357.150684 | Val Loss: 9808560801.315069\n",
      "Epoch 91/500 | Train Loss: 9351604041.643835 | Val Loss: 9738326366.684931\n",
      "Epoch 92/500 | Train Loss: 9329419144.767124 | Val Loss: 9806373523.287672\n",
      "Epoch 93/500 | Train Loss: 9342945378.191780 | Val Loss: 9712776234.082191\n",
      "Epoch 94/500 | Train Loss: 9307278756.821918 | Val Loss: 9829045374.246574\n",
      "Epoch 95/500 | Train Loss: 9352493182.246574 | Val Loss: 9708252145.972603\n",
      "Epoch 96/500 | Train Loss: 9211856068.383562 | Val Loss: 9767287878.136986\n",
      "Epoch 97/500 | Train Loss: 9304305369.424658 | Val Loss: 9604266432.876713\n",
      "Epoch 98/500 | Train Loss: 9241217977.863014 | Val Loss: 9769458856.328768\n",
      "Epoch 99/500 | Train Loss: 9160901554.849316 | Val Loss: 9645204816.657534\n",
      "Epoch 100/500 | Train Loss: 9163500621.150684 | Val Loss: 9604898437.260275\n",
      "Epoch 101/500 | Train Loss: 9104591493.260275 | Val Loss: 9525848134.136986\n",
      "Epoch 102/500 | Train Loss: 9095221752.986301 | Val Loss: 9579972481.753426\n",
      "Epoch 103/500 | Train Loss: 9041890219.835617 | Val Loss: 9455788508.931507\n",
      "Epoch 104/500 | Train Loss: 9027183756.273973 | Val Loss: 9396201135.342466\n",
      "Epoch 105/500 | Train Loss: 9036960838.136986 | Val Loss: 9340118899.726027\n",
      "Epoch 106/500 | Train Loss: 9007816016.657534 | Val Loss: 9406378559.123287\n",
      "Epoch 107/500 | Train Loss: 9028926025.643835 | Val Loss: 9620145502.684931\n",
      "Epoch 108/500 | Train Loss: 8913594283.835617 | Val Loss: 9333120056.109589\n",
      "Epoch 109/500 | Train Loss: 8981914848.438356 | Val Loss: 9397352391.890411\n",
      "Epoch 110/500 | Train Loss: 8863124059.178082 | Val Loss: 9347887791.342466\n",
      "Epoch 111/500 | Train Loss: 8793118327.232876 | Val Loss: 9239406634.082191\n",
      "Epoch 112/500 | Train Loss: 8807051207.890411 | Val Loss: 9199661070.027397\n",
      "Epoch 113/500 | Train Loss: 8818621601.315069 | Val Loss: 9293353984.000000\n",
      "Epoch 114/500 | Train Loss: 8926641053.808220 | Val Loss: 9077058616.109589\n",
      "Epoch 115/500 | Train Loss: 8696576925.808220 | Val Loss: 9389908879.780823\n",
      "Epoch 116/500 | Train Loss: 8702884120.547945 | Val Loss: 9105729514.958904\n",
      "Epoch 117/500 | Train Loss: 8761860502.794521 | Val Loss: 9467705975.232876\n",
      "Epoch 118/500 | Train Loss: 8634061066.520548 | Val Loss: 8941125414.575342\n",
      "Epoch 119/500 | Train Loss: 8505086576.219178 | Val Loss: 9071496612.821918\n",
      "Epoch 120/500 | Train Loss: 8480921263.342465 | Val Loss: 9040151986.849316\n",
      "Epoch 121/500 | Train Loss: 8592015331.945206 | Val Loss: 9027012383.561644\n",
      "Epoch 122/500 | Train Loss: 8568401800.767123 | Val Loss: 9397682596.821918\n",
      "Epoch 123/500 | Train Loss: 8347630072.986301 | Val Loss: 8976605653.917809\n",
      "Epoch 124/500 | Train Loss: 8370905929.643836 | Val Loss: 8864670600.767124\n",
      "Epoch 125/500 | Train Loss: 8343184152.547945 | Val Loss: 8711327126.794521\n",
      "Epoch 126/500 | Train Loss: 8190894697.205480 | Val Loss: 8955468379.178082\n",
      "Epoch 127/500 | Train Loss: 8256192385.753425 | Val Loss: 8869524332.712328\n",
      "Epoch 128/500 | Train Loss: 8269755630.465754 | Val Loss: 8610220684.273973\n",
      "Epoch 129/500 | Train Loss: 8037950786.630137 | Val Loss: 8672791972.821918\n",
      "Epoch 130/500 | Train Loss: 7979441565.808219 | Val Loss: 8504475339.397261\n",
      "Epoch 131/500 | Train Loss: 8033382989.150685 | Val Loss: 8560295711.561644\n",
      "Epoch 132/500 | Train Loss: 7935337787.616438 | Val Loss: 8258591063.671233\n",
      "Epoch 133/500 | Train Loss: 7863370443.397261 | Val Loss: 8473100624.657535\n",
      "Epoch 134/500 | Train Loss: 7762518352.657535 | Val Loss: 8288712044.712329\n",
      "Epoch 135/500 | Train Loss: 7622200249.863013 | Val Loss: 8133658273.315068\n",
      "Epoch 136/500 | Train Loss: 7641081042.410959 | Val Loss: 8172244031.123287\n",
      "Epoch 137/500 | Train Loss: 7507296452.383562 | Val Loss: 7890451147.397261\n",
      "Epoch 138/500 | Train Loss: 7532542183.452055 | Val Loss: 8258376668.931507\n",
      "Epoch 139/500 | Train Loss: 7302564008.328767 | Val Loss: 7826636631.671233\n",
      "Epoch 140/500 | Train Loss: 7272713664.876713 | Val Loss: 7993736675.945206\n",
      "Epoch 141/500 | Train Loss: 7199428257.315068 | Val Loss: 7650273455.342465\n",
      "Epoch 142/500 | Train Loss: 7125041600.876713 | Val Loss: 7572572363.397261\n",
      "Epoch 143/500 | Train Loss: 6986041344.000000 | Val Loss: 7531008483.945206\n",
      "Epoch 144/500 | Train Loss: 6846374673.534246 | Val Loss: 7342479233.753425\n",
      "Epoch 145/500 | Train Loss: 6711212789.479452 | Val Loss: 7432636072.328767\n",
      "Epoch 146/500 | Train Loss: 6600569603.506849 | Val Loss: 7158577888.438356\n",
      "Epoch 147/500 | Train Loss: 6443594464.438356 | Val Loss: 6950048957.369863\n",
      "Epoch 148/500 | Train Loss: 6256632607.561644 | Val Loss: 6789466055.890411\n",
      "Epoch 149/500 | Train Loss: 6186619672.547945 | Val Loss: 6700941578.520548\n",
      "Epoch 150/500 | Train Loss: 6016247892.164384 | Val Loss: 6674882005.917809\n",
      "Epoch 151/500 | Train Loss: 5968264184.986301 | Val Loss: 6409982127.342465\n",
      "Epoch 152/500 | Train Loss: 5763224034.191781 | Val Loss: 6281462559.561644\n",
      "Epoch 153/500 | Train Loss: 5576059791.780822 | Val Loss: 6244701653.917809\n",
      "Epoch 154/500 | Train Loss: 5361949226.082191 | Val Loss: 5877666402.191781\n",
      "Epoch 155/500 | Train Loss: 5274687319.671233 | Val Loss: 6075644878.904110\n",
      "Epoch 156/500 | Train Loss: 5927214749.808219 | Val Loss: 5730444596.602739\n",
      "Epoch 157/500 | Train Loss: 4972139835.616438 | Val Loss: 5389152936.328767\n",
      "Epoch 158/500 | Train Loss: 4662946281.205480 | Val Loss: 5079597427.726027\n",
      "Epoch 159/500 | Train Loss: 4605816158.684932 | Val Loss: 5229301374.246575\n",
      "Epoch 160/500 | Train Loss: 4280051003.616438 | Val Loss: 4868737900.712329\n",
      "Epoch 161/500 | Train Loss: 4036842176.876712 | Val Loss: 4518191104.000000\n",
      "Epoch 162/500 | Train Loss: 3976680658.410959 | Val Loss: 4276779081.643836\n",
      "Epoch 163/500 | Train Loss: 3872445040.219178 | Val Loss: 4821194752.000000\n",
      "Epoch 164/500 | Train Loss: 3732912924.054794 | Val Loss: 3949343382.794520\n",
      "Epoch 165/500 | Train Loss: 3358456211.287671 | Val Loss: 3702275475.287671\n",
      "Epoch 166/500 | Train Loss: 3227176411.178082 | Val Loss: 3760660718.465754\n",
      "Epoch 167/500 | Train Loss: 2882986737.972603 | Val Loss: 3621493258.520548\n",
      "Epoch 168/500 | Train Loss: 2744158067.726027 | Val Loss: 3228207801.863014\n",
      "Epoch 169/500 | Train Loss: 2653268106.520548 | Val Loss: 3067235710.246575\n",
      "Epoch 170/500 | Train Loss: 2452673220.383562 | Val Loss: 3095532821.041096\n",
      "Epoch 171/500 | Train Loss: 2342355694.465754 | Val Loss: 3027850804.602740\n",
      "Epoch 172/500 | Train Loss: 2277698830.904109 | Val Loss: 3052739173.698630\n",
      "Epoch 173/500 | Train Loss: 2117342691.945205 | Val Loss: 2637723925.041096\n",
      "Epoch 174/500 | Train Loss: 1992370075.178082 | Val Loss: 2472471222.356164\n",
      "Epoch 175/500 | Train Loss: 1888103667.726027 | Val Loss: 2427035770.739726\n",
      "Epoch 176/500 | Train Loss: 1835887440.657534 | Val Loss: 2305379594.520548\n",
      "Epoch 177/500 | Train Loss: 1922865277.369863 | Val Loss: 2420053686.356164\n",
      "Epoch 178/500 | Train Loss: 1855930080.438356 | Val Loss: 2595993834.958904\n",
      "Epoch 179/500 | Train Loss: 1796336240.219178 | Val Loss: 2384629165.589041\n",
      "Epoch 180/500 | Train Loss: 1721946664.328767 | Val Loss: 2217870772.602740\n",
      "Epoch 181/500 | Train Loss: 1726202787.068493 | Val Loss: 2264229112.986301\n",
      "Epoch 182/500 | Train Loss: 1703389359.342466 | Val Loss: 2159251996.054794\n",
      "Epoch 183/500 | Train Loss: 1638891172.821918 | Val Loss: 2140807367.890411\n",
      "Epoch 184/500 | Train Loss: 1708212834.191781 | Val Loss: 2243495657.205480\n",
      "Epoch 185/500 | Train Loss: 1609510899.726027 | Val Loss: 2064009556.164384\n",
      "Epoch 186/500 | Train Loss: 1608884018.849315 | Val Loss: 2036709267.287671\n",
      "Epoch 187/500 | Train Loss: 1784824348.054795 | Val Loss: 2220641218.630137\n",
      "Epoch 188/500 | Train Loss: 1708986360.986301 | Val Loss: 2050761561.424658\n",
      "Epoch 189/500 | Train Loss: 1620589869.589041 | Val Loss: 2002619597.150685\n",
      "Epoch 190/500 | Train Loss: 1677839447.671233 | Val Loss: 2011652483.506849\n",
      "Epoch 191/500 | Train Loss: 1573334675.287671 | Val Loss: 2255206400.000000\n",
      "Epoch 192/500 | Train Loss: 1571033834.958904 | Val Loss: 1959808096.438356\n",
      "Epoch 193/500 | Train Loss: 1571904981.917808 | Val Loss: 2013225778.849315\n",
      "Epoch 194/500 | Train Loss: 1587179041.315068 | Val Loss: 1997502632.328767\n",
      "Epoch 195/500 | Train Loss: 1592189522.410959 | Val Loss: 1932234503.013699\n",
      "Epoch 196/500 | Train Loss: 1578634955.397260 | Val Loss: 1981019584.876712\n",
      "Epoch 197/500 | Train Loss: 1554913322.082192 | Val Loss: 1932901202.410959\n",
      "Epoch 198/500 | Train Loss: 1545236309.917808 | Val Loss: 1947042014.684932\n",
      "Epoch 199/500 | Train Loss: 1545446882.630137 | Val Loss: 2051183165.369863\n",
      "Epoch 200/500 | Train Loss: 1545992627.726027 | Val Loss: 1951765276.054795\n",
      "Epoch 201/500 | Train Loss: 1534179139.068493 | Val Loss: 1918098449.534247\n",
      "Epoch 202/500 | Train Loss: 1524623690.520548 | Val Loss: 2102260830.684932\n",
      "Epoch 203/500 | Train Loss: 1583776549.698630 | Val Loss: 2184701443.506849\n",
      "Epoch 204/500 | Train Loss: 1574247651.945205 | Val Loss: 1877752684.712329\n",
      "Epoch 205/500 | Train Loss: 1654733271.671233 | Val Loss: 1947688856.547945\n",
      "Epoch 206/500 | Train Loss: 1552683313.095891 | Val Loss: 1862136067.506849\n",
      "Epoch 207/500 | Train Loss: 1588453778.410959 | Val Loss: 1875228277.479452\n",
      "Epoch 208/500 | Train Loss: 1543834020.821918 | Val Loss: 1851988786.849315\n",
      "Epoch 209/500 | Train Loss: 1573709681.095891 | Val Loss: 1972653161.205479\n",
      "Epoch 210/500 | Train Loss: 1507013105.972603 | Val Loss: 1860831265.315068\n",
      "Epoch 211/500 | Train Loss: 1538465888.438356 | Val Loss: 1921331068.493151\n",
      "Epoch 212/500 | Train Loss: 1581565401.424658 | Val Loss: 1947276815.780822\n",
      "Epoch 213/500 | Train Loss: 1493521912.986301 | Val Loss: 1887651876.821918\n",
      "Epoch 214/500 | Train Loss: 1482077952.000000 | Val Loss: 1806893141.917808\n",
      "Epoch 215/500 | Train Loss: 1458309319.890411 | Val Loss: 1964147473.534247\n",
      "Epoch 216/500 | Train Loss: 1455401142.356164 | Val Loss: 1791174275.506849\n",
      "Epoch 217/500 | Train Loss: 1489748381.808219 | Val Loss: 1826444691.287671\n",
      "Epoch 218/500 | Train Loss: 1500021574.136986 | Val Loss: 1780462746.301370\n",
      "Epoch 219/500 | Train Loss: 1500028885.917808 | Val Loss: 1826406889.205479\n",
      "Epoch 220/500 | Train Loss: 1528404451.945205 | Val Loss: 1825276635.178082\n",
      "Epoch 221/500 | Train Loss: 1509906140.931507 | Val Loss: 1761546397.808219\n",
      "Epoch 222/500 | Train Loss: 1770912583.890411 | Val Loss: 1816330448.657534\n",
      "Epoch 223/500 | Train Loss: 1566749966.027397 | Val Loss: 1816000005.260274\n",
      "Epoch 224/500 | Train Loss: 1505348363.397260 | Val Loss: 1795694655.123288\n",
      "Epoch 225/500 | Train Loss: 1478308208.219178 | Val Loss: 1748090581.917808\n",
      "Epoch 226/500 | Train Loss: 1483451977.643836 | Val Loss: 1958444442.301370\n",
      "Epoch 227/500 | Train Loss: 1451037213.369863 | Val Loss: 1839595628.712329\n",
      "Epoch 228/500 | Train Loss: 1482093082.301370 | Val Loss: 1780834661.698630\n",
      "Epoch 229/500 | Train Loss: 1425804428.273973 | Val Loss: 1770503800.986301\n",
      "Epoch 230/500 | Train Loss: 1411845491.726027 | Val Loss: 1931980687.780822\n",
      "Epoch 231/500 | Train Loss: 1530422307.068493 | Val Loss: 1761611839.123288\n",
      "Epoch 232/500 | Train Loss: 1461270037.041096 | Val Loss: 1735711633.534247\n",
      "Epoch 233/500 | Train Loss: 1466924636.493151 | Val Loss: 1760888497.095891\n",
      "Epoch 234/500 | Train Loss: 1483154342.575342 | Val Loss: 1763896558.465753\n",
      "Epoch 235/500 | Train Loss: 1521761595.616438 | Val Loss: 1814693356.712329\n",
      "Epoch 236/500 | Train Loss: 1585772505.424658 | Val Loss: 1724524636.931507\n",
      "Epoch 237/500 | Train Loss: 1753865745.534247 | Val Loss: 2683966102.794520\n",
      "Epoch 238/500 | Train Loss: 1695791009.315068 | Val Loss: 1876255458.191781\n",
      "Epoch 239/500 | Train Loss: 1448500935.890411 | Val Loss: 1766290211.068493\n",
      "Epoch 240/500 | Train Loss: 1418670995.287671 | Val Loss: 1697324328.328767\n",
      "Epoch 241/500 | Train Loss: 1399816465.534247 | Val Loss: 1807898478.465753\n",
      "Epoch 242/500 | Train Loss: 1400579871.561644 | Val Loss: 1857439894.794521\n",
      "Epoch 243/500 | Train Loss: 1388713401.863014 | Val Loss: 1720777464.986301\n",
      "Epoch 244/500 | Train Loss: 1386014327.232877 | Val Loss: 1848031111.013699\n",
      "Epoch 245/500 | Train Loss: 1443591211.835616 | Val Loss: 1773158170.301370\n",
      "Epoch 246/500 | Train Loss: 1361531504.219178 | Val Loss: 2108884183.671233\n",
      "Epoch 247/500 | Train Loss: 1348108445.808219 | Val Loss: 1677663623.013699\n",
      "Epoch 248/500 | Train Loss: 1520329639.452055 | Val Loss: 1900927754.520548\n",
      "Epoch 249/500 | Train Loss: 1424252238.904109 | Val Loss: 1738291713.753425\n",
      "Epoch 250/500 | Train Loss: 1366794289.095891 | Val Loss: 1723256151.671233\n",
      "Epoch 251/500 | Train Loss: 1356763873.315068 | Val Loss: 1675097375.561644\n",
      "Epoch 252/500 | Train Loss: 1385552582.136986 | Val Loss: 1697525386.520548\n",
      "Epoch 253/500 | Train Loss: 1426940565.917808 | Val Loss: 1661528030.684932\n",
      "Epoch 254/500 | Train Loss: 1345538034.410959 | Val Loss: 1727059406.904109\n",
      "Epoch 255/500 | Train Loss: 1428344250.739726 | Val Loss: 1665787425.315068\n",
      "Epoch 256/500 | Train Loss: 1361315573.479452 | Val Loss: 1652832461.150685\n",
      "Epoch 257/500 | Train Loss: 1394077897.643836 | Val Loss: 1711226643.287671\n",
      "Epoch 258/500 | Train Loss: 1511361532.493151 | Val Loss: 1885450027.835616\n",
      "Epoch 259/500 | Train Loss: 1463204861.369863 | Val Loss: 1659820131.945205\n",
      "Epoch 260/500 | Train Loss: 1388863033.863014 | Val Loss: 1860088125.369863\n",
      "Epoch 261/500 | Train Loss: 1475110528.000000 | Val Loss: 1855793523.726027\n",
      "Epoch 262/500 | Train Loss: 1361871419.616438 | Val Loss: 2207087484.493151\n",
      "Epoch 263/500 | Train Loss: 1374335736.109589 | Val Loss: 1613070350.027397\n",
      "Epoch 264/500 | Train Loss: 1351616720.657534 | Val Loss: 1613452559.780822\n",
      "Epoch 265/500 | Train Loss: 1359040335.780822 | Val Loss: 1618483545.424658\n",
      "Epoch 266/500 | Train Loss: 1403360739.945205 | Val Loss: 1639137111.671233\n",
      "Epoch 267/500 | Train Loss: 1330621038.465753 | Val Loss: 1612507498.958904\n",
      "Epoch 268/500 | Train Loss: 1358833724.493151 | Val Loss: 1673317237.479452\n",
      "Epoch 269/500 | Train Loss: 1315348046.904109 | Val Loss: 1716571504.219178\n",
      "Epoch 270/500 | Train Loss: 1309409912.109589 | Val Loss: 1593433759.561644\n",
      "Epoch 271/500 | Train Loss: 1400150638.465753 | Val Loss: 1579206301.808219\n",
      "Epoch 272/500 | Train Loss: 1413611778.630137 | Val Loss: 1614666581.917808\n",
      "Epoch 273/500 | Train Loss: 1364678660.821918 | Val Loss: 1596026787.068493\n",
      "Epoch 274/500 | Train Loss: 1303758727.013699 | Val Loss: 1605043261.369863\n",
      "Epoch 275/500 | Train Loss: 1327326094.904109 | Val Loss: 1578457037.150685\n",
      "Epoch 276/500 | Train Loss: 1417483706.739726 | Val Loss: 1603554823.013699\n",
      "Epoch 277/500 | Train Loss: 1311182253.589041 | Val Loss: 1599589034.082192\n",
      "Epoch 278/500 | Train Loss: 1360161623.671233 | Val Loss: 1560759159.232877\n",
      "Epoch 279/500 | Train Loss: 1456975089.972603 | Val Loss: 1625665900.712329\n",
      "Epoch 280/500 | Train Loss: 1381414294.794521 | Val Loss: 1583061800.328767\n",
      "Epoch 281/500 | Train Loss: 1458564666.739726 | Val Loss: 1575702873.424658\n",
      "Epoch 282/500 | Train Loss: 1441823606.356164 | Val Loss: 1603938826.520548\n",
      "Epoch 283/500 | Train Loss: 1484890641.534247 | Val Loss: 1593685900.273973\n",
      "Epoch 284/500 | Train Loss: 1449835313.095891 | Val Loss: 1583720905.643836\n",
      "Epoch 285/500 | Train Loss: 1359513444.821918 | Val Loss: 1646051548.931507\n",
      "Epoch 286/500 | Train Loss: 1309024704.876712 | Val Loss: 1723256225.315068\n",
      "Epoch 287/500 | Train Loss: 1311495327.561644 | Val Loss: 1951990305.315068\n",
      "Epoch 288/500 | Train Loss: 1388468403.726027 | Val Loss: 1733874274.191781\n",
      "Epoch 289/500 | Train Loss: 1347129378.630137 | Val Loss: 1995176658.410959\n",
      "Epoch 290/500 | Train Loss: 1448979989.041096 | Val Loss: 1801204038.136986\n",
      "Epoch 291/500 | Train Loss: 1280571496.328767 | Val Loss: 1611081999.780822\n",
      "Epoch 292/500 | Train Loss: 1347100821.041096 | Val Loss: 1596609932.273973\n",
      "Epoch 293/500 | Train Loss: 1386869018.301370 | Val Loss: 1532497604.383562\n",
      "Epoch 294/500 | Train Loss: 1455000879.342466 | Val Loss: 1530656829.369863\n",
      "Epoch 295/500 | Train Loss: 1300412451.945205 | Val Loss: 1595488548.821918\n",
      "Epoch 296/500 | Train Loss: 1264728089.863014 | Val Loss: 1696044494.904109\n",
      "Epoch 297/500 | Train Loss: 1319001568.438356 | Val Loss: 1621527055.780822\n",
      "Epoch 298/500 | Train Loss: 1276822498.191781 | Val Loss: 1512890867.726027\n",
      "Epoch 299/500 | Train Loss: 1319026162.849315 | Val Loss: 1543153157.260274\n",
      "Epoch 300/500 | Train Loss: 1327573732.821918 | Val Loss: 1512459225.424658\n",
      "Epoch 301/500 | Train Loss: 1309502961.972603 | Val Loss: 1513693124.383562\n",
      "Epoch 302/500 | Train Loss: 1275649054.246575 | Val Loss: 1677689749.041096\n",
      "Epoch 303/500 | Train Loss: 1249237476.821918 | Val Loss: 1562014080.000000\n",
      "Epoch 304/500 | Train Loss: 1269863877.260274 | Val Loss: 1606816292.821918\n",
      "Epoch 305/500 | Train Loss: 1300665744.219178 | Val Loss: 1510044819.287671\n",
      "Epoch 306/500 | Train Loss: 1292830265.863014 | Val Loss: 1524409663.123288\n",
      "Epoch 307/500 | Train Loss: 1356922155.835616 | Val Loss: 1498624210.410959\n",
      "Epoch 308/500 | Train Loss: 1249090004.164384 | Val Loss: 1651820188.054795\n",
      "Epoch 309/500 | Train Loss: 1242419622.575342 | Val Loss: 1561161712.219178\n",
      "Epoch 310/500 | Train Loss: 1316600975.780822 | Val Loss: 1494434305.753425\n",
      "Epoch 311/500 | Train Loss: 1282246270.246575 | Val Loss: 1565007473.972603\n",
      "Epoch 312/500 | Train Loss: 1329389624.109589 | Val Loss: 1610413078.794521\n",
      "Epoch 313/500 | Train Loss: 1277857181.808219 | Val Loss: 2172777840.219178\n",
      "Epoch 314/500 | Train Loss: 1509793993.643836 | Val Loss: 1999598279.890411\n",
      "Epoch 315/500 | Train Loss: 1315685123.068493 | Val Loss: 1865949557.479452\n",
      "Epoch 316/500 | Train Loss: 1309715280.657534 | Val Loss: 1628781166.465753\n",
      "Epoch 317/500 | Train Loss: 1259703082.520548 | Val Loss: 1809548815.780822\n",
      "Epoch 318/500 | Train Loss: 1355421685.479452 | Val Loss: 1677410879.123288\n",
      "Epoch 319/500 | Train Loss: 1291889217.315068 | Val Loss: 1512557960.767123\n",
      "Epoch 320/500 | Train Loss: 1247231903.561644 | Val Loss: 1542275301.698630\n",
      "Epoch 321/500 | Train Loss: 1212815827.287671 | Val Loss: 1567279184.657534\n",
      "Epoch 322/500 | Train Loss: 1241326941.808219 | Val Loss: 1505936555.835616\n",
      "Epoch 323/500 | Train Loss: 1233050283.835616 | Val Loss: 1793903705.424658\n",
      "Epoch 324/500 | Train Loss: 1293506613.041096 | Val Loss: 1954328291.945205\n",
      "Epoch 325/500 | Train Loss: 1363565392.657534 | Val Loss: 1844406580.602740\n",
      "Epoch 326/500 | Train Loss: 1242813964.273973 | Val Loss: 2026810404.821918\n",
      "Epoch 327/500 | Train Loss: 1291564486.136986 | Val Loss: 1522739052.712329\n",
      "Epoch 328/500 | Train Loss: 1236671817.643836 | Val Loss: 1449598870.794521\n",
      "Epoch 329/500 | Train Loss: 1212913516.493151 | Val Loss: 1436802044.493151\n",
      "Epoch 330/500 | Train Loss: 1223696328.767123 | Val Loss: 1448889931.397260\n",
      "Epoch 331/500 | Train Loss: 1188562540.712329 | Val Loss: 1550414727.013699\n",
      "Epoch 332/500 | Train Loss: 1193572557.150685 | Val Loss: 1458690388.164384\n",
      "Epoch 333/500 | Train Loss: 1182470414.904109 | Val Loss: 1472464494.465753\n",
      "Epoch 334/500 | Train Loss: 1233319383.671233 | Val Loss: 1430155884.712329\n",
      "Epoch 335/500 | Train Loss: 1184598228.383562 | Val Loss: 1577847175.013699\n",
      "Epoch 336/500 | Train Loss: 1222414911.561644 | Val Loss: 1812422701.589041\n",
      "Epoch 337/500 | Train Loss: 1241078200.986301 | Val Loss: 1461337875.287671\n",
      "Epoch 338/500 | Train Loss: 1236748773.698630 | Val Loss: 1581936764.493151\n",
      "Epoch 339/500 | Train Loss: 1169216547.945205 | Val Loss: 1415832670.684932\n",
      "Epoch 340/500 | Train Loss: 1185878844.493151 | Val Loss: 1555845584.657534\n",
      "Epoch 341/500 | Train Loss: 1187502137.863014 | Val Loss: 1618613028.821918\n",
      "Epoch 342/500 | Train Loss: 1180716237.150685 | Val Loss: 1442890368.000000\n",
      "Epoch 343/500 | Train Loss: 1174339404.273973 | Val Loss: 1407128816.219178\n",
      "Epoch 344/500 | Train Loss: 1184939602.410959 | Val Loss: 1777429330.410959\n",
      "Epoch 345/500 | Train Loss: 1544003852.712329 | Val Loss: 1732426134.794521\n",
      "Epoch 346/500 | Train Loss: 1255362317.150685 | Val Loss: 1624682452.164384\n",
      "Epoch 347/500 | Train Loss: 1265195283.287671 | Val Loss: 1906963775.123288\n",
      "Epoch 348/500 | Train Loss: 1386467378.849315 | Val Loss: 1459097349.260274\n",
      "Epoch 349/500 | Train Loss: 1195158947.068493 | Val Loss: 1623274927.342466\n",
      "Epoch 350/500 | Train Loss: 1218755672.547945 | Val Loss: 1601267201.753425\n",
      "Epoch 351/500 | Train Loss: 1174675362.191781 | Val Loss: 1407931106.191781\n",
      "Epoch 352/500 | Train Loss: 1235615816.767123 | Val Loss: 1413369031.890411\n",
      "Epoch 353/500 | Train Loss: 1236870433.315068 | Val Loss: 1406825000.328767\n",
      "Epoch 354/500 | Train Loss: 1176217843.726027 | Val Loss: 1622805425.095891\n",
      "Epoch 355/500 | Train Loss: 1165366349.150685 | Val Loss: 1586593451.835616\n",
      "Epoch 356/500 | Train Loss: 1199498868.164384 | Val Loss: 2061906647.671233\n",
      "Epoch 357/500 | Train Loss: 1454313234.410959 | Val Loss: 1707669381.260274\n",
      "Epoch 358/500 | Train Loss: 1237730107.616438 | Val Loss: 1840137119.561644\n",
      "Epoch 359/500 | Train Loss: 1231693036.712329 | Val Loss: 1411677182.246575\n",
      "Epoch 360/500 | Train Loss: 1172227701.479452 | Val Loss: 1387847269.698630\n",
      "Epoch 361/500 | Train Loss: 1174744327.013699 | Val Loss: 1389654524.493151\n",
      "Epoch 362/500 | Train Loss: 1164493475.068493 | Val Loss: 1397545191.452055\n",
      "Epoch 363/500 | Train Loss: 1167000860.931507 | Val Loss: 1374943621.260274\n",
      "Epoch 364/500 | Train Loss: 1167808120.986301 | Val Loss: 1453437596.054795\n",
      "Epoch 365/500 | Train Loss: 1155383722.082192 | Val Loss: 1441033519.342466\n",
      "Epoch 366/500 | Train Loss: 1153853038.465753 | Val Loss: 1421828646.575342\n",
      "Epoch 367/500 | Train Loss: 1169714220.931507 | Val Loss: 1454054822.575342\n",
      "Epoch 368/500 | Train Loss: 1157484267.835616 | Val Loss: 1579262700.712329\n",
      "Epoch 369/500 | Train Loss: 1141880297.205479 | Val Loss: 1527900540.493151\n",
      "Epoch 370/500 | Train Loss: 1184600270.904109 | Val Loss: 1520336198.136986\n",
      "Epoch 371/500 | Train Loss: 1176426670.465753 | Val Loss: 1382171783.013699\n",
      "Epoch 372/500 | Train Loss: 1219399886.027397 | Val Loss: 1379056524.273973\n",
      "Epoch 373/500 | Train Loss: 1158685646.904109 | Val Loss: 1467696003.506849\n",
      "Epoch 374/500 | Train Loss: 1160706143.561644 | Val Loss: 1489545412.383562\n",
      "Epoch 375/500 | Train Loss: 1169096642.630137 | Val Loss: 1397469883.616438\n",
      "Epoch 376/500 | Train Loss: 1207890088.767123 | Val Loss: 1804520093.808219\n",
      "Epoch 377/500 | Train Loss: 1276062284.493151 | Val Loss: 2045282861.589041\n",
      "Epoch 378/500 | Train Loss: 1168880797.808219 | Val Loss: 1470026248.767123\n",
      "Epoch 379/500 | Train Loss: 1112632778.520548 | Val Loss: 1362054419.287671\n",
      "Epoch 380/500 | Train Loss: 1122774754.191781 | Val Loss: 1347069624.109589\n",
      "Epoch 381/500 | Train Loss: 1228578924.712329 | Val Loss: 1387359803.616438\n",
      "Epoch 382/500 | Train Loss: 1183619029.917808 | Val Loss: 1479190456.109589\n",
      "Epoch 383/500 | Train Loss: 1101576999.232877 | Val Loss: 1386257530.739726\n",
      "Epoch 384/500 | Train Loss: 1116027627.835616 | Val Loss: 1544921282.630137\n",
      "Epoch 385/500 | Train Loss: 1096645197.150685 | Val Loss: 1366752874.958904\n",
      "Epoch 386/500 | Train Loss: 1181182199.232877 | Val Loss: 1419090379.397260\n",
      "Epoch 387/500 | Train Loss: 1104666901.041096 | Val Loss: 1343579420.054795\n",
      "Epoch 388/500 | Train Loss: 1194697496.547945 | Val Loss: 1386693760.000000\n",
      "Epoch 389/500 | Train Loss: 1187077525.041096 | Val Loss: 1395656991.561644\n",
      "Epoch 390/500 | Train Loss: 1128481446.575342 | Val Loss: 1367242427.616438\n",
      "Epoch 391/500 | Train Loss: 1105111714.191781 | Val Loss: 1509516959.561644\n",
      "Epoch 392/500 | Train Loss: 1089486479.123288 | Val Loss: 1359724589.589041\n",
      "Epoch 393/500 | Train Loss: 1119080283.178082 | Val Loss: 1315925360.219178\n",
      "Epoch 394/500 | Train Loss: 1140663951.780822 | Val Loss: 1310735165.369863\n",
      "Epoch 395/500 | Train Loss: 1168130858.082192 | Val Loss: 1368952353.315068\n",
      "Epoch 396/500 | Train Loss: 1123003972.383562 | Val Loss: 1324011556.821918\n",
      "Epoch 397/500 | Train Loss: 1138208803.506849 | Val Loss: 1310795832.109589\n",
      "Epoch 398/500 | Train Loss: 1244928363.835616 | Val Loss: 1330486203.616438\n",
      "Epoch 399/500 | Train Loss: 1084200895.123288 | Val Loss: 1367781653.041096\n",
      "Epoch 400/500 | Train Loss: 1102744074.520548 | Val Loss: 1399317063.890411\n",
      "Epoch 401/500 | Train Loss: 1142500402.849315 | Val Loss: 1960775539.726027\n",
      "Epoch 402/500 | Train Loss: 1296446243.945205 | Val Loss: 1810857834.958904\n",
      "Epoch 403/500 | Train Loss: 1248207193.424658 | Val Loss: 1891742141.369863\n",
      "Epoch 404/500 | Train Loss: 1281899690.958904 | Val Loss: 1868702216.767123\n",
      "Epoch 405/500 | Train Loss: 1106519166.246575 | Val Loss: 1400934082.630137\n",
      "Epoch 406/500 | Train Loss: 1103464509.369863 | Val Loss: 1330764703.561644\n",
      "Epoch 407/500 | Train Loss: 1175901452.273973 | Val Loss: 1480291178.958904\n",
      "Epoch 408/500 | Train Loss: 1284840804.821918 | Val Loss: 1426893180.493151\n",
      "Epoch 409/500 | Train Loss: 1136974178.191781 | Val Loss: 1468421649.534247\n",
      "Epoch 410/500 | Train Loss: 1098450178.630137 | Val Loss: 1704173913.424658\n",
      "Epoch 411/500 | Train Loss: 1215164470.356164 | Val Loss: 1322979578.739726\n",
      "Epoch 412/500 | Train Loss: 1251870068.602740 | Val Loss: 1324272543.561644\n",
      "Epoch 413/500 | Train Loss: 1135334434.191781 | Val Loss: 1313672558.465753\n",
      "Epoch 414/500 | Train Loss: 1092512957.369863 | Val Loss: 1377633187.068493\n",
      "Epoch 415/500 | Train Loss: 1100451366.575342 | Val Loss: 1322881451.835616\n",
      "Epoch 416/500 | Train Loss: 1069712052.602740 | Val Loss: 1294171639.232877\n",
      "Epoch 417/500 | Train Loss: 1050994209.315068 | Val Loss: 1286489785.863014\n",
      "Epoch 418/500 | Train Loss: 1102973450.520548 | Val Loss: 1329308933.260274\n",
      "Epoch 419/500 | Train Loss: 1088573582.904109 | Val Loss: 1279655455.561644\n",
      "Epoch 420/500 | Train Loss: 1046777901.589041 | Val Loss: 1369543439.780822\n",
      "Epoch 421/500 | Train Loss: 1064637068.273973 | Val Loss: 1522315583.123288\n",
      "Epoch 422/500 | Train Loss: 1063838565.698630 | Val Loss: 1548339989.041096\n",
      "Epoch 423/500 | Train Loss: 1117530051.506849 | Val Loss: 1313120094.684932\n",
      "Epoch 424/500 | Train Loss: 1118075251.726027 | Val Loss: 1317390058.958904\n",
      "Epoch 425/500 | Train Loss: 1106606021.698630 | Val Loss: 1288243587.506849\n",
      "Epoch 426/500 | Train Loss: 1085480930.630137 | Val Loss: 1269913216.000000\n",
      "Epoch 427/500 | Train Loss: 1101336609.315068 | Val Loss: 1271833433.424658\n",
      "Epoch 428/500 | Train Loss: 1116756809.643836 | Val Loss: 1277625191.452055\n",
      "Epoch 429/500 | Train Loss: 1104915760.219178 | Val Loss: 1299510631.452055\n",
      "Epoch 430/500 | Train Loss: 1104026382.904109 | Val Loss: 1493486611.287671\n",
      "Epoch 431/500 | Train Loss: 1092211557.698630 | Val Loss: 1311219934.684932\n",
      "Epoch 432/500 | Train Loss: 1127284464.219178 | Val Loss: 1361216387.506849\n",
      "Epoch 433/500 | Train Loss: 1123167148.712329 | Val Loss: 1315237339.178082\n",
      "Epoch 434/500 | Train Loss: 1140343731.726027 | Val Loss: 1302062846.246575\n",
      "Epoch 435/500 | Train Loss: 1152526874.301370 | Val Loss: 1263044646.575342\n",
      "Epoch 436/500 | Train Loss: 1111686337.315068 | Val Loss: 1262626288.219178\n",
      "Epoch 437/500 | Train Loss: 1058693909.041096 | Val Loss: 1246383289.863014\n",
      "Epoch 438/500 | Train Loss: 1043480393.643836 | Val Loss: 1252863104.000000\n",
      "Epoch 439/500 | Train Loss: 1069590533.260274 | Val Loss: 1511786427.616438\n",
      "Epoch 440/500 | Train Loss: 1056882710.794520 | Val Loss: 1287025294.027397\n",
      "Epoch 441/500 | Train Loss: 1072714229.479452 | Val Loss: 1233643793.534247\n",
      "Epoch 442/500 | Train Loss: 1031190040.547945 | Val Loss: 1235448400.657534\n",
      "Epoch 443/500 | Train Loss: 1026581770.520548 | Val Loss: 1389315990.794521\n",
      "Epoch 444/500 | Train Loss: 1017387264.876712 | Val Loss: 1370761040.657534\n",
      "Epoch 445/500 | Train Loss: 1050027186.849315 | Val Loss: 1246138052.383562\n",
      "Epoch 446/500 | Train Loss: 1076108499.287671 | Val Loss: 1245198599.013699\n",
      "Epoch 447/500 | Train Loss: 1059207386.301370 | Val Loss: 1226625618.410959\n",
      "Epoch 448/500 | Train Loss: 1131444675.506849 | Val Loss: 1550154683.616438\n",
      "Epoch 449/500 | Train Loss: 1142150170.301370 | Val Loss: 1244652324.821918\n",
      "Epoch 450/500 | Train Loss: 1028098466.191781 | Val Loss: 1294900736.000000\n",
      "Epoch 451/500 | Train Loss: 1032091761.972603 | Val Loss: 1219108443.178082\n",
      "Epoch 452/500 | Train Loss: 1037605448.767123 | Val Loss: 1399546496.000000\n",
      "Epoch 453/500 | Train Loss: 1039628019.726027 | Val Loss: 1398583357.369863\n",
      "Epoch 454/500 | Train Loss: 1156467782.136986 | Val Loss: 1947538596.821918\n",
      "Epoch 455/500 | Train Loss: 1176216870.575342 | Val Loss: 1309711473.972603\n",
      "Epoch 456/500 | Train Loss: 1038270350.027397 | Val Loss: 1301386673.095891\n",
      "Epoch 457/500 | Train Loss: 995996009.205480 | Val Loss: 1501856810.082192\n",
      "Epoch 458/500 | Train Loss: 1041566559.123288 | Val Loss: 1274537279.123288\n",
      "Epoch 459/500 | Train Loss: 1112438950.575342 | Val Loss: 1262125936.219178\n",
      "Epoch 460/500 | Train Loss: 1131662031.780822 | Val Loss: 1282647103.123288\n",
      "Epoch 461/500 | Train Loss: 1023405134.904110 | Val Loss: 1221500070.575342\n",
      "Epoch 462/500 | Train Loss: 1010942148.383562 | Val Loss: 1291033408.876712\n",
      "Epoch 463/500 | Train Loss: 977439163.616438 | Val Loss: 1207451079.890411\n",
      "Epoch 464/500 | Train Loss: 1024996970.958904 | Val Loss: 1204737823.561644\n",
      "Epoch 465/500 | Train Loss: 1108360917.041096 | Val Loss: 1209281306.301370\n",
      "Epoch 466/500 | Train Loss: 1022547269.698630 | Val Loss: 1192927854.465753\n",
      "Epoch 467/500 | Train Loss: 1073364586.082192 | Val Loss: 1208527105.753425\n",
      "Epoch 468/500 | Train Loss: 1107444386.191781 | Val Loss: 1224666525.808219\n",
      "Epoch 469/500 | Train Loss: 1066983857.095890 | Val Loss: 1223616070.136986\n",
      "Epoch 470/500 | Train Loss: 995273695.561644 | Val Loss: 1269588467.726027\n",
      "Epoch 471/500 | Train Loss: 984546728.328767 | Val Loss: 1604048129.753425\n",
      "Epoch 472/500 | Train Loss: 1115983193.424658 | Val Loss: 1246250443.397260\n",
      "Epoch 473/500 | Train Loss: 1000781696.876712 | Val Loss: 1238548755.287671\n",
      "Epoch 474/500 | Train Loss: 987933445.041096 | Val Loss: 1220552042.958904\n",
      "Epoch 475/500 | Train Loss: 972952026.739726 | Val Loss: 1501200617.205479\n",
      "Epoch 476/500 | Train Loss: 1076461425.972603 | Val Loss: 1295768213.041096\n",
      "Epoch 477/500 | Train Loss: 1012803991.671233 | Val Loss: 1492704419.068493\n",
      "Epoch 478/500 | Train Loss: 1024090992.219178 | Val Loss: 1864836532.602740\n",
      "Epoch 479/500 | Train Loss: 1479091123.726027 | Val Loss: 1551183263.561644\n",
      "Epoch 480/500 | Train Loss: 1028153839.780822 | Val Loss: 1297722455.671233\n",
      "Epoch 481/500 | Train Loss: 1033410258.849315 | Val Loss: 1247230777.863014\n",
      "Epoch 482/500 | Train Loss: 990103374.027397 | Val Loss: 1225573670.575342\n",
      "Epoch 483/500 | Train Loss: 963046530.191781 | Val Loss: 1476149090.191781\n",
      "Epoch 484/500 | Train Loss: 1000339295.561644 | Val Loss: 1449739770.739726\n",
      "Epoch 485/500 | Train Loss: 988377280.876712 | Val Loss: 1332491079.890411\n",
      "Epoch 486/500 | Train Loss: 978512241.972603 | Val Loss: 1277171273.643836\n",
      "Epoch 487/500 | Train Loss: 1157644793.863014 | Val Loss: 1703235676.931507\n",
      "Epoch 488/500 | Train Loss: 1050703493.698630 | Val Loss: 1433347312.219178\n",
      "Epoch 489/500 | Train Loss: 1042973051.616438 | Val Loss: 1275596494.904109\n",
      "Epoch 490/500 | Train Loss: 988655682.630137 | Val Loss: 1203342478.027397\n",
      "Epoch 491/500 | Train Loss: 942616926.684932 | Val Loss: 1186721967.342466\n",
      "Epoch 492/500 | Train Loss: 974412570.301370 | Val Loss: 1329204944.657534\n",
      "Epoch 493/500 | Train Loss: 936426090.082192 | Val Loss: 1180848434.849315\n",
      "Epoch 494/500 | Train Loss: 941449777.095890 | Val Loss: 1436415068.931507\n",
      "Epoch 495/500 | Train Loss: 977855563.397260 | Val Loss: 1208787182.465753\n",
      "Epoch 496/500 | Train Loss: 998061339.178082 | Val Loss: 1139762132.164384\n",
      "Epoch 497/500 | Train Loss: 994069793.315068 | Val Loss: 1141791253.041096\n",
      "Epoch 498/500 | Train Loss: 949676748.273973 | Val Loss: 1253111113.643836\n",
      "Epoch 499/500 | Train Loss: 952597330.410959 | Val Loss: 1137470080.000000\n",
      "Epoch 500/500 | Train Loss: 958939723.397260 | Val Loss: 1199377453.589041\n"
     ]
    }
   ],
   "execution_count": 92
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
